{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Fast, accurate and scalable probabilistic data linkage \u00b6 Splink is a Python package for probabilistic record linkage (entity resolution) that allows you to deduplicate and link records from datasets without unique identifiers. Key Features \u00b6 Speed: Capable of linking a million records on a laptop in approximately one minute. Accuracy: Full support for term frequency adjustments and user-defined fuzzy matching logic. Scalability: Execute linkage jobs in Python (using DuckDB) or big-data backends like AWS Athena or Spark for 100+ million records. Unsupervised Learning: No training data is required, as models can be trained using an unsupervised approach. Interactive Outputs: Provides a wide range of interactive outputs to help users understand their model and diagnose linkage problems. Splink's core linkage algorithm is based on Fellegi-Sunter's model of record linkage, with various customizations to improve accuracy. What does Splink do? \u00b6 Consider the following records that lack a unique person identifier: Splink predicts which rows link together: and clusters these links to produce an estimated person ID: What data does Splink work best with? \u00b6 Before using Splink, input data should be standardized, with consistent column names and formatting (e.g., lowercased, punctuation cleaned up, etc.). Splink performs best with input data containing multiple columns that are not highly correlated . For instance, if the entity type is persons, you may have columns for full name, date of birth, and city. If the entity type is companies, you could have columns for name, turnover, sector, and telephone number. High correlation occurs when the value of a column is highly constrained (predictable) from the value of another column. For example, a 'city' field is almost perfectly correlated with 'postcode'. Gender is highly correlated with 'first name'. Correlation is particularly problematic if all of your input columns are highly correlated. Splink is not designed for linking a single column containing a 'bag of words'. For example, a table with a single 'company name' column, and no other details. Documentation \u00b6 The homepage for the Splink documentation can be found here . Interactive demos can be found here , or by clicking the following Binder link: The specification of the Fellegi Sunter statistical model behind splink is similar as that used in the R fastLink package . Accompanying the fastLink package is an academic paper that describes this model. A series of interactive articles also explores the theory behind Splink. The Office for National Statistics have written a case study about using Splink to link 2021 Census data to itself. Installation \u00b6 Splink supports python 3.7+. To obtain the latest released version of splink you can install from PyPI using pip: pip install splink or, if you prefer, you can instead install splink using conda: conda install -c conda-forge splink Quickstart \u00b6 The following code demonstrates how to estimate the parameters of a deduplication model, use it to identify duplicate records, and then use clustering to generate an estimated unique person ID. For more detailed tutorials, please see here . from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb.duckdb_comparison_library import ( exact_match , levenshtein_at_thresholds , ) import pandas as pd df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ levenshtein_at_thresholds ( \"first_name\" , 2 ), exact_match ( \"surname\" ), exact_match ( \"dob\" ), exact_match ( \"city\" , term_frequency_adjustments = True ), exact_match ( \"email\" ), ], } linker = DuckDBLinker ( df , settings ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) blocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) blocking_rule_for_training = \"l.dob = r.dob\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) pairwise_predictions = linker . predict () clusters = linker . cluster_pairwise_predictions_at_threshold ( pairwise_predictions , 0.95 ) clusters . as_pandas_dataframe ( limit = 5 ) Videos \u00b6 A introductory presentation on Splink An introduction to the Splink Comparison Viewer dashboard Support \u00b6 Please post on the discussion forums if you have any questions. If you think you have found a bug, pleaase raise an issue . Awards \u00b6 \ud83e\udd47 Analysis in Government Awards 2020: Innovative Methods: Winner \ud83e\udd47 MoJ DASD Awards 2020: Innovation and Impact - Winner \ud83e\udd47 Analysis in Government Awards 2022: People's Choice Award - Winner \ud83e\udd48 Analysis in Government Awards 2022: Innovative Methods Runner up Citation \u00b6 If you use Splink in your research, we'd be grateful for a citation as follows: @article { Linacre_Lindsay_Manassis_Slade_Hepworth_2022 , title = {Splink: Free software for probabilistic record linkage at scale.} , author = {Linacre, Robin and Lindsay, Sam and Manassis, Theodore and Slade, Zoe and Hepworth, Tom} , year = 2022 , month = {Aug.} , journal = {International Journal of Population Data Science} , volume = 7 , number = 3 , doi = {10.23889/ijpds.v7i3.1794} , url = {https://ijpds.org/article/view/1794} , } Acknowledgements \u00b6 We are very grateful to ADR UK (Administrative Data Research UK) for providing the initial funding for this work as part of the Data First project. We are extremely grateful to professors Katie Harron, James Doidge and Peter Christen for their expert advice and guidance in the development of Splink. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work. Any errors remain our own.","title":"Home"},{"location":"index.html#fast-accurate-and-scalable-probabilistic-data-linkage","text":"Splink is a Python package for probabilistic record linkage (entity resolution) that allows you to deduplicate and link records from datasets without unique identifiers.","title":"Fast, accurate and scalable probabilistic data linkage"},{"location":"index.html#key-features","text":"Speed: Capable of linking a million records on a laptop in approximately one minute. Accuracy: Full support for term frequency adjustments and user-defined fuzzy matching logic. Scalability: Execute linkage jobs in Python (using DuckDB) or big-data backends like AWS Athena or Spark for 100+ million records. Unsupervised Learning: No training data is required, as models can be trained using an unsupervised approach. Interactive Outputs: Provides a wide range of interactive outputs to help users understand their model and diagnose linkage problems. Splink's core linkage algorithm is based on Fellegi-Sunter's model of record linkage, with various customizations to improve accuracy.","title":"Key Features"},{"location":"index.html#what-does-splink-do","text":"Consider the following records that lack a unique person identifier: Splink predicts which rows link together: and clusters these links to produce an estimated person ID:","title":"What does Splink do?"},{"location":"index.html#what-data-does-splink-work-best-with","text":"Before using Splink, input data should be standardized, with consistent column names and formatting (e.g., lowercased, punctuation cleaned up, etc.). Splink performs best with input data containing multiple columns that are not highly correlated . For instance, if the entity type is persons, you may have columns for full name, date of birth, and city. If the entity type is companies, you could have columns for name, turnover, sector, and telephone number. High correlation occurs when the value of a column is highly constrained (predictable) from the value of another column. For example, a 'city' field is almost perfectly correlated with 'postcode'. Gender is highly correlated with 'first name'. Correlation is particularly problematic if all of your input columns are highly correlated. Splink is not designed for linking a single column containing a 'bag of words'. For example, a table with a single 'company name' column, and no other details.","title":"What data does Splink work best with?"},{"location":"index.html#documentation","text":"The homepage for the Splink documentation can be found here . Interactive demos can be found here , or by clicking the following Binder link: The specification of the Fellegi Sunter statistical model behind splink is similar as that used in the R fastLink package . Accompanying the fastLink package is an academic paper that describes this model. A series of interactive articles also explores the theory behind Splink. The Office for National Statistics have written a case study about using Splink to link 2021 Census data to itself.","title":"Documentation"},{"location":"index.html#installation","text":"Splink supports python 3.7+. To obtain the latest released version of splink you can install from PyPI using pip: pip install splink or, if you prefer, you can instead install splink using conda: conda install -c conda-forge splink","title":"Installation"},{"location":"index.html#quickstart","text":"The following code demonstrates how to estimate the parameters of a deduplication model, use it to identify duplicate records, and then use clustering to generate an estimated unique person ID. For more detailed tutorials, please see here . from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb.duckdb_comparison_library import ( exact_match , levenshtein_at_thresholds , ) import pandas as pd df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ levenshtein_at_thresholds ( \"first_name\" , 2 ), exact_match ( \"surname\" ), exact_match ( \"dob\" ), exact_match ( \"city\" , term_frequency_adjustments = True ), exact_match ( \"email\" ), ], } linker = DuckDBLinker ( df , settings ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) blocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) blocking_rule_for_training = \"l.dob = r.dob\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) pairwise_predictions = linker . predict () clusters = linker . cluster_pairwise_predictions_at_threshold ( pairwise_predictions , 0.95 ) clusters . as_pandas_dataframe ( limit = 5 )","title":"Quickstart"},{"location":"index.html#videos","text":"A introductory presentation on Splink An introduction to the Splink Comparison Viewer dashboard","title":"Videos"},{"location":"index.html#support","text":"Please post on the discussion forums if you have any questions. If you think you have found a bug, pleaase raise an issue .","title":"Support"},{"location":"index.html#awards","text":"\ud83e\udd47 Analysis in Government Awards 2020: Innovative Methods: Winner \ud83e\udd47 MoJ DASD Awards 2020: Innovation and Impact - Winner \ud83e\udd47 Analysis in Government Awards 2022: People's Choice Award - Winner \ud83e\udd48 Analysis in Government Awards 2022: Innovative Methods Runner up","title":"Awards"},{"location":"index.html#citation","text":"If you use Splink in your research, we'd be grateful for a citation as follows: @article { Linacre_Lindsay_Manassis_Slade_Hepworth_2022 , title = {Splink: Free software for probabilistic record linkage at scale.} , author = {Linacre, Robin and Lindsay, Sam and Manassis, Theodore and Slade, Zoe and Hepworth, Tom} , year = 2022 , month = {Aug.} , journal = {International Journal of Population Data Science} , volume = 7 , number = 3 , doi = {10.23889/ijpds.v7i3.1794} , url = {https://ijpds.org/article/view/1794} , }","title":"Citation"},{"location":"index.html#acknowledgements","text":"We are very grateful to ADR UK (Administrative Data Research UK) for providing the initial funding for this work as part of the Data First project. We are extremely grateful to professors Katie Harron, James Doidge and Peter Christen for their expert advice and guidance in the development of Splink. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work. Any errors remain our own.","title":"Acknowledgements"},{"location":"LICENSE.html","text":"MIT License Copyright (c) 2020 Ministry of Justice Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"LICENSE"},{"location":"SplinkDataFrame.html","tags":["API"],"text":"Documentation for SplinkDataFrame object \u00b6 Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like as_pandas_dataframe() and as_record_dict() to retrieve data Source code in splink/splink_dataframe.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class SplinkDataFrame : \"\"\"Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data \"\"\" def __init__ ( self , templated_name : str , physical_name : str , linker : Linker ): self . templated_name = templated_name self . physical_name = physical_name self . linker = linker @property def columns ( self ): pass @property def columns_escaped ( self ): cols = self . columns return [ c . name () for c in cols ] def validate (): pass def _random_sample_sql ( percent ): raise NotImplementedError ( \"Random sample sql not implemented for this linker\" ) @property def physical_and_template_names_equal ( self ): return self . templated_name == self . physical_name def _check_drop_table_created_by_splink ( self , force_non_splink_table = False ): if not self . physical_name . startswith ( \"__splink__\" ): if not force_non_splink_table : raise ValueError ( f \"You've asked to drop table { self . physical_name } from your \" \"database which is not a table created by Splink. If you really \" \"want to drop this table, you can do so by setting \" \"force_non_splink_table=True\" ) logger . debug ( f \"Dropping table with templated name { self . templated_name } and \" f \"physical name { self . physical_name } \" ) def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" ) def as_record_dict ( self , limit = None ): pass def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit )) def _repr_pretty_ ( self , p , cycle ): msg = ( f \"Table name in database: ` { self . physical_name } ` \\n \" \" \\n To retrieve records, you can call the following methods on this object:\" \" \\n `.as_record_dict(limit=5)` or \" \"`.as_pandas_dataframe(limit=5)`. \\n \" \" \\n You may omit the `limit` argument to return all records.\" \" \\n\\n This table represents the following splink entity: \" f \" { self . templated_name } \" ) p . text ( msg ) drop_table_from_database ( force_non_splink_table = False ) \u00b6 Source code in splink/splink_dataframe.py 58 59 60 61 def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" ) as_pandas_dataframe ( limit = None ) \u00b6 Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Parameters: Name Type Description Default limit int If provided, return this number of rows (equivalent None Returns: Type Description pandas.DataFrame: pandas Dataframe Source code in splink/splink_dataframe.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit ))","title":"SplinkDataFrame API"},{"location":"SplinkDataFrame.html#documentation-for-splinkdataframe-object","text":"Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like as_pandas_dataframe() and as_record_dict() to retrieve data Source code in splink/splink_dataframe.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class SplinkDataFrame : \"\"\"Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data \"\"\" def __init__ ( self , templated_name : str , physical_name : str , linker : Linker ): self . templated_name = templated_name self . physical_name = physical_name self . linker = linker @property def columns ( self ): pass @property def columns_escaped ( self ): cols = self . columns return [ c . name () for c in cols ] def validate (): pass def _random_sample_sql ( percent ): raise NotImplementedError ( \"Random sample sql not implemented for this linker\" ) @property def physical_and_template_names_equal ( self ): return self . templated_name == self . physical_name def _check_drop_table_created_by_splink ( self , force_non_splink_table = False ): if not self . physical_name . startswith ( \"__splink__\" ): if not force_non_splink_table : raise ValueError ( f \"You've asked to drop table { self . physical_name } from your \" \"database which is not a table created by Splink. If you really \" \"want to drop this table, you can do so by setting \" \"force_non_splink_table=True\" ) logger . debug ( f \"Dropping table with templated name { self . templated_name } and \" f \"physical name { self . physical_name } \" ) def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" ) def as_record_dict ( self , limit = None ): pass def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit )) def _repr_pretty_ ( self , p , cycle ): msg = ( f \"Table name in database: ` { self . physical_name } ` \\n \" \" \\n To retrieve records, you can call the following methods on this object:\" \" \\n `.as_record_dict(limit=5)` or \" \"`.as_pandas_dataframe(limit=5)`. \\n \" \" \\n You may omit the `limit` argument to return all records.\" \" \\n\\n This table represents the following splink entity: \" f \" { self . templated_name } \" ) p . text ( msg )","title":"Documentation for SplinkDataFrame object"},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.drop_table_from_database","text":"Source code in splink/splink_dataframe.py 58 59 60 61 def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" )","title":"drop_table_from_database()"},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.as_pandas_dataframe","text":"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Parameters: Name Type Description Default limit int If provided, return this number of rows (equivalent None Returns: Type Description pandas.DataFrame: pandas Dataframe Source code in splink/splink_dataframe.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit ))","title":"as_pandas_dataframe()"},{"location":"comparison.html","tags":["API"],"text":"Documentation for Comparison object \u00b6 Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more ComparisonLevel s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. Source code in splink/comparison.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 class Comparison : \"\"\"Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more `ComparisonLevel`s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: ``` Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. ``` \"\"\" def __init__ ( self , comparison_dict , settings_obj : Settings = None ): # Protected because we don't want to modify self . _comparison_dict = comparison_dict comparison_level_list = comparison_dict [ \"comparison_levels\" ] self . comparison_levels : list [ ComparisonLevel ] = [] # If comparison_levels are already of type ComparisonLevel, register # the settings object on them # otherwise turn the dictionaries into ComparisonLevel for cl in comparison_level_list : if isinstance ( cl , ComparisonLevel ): cl . comparison = self elif settings_obj is None : cl = ComparisonLevel ( cl , self ) else : cl = ComparisonLevel ( cl , self , sql_dialect = settings_obj . _sql_dialect ) self . comparison_levels . append ( cl ) self . _settings_obj : Settings = settings_obj # Assign comparison vector values starting at highest level, count down to 0 num_levels = self . _num_levels counter = num_levels - 1 for level in self . comparison_levels : if level . is_null_level : level . _comparison_vector_value = - 1 level . _max_level = False else : level . _comparison_vector_value = counter if counter == num_levels - 1 : level . _max_level = True else : level . _max_level = False counter -= 1 def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the Comparison which is independent of the original e.g. modifying the copy will not affect the original. This method implements ensures the Comparison can be deepcopied. \"\"\" cc = Comparison ( self . as_dict (), self . _settings_obj ) return cc @property def _num_levels ( self ): return len ([ cl for cl in self . comparison_levels if not cl . is_null_level ]) @property def _comparison_levels_excluding_null ( self ): return [ cl for cl in self . comparison_levels if not cl . is_null_level ] @property def _gamma_prefix ( self ): return self . _settings_obj . _gamma_prefix @property def _retain_intermediate_calculation_columns ( self ): return self . _settings_obj . _retain_intermediate_calculation_columns @property def _bf_column_name ( self ): return f \" { self . _settings_obj . _bf_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _has_null_level ( self ): return any ([ cl . is_null_level for cl in self . comparison_levels ]) @property def _bf_tf_adj_column_name ( self ): bf = self . _settings_obj . _bf_prefix tf = self . _settings_obj . _tf_prefix cc_name = self . _output_column_name return f \" { bf }{ tf } adj_ { cc_name } \" . replace ( \" \" , \"_\" ) @property def _has_tf_adjustments ( self ): return any ([ cl . _has_tf_adjustments for cl in self . comparison_levels ]) @property def _case_statement ( self ): sqls = [ cl . _when_then_comparison_vector_value_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _gamma_column_name } \" return sql @property def _input_columns_used_by_case_statement ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _input_columns_used_by_sql_condition ) # dedupe_preserving_order on input column already_observed = [] deduped_cols = [] for col in cols : if col . input_name not in already_observed : deduped_cols . append ( col ) already_observed . append ( col . input_name ) return deduped_cols @property def _output_column_name ( self ): if \"output_column_name\" in self . _comparison_dict : return self . _comparison_dict [ \"output_column_name\" ] else : cols = self . _input_columns_used_by_case_statement cols = [ c . input_name for c in cols ] if len ( cols ) == 1 : return cols [ 0 ] else : return f \"custom_ { '_' . join ( cols ) } \" @property def _comparison_description ( self ): if \"comparison_description\" in self . _comparison_dict : return self . _comparison_dict [ \"comparison_description\" ] else : return self . _output_column_name @property def _gamma_column_name ( self ): return f \" { self . _gamma_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _tf_adjustment_input_col_names ( self ): cols = [ cl . _tf_adjustment_input_column_name for cl in self . comparison_levels ] cols = [ c for c in cols if c ] return cols @property def _columns_to_select_for_blocking ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _columns_to_select_for_blocking ) return dedupe_preserving_order ( cols ) @property def _columns_to_select_for_comparison_vector_values ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _case_statement ) for cl in self . comparison_levels : if cl . _has_tf_adjustments : col = cl . _tf_adjustment_input_column output_cols . extend ( col . tf_name_l_r ()) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_bayes_factor_parts ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _gamma_column_name ) for cl in self . comparison_levels : if ( cl . _has_tf_adjustments and self . _settings_obj . _retain_intermediate_calculation_columns ): col = cl . _tf_adjustment_input_column output_cols . extend ( col . tf_name_l_r ()) # Bayes factor case when statement sqls = [ cl . _bayes_factor_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_column_name } \" output_cols . append ( sql ) # tf adjustment case when statement if self . _has_tf_adjustments : sqls = [ cl . _tf_adjustment_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_tf_adj_column_name } \" output_cols . append ( sql ) output_cols . append ( self . _gamma_column_name ) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_predict ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) if ( self . _settings_obj . _training_mode or self . _settings_obj . _retain_matching_columns ): output_cols . append ( self . _gamma_column_name ) for cl in self . comparison_levels : if ( cl . _has_tf_adjustments and self . _settings_obj . _retain_intermediate_calculation_columns ): col = cl . _tf_adjustment_input_column output_cols . extend ( col . tf_name_l_r ()) for _col in input_cols : if self . _settings_obj . _retain_intermediate_calculation_columns : output_cols . extend ( self . _match_weight_columns_to_multiply ) return dedupe_preserving_order ( output_cols ) @property def _match_weight_columns_to_multiply ( self ): cols = [] cols . append ( self . _bf_column_name ) if self . _has_tf_adjustments : cols . append ( self . _bf_tf_adj_column_name ) return cols @property def _term_frequency_columns ( self ): cols = set () for cl in self . comparison_levels : cols . add ( cl . tf_adjustment_input_col_name ) return list ( cols ) def as_dict ( self ): d = { \"output_column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . as_dict () for cl in self . comparison_levels ], } if \"comparison_description\" in self . _comparison_dict : d [ \"comparison_description\" ] = self . _comparison_dict [ \"comparison_description\" ] return d def _as_completed_dict ( self ): return { \"column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . _as_completed_dict () for cl in self . comparison_levels ], \"input_columns_used_by_case_statement\" : [ c . input_name for c in self . _input_columns_used_by_case_statement ], } @property def _has_estimated_m_values ( self ): return all ( cl . _has_estimated_m_values for cl in self . comparison_levels ) @property def _has_estimated_u_values ( self ): return all ( cl . _has_estimated_u_values for cl in self . comparison_levels ) @property def _all_m_are_trained ( self ): return all ( cl . _m_is_trained for cl in self . comparison_levels ) @property def _all_u_are_trained ( self ): return all ( cl . _u_is_trained for cl in self . comparison_levels ) @property def _some_m_are_trained ( self ): return any ( cl . _m_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _some_u_are_trained ( self ): return any ( cl . _u_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _is_trained_message ( self ): messages = [] if self . _all_m_are_trained and self . _all_u_are_trained : return None if not self . _some_u_are_trained : messages . append ( \"no u values are trained\" ) elif self . _some_u_are_trained and not self . _all_u_are_trained : messages . append ( \"some u values are not trained\" ) if not self . _some_m_are_trained : messages . append ( \"no m values are trained\" ) elif self . _some_m_are_trained and not self . _all_m_are_trained : messages . append ( \"some m values are not trained\" ) message = \", \" . join ( messages ) message = f \" - { self . _output_column_name } ( { message } ).\" return message @property def _is_trained ( self ): return self . _all_m_are_trained and self . _all_u_are_trained @property def _as_detailed_records ( self ): records = [] for cl in self . comparison_levels : record = {} record [ \"comparison_name\" ] = self . _output_column_name record = { ** record , ** cl . _as_detailed_record } records . append ( record ) return records @property def _parameter_estimates_as_records ( self ): records = [] for cl in self . comparison_levels : new_records = cl . _parameter_estimates_as_records for r in new_records : r [ \"comparison_name\" ] = self . _output_column_name records . extend ( new_records ) return records def _get_comparison_level_by_comparison_vector_value ( self , value ) -> ComparisonLevel : for cl in self . comparison_levels : if cl . _comparison_vector_value == value : return cl raise ValueError ( f \"No comparison level with comparison vector value { value } \" ) def __repr__ ( self ): return ( f \"<Comparison { self . _comparison_description } with \" f \" { self . _num_levels } levels at { hex ( id ( self )) } >\" ) @property def _not_trained_messages ( self ): msgs = [] cname = self . _output_column_name header = f \"Comparison: ' { cname } ': \\n \" msg_template = \" {header} {m_or_u} values not fully trained\" if not self . _all_m_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"m\" )) if not self . _all_u_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"u\" )) return msgs @property def _comparison_level_description_list ( self ): cl_template = \" - ' {label} ' with SQL rule: {sql} \\n \" comp_levels = [ cl_template . format ( cvv = cl . _comparison_vector_value , label = cl . label_for_charts , sql = cl . sql_condition , ) for cl in self . comparison_levels ] comp_levels = \"\" . join ( comp_levels ) return comp_levels @property def _human_readable_description_succinct ( self ): input_cols = join_list_with_commas_final_and ( [ c . name () for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = ( f \"of { input_cols } \\n Description: ' { self . _comparison_description } '\" ) else : main_desc = f \"of { input_cols } \" desc = f \"Comparison { main_desc } \\n Comparison levels: \\n { comp_levels } \" return desc @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name () for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = f \"' { self . _comparison_description } ' of { input_cols } \" else : main_desc = f \"of { input_cols } \" desc = ( f \"Comparison { main_desc } . \\n \" \"Similarity is assessed using the following \" f \"ComparisonLevels: \\n { comp_levels } \" ) return desc def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict ) human_readable_description property \u00b6 __init__ ( comparison_dict , settings_obj = None ) \u00b6 Source code in splink/comparison.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , comparison_dict , settings_obj : Settings = None ): # Protected because we don't want to modify self . _comparison_dict = comparison_dict comparison_level_list = comparison_dict [ \"comparison_levels\" ] self . comparison_levels : list [ ComparisonLevel ] = [] # If comparison_levels are already of type ComparisonLevel, register # the settings object on them # otherwise turn the dictionaries into ComparisonLevel for cl in comparison_level_list : if isinstance ( cl , ComparisonLevel ): cl . comparison = self elif settings_obj is None : cl = ComparisonLevel ( cl , self ) else : cl = ComparisonLevel ( cl , self , sql_dialect = settings_obj . _sql_dialect ) self . comparison_levels . append ( cl ) self . _settings_obj : Settings = settings_obj # Assign comparison vector values starting at highest level, count down to 0 num_levels = self . _num_levels counter = num_levels - 1 for level in self . comparison_levels : if level . is_null_level : level . _comparison_vector_value = - 1 level . _max_level = False else : level . _comparison_vector_value = counter if counter == num_levels - 1 : level . _max_level = True else : level . _max_level = False counter -= 1 match_weights_chart ( as_dict = False ) \u00b6 Display a chart of comparison levels of the comparison Source code in splink/comparison.py 484 485 486 487 488 489 def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict )","title":"Comparison"},{"location":"comparison.html#documentation-for-comparison-object","text":"Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more ComparisonLevel s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. Source code in splink/comparison.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 class Comparison : \"\"\"Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more `ComparisonLevel`s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: ``` Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. ``` \"\"\" def __init__ ( self , comparison_dict , settings_obj : Settings = None ): # Protected because we don't want to modify self . _comparison_dict = comparison_dict comparison_level_list = comparison_dict [ \"comparison_levels\" ] self . comparison_levels : list [ ComparisonLevel ] = [] # If comparison_levels are already of type ComparisonLevel, register # the settings object on them # otherwise turn the dictionaries into ComparisonLevel for cl in comparison_level_list : if isinstance ( cl , ComparisonLevel ): cl . comparison = self elif settings_obj is None : cl = ComparisonLevel ( cl , self ) else : cl = ComparisonLevel ( cl , self , sql_dialect = settings_obj . _sql_dialect ) self . comparison_levels . append ( cl ) self . _settings_obj : Settings = settings_obj # Assign comparison vector values starting at highest level, count down to 0 num_levels = self . _num_levels counter = num_levels - 1 for level in self . comparison_levels : if level . is_null_level : level . _comparison_vector_value = - 1 level . _max_level = False else : level . _comparison_vector_value = counter if counter == num_levels - 1 : level . _max_level = True else : level . _max_level = False counter -= 1 def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the Comparison which is independent of the original e.g. modifying the copy will not affect the original. This method implements ensures the Comparison can be deepcopied. \"\"\" cc = Comparison ( self . as_dict (), self . _settings_obj ) return cc @property def _num_levels ( self ): return len ([ cl for cl in self . comparison_levels if not cl . is_null_level ]) @property def _comparison_levels_excluding_null ( self ): return [ cl for cl in self . comparison_levels if not cl . is_null_level ] @property def _gamma_prefix ( self ): return self . _settings_obj . _gamma_prefix @property def _retain_intermediate_calculation_columns ( self ): return self . _settings_obj . _retain_intermediate_calculation_columns @property def _bf_column_name ( self ): return f \" { self . _settings_obj . _bf_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _has_null_level ( self ): return any ([ cl . is_null_level for cl in self . comparison_levels ]) @property def _bf_tf_adj_column_name ( self ): bf = self . _settings_obj . _bf_prefix tf = self . _settings_obj . _tf_prefix cc_name = self . _output_column_name return f \" { bf }{ tf } adj_ { cc_name } \" . replace ( \" \" , \"_\" ) @property def _has_tf_adjustments ( self ): return any ([ cl . _has_tf_adjustments for cl in self . comparison_levels ]) @property def _case_statement ( self ): sqls = [ cl . _when_then_comparison_vector_value_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _gamma_column_name } \" return sql @property def _input_columns_used_by_case_statement ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _input_columns_used_by_sql_condition ) # dedupe_preserving_order on input column already_observed = [] deduped_cols = [] for col in cols : if col . input_name not in already_observed : deduped_cols . append ( col ) already_observed . append ( col . input_name ) return deduped_cols @property def _output_column_name ( self ): if \"output_column_name\" in self . _comparison_dict : return self . _comparison_dict [ \"output_column_name\" ] else : cols = self . _input_columns_used_by_case_statement cols = [ c . input_name for c in cols ] if len ( cols ) == 1 : return cols [ 0 ] else : return f \"custom_ { '_' . join ( cols ) } \" @property def _comparison_description ( self ): if \"comparison_description\" in self . _comparison_dict : return self . _comparison_dict [ \"comparison_description\" ] else : return self . _output_column_name @property def _gamma_column_name ( self ): return f \" { self . _gamma_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _tf_adjustment_input_col_names ( self ): cols = [ cl . _tf_adjustment_input_column_name for cl in self . comparison_levels ] cols = [ c for c in cols if c ] return cols @property def _columns_to_select_for_blocking ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _columns_to_select_for_blocking ) return dedupe_preserving_order ( cols ) @property def _columns_to_select_for_comparison_vector_values ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _case_statement ) for cl in self . comparison_levels : if cl . _has_tf_adjustments : col = cl . _tf_adjustment_input_column output_cols . extend ( col . tf_name_l_r ()) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_bayes_factor_parts ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _gamma_column_name ) for cl in self . comparison_levels : if ( cl . _has_tf_adjustments and self . _settings_obj . _retain_intermediate_calculation_columns ): col = cl . _tf_adjustment_input_column output_cols . extend ( col . tf_name_l_r ()) # Bayes factor case when statement sqls = [ cl . _bayes_factor_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_column_name } \" output_cols . append ( sql ) # tf adjustment case when statement if self . _has_tf_adjustments : sqls = [ cl . _tf_adjustment_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_tf_adj_column_name } \" output_cols . append ( sql ) output_cols . append ( self . _gamma_column_name ) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_predict ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) if ( self . _settings_obj . _training_mode or self . _settings_obj . _retain_matching_columns ): output_cols . append ( self . _gamma_column_name ) for cl in self . comparison_levels : if ( cl . _has_tf_adjustments and self . _settings_obj . _retain_intermediate_calculation_columns ): col = cl . _tf_adjustment_input_column output_cols . extend ( col . tf_name_l_r ()) for _col in input_cols : if self . _settings_obj . _retain_intermediate_calculation_columns : output_cols . extend ( self . _match_weight_columns_to_multiply ) return dedupe_preserving_order ( output_cols ) @property def _match_weight_columns_to_multiply ( self ): cols = [] cols . append ( self . _bf_column_name ) if self . _has_tf_adjustments : cols . append ( self . _bf_tf_adj_column_name ) return cols @property def _term_frequency_columns ( self ): cols = set () for cl in self . comparison_levels : cols . add ( cl . tf_adjustment_input_col_name ) return list ( cols ) def as_dict ( self ): d = { \"output_column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . as_dict () for cl in self . comparison_levels ], } if \"comparison_description\" in self . _comparison_dict : d [ \"comparison_description\" ] = self . _comparison_dict [ \"comparison_description\" ] return d def _as_completed_dict ( self ): return { \"column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . _as_completed_dict () for cl in self . comparison_levels ], \"input_columns_used_by_case_statement\" : [ c . input_name for c in self . _input_columns_used_by_case_statement ], } @property def _has_estimated_m_values ( self ): return all ( cl . _has_estimated_m_values for cl in self . comparison_levels ) @property def _has_estimated_u_values ( self ): return all ( cl . _has_estimated_u_values for cl in self . comparison_levels ) @property def _all_m_are_trained ( self ): return all ( cl . _m_is_trained for cl in self . comparison_levels ) @property def _all_u_are_trained ( self ): return all ( cl . _u_is_trained for cl in self . comparison_levels ) @property def _some_m_are_trained ( self ): return any ( cl . _m_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _some_u_are_trained ( self ): return any ( cl . _u_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _is_trained_message ( self ): messages = [] if self . _all_m_are_trained and self . _all_u_are_trained : return None if not self . _some_u_are_trained : messages . append ( \"no u values are trained\" ) elif self . _some_u_are_trained and not self . _all_u_are_trained : messages . append ( \"some u values are not trained\" ) if not self . _some_m_are_trained : messages . append ( \"no m values are trained\" ) elif self . _some_m_are_trained and not self . _all_m_are_trained : messages . append ( \"some m values are not trained\" ) message = \", \" . join ( messages ) message = f \" - { self . _output_column_name } ( { message } ).\" return message @property def _is_trained ( self ): return self . _all_m_are_trained and self . _all_u_are_trained @property def _as_detailed_records ( self ): records = [] for cl in self . comparison_levels : record = {} record [ \"comparison_name\" ] = self . _output_column_name record = { ** record , ** cl . _as_detailed_record } records . append ( record ) return records @property def _parameter_estimates_as_records ( self ): records = [] for cl in self . comparison_levels : new_records = cl . _parameter_estimates_as_records for r in new_records : r [ \"comparison_name\" ] = self . _output_column_name records . extend ( new_records ) return records def _get_comparison_level_by_comparison_vector_value ( self , value ) -> ComparisonLevel : for cl in self . comparison_levels : if cl . _comparison_vector_value == value : return cl raise ValueError ( f \"No comparison level with comparison vector value { value } \" ) def __repr__ ( self ): return ( f \"<Comparison { self . _comparison_description } with \" f \" { self . _num_levels } levels at { hex ( id ( self )) } >\" ) @property def _not_trained_messages ( self ): msgs = [] cname = self . _output_column_name header = f \"Comparison: ' { cname } ': \\n \" msg_template = \" {header} {m_or_u} values not fully trained\" if not self . _all_m_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"m\" )) if not self . _all_u_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"u\" )) return msgs @property def _comparison_level_description_list ( self ): cl_template = \" - ' {label} ' with SQL rule: {sql} \\n \" comp_levels = [ cl_template . format ( cvv = cl . _comparison_vector_value , label = cl . label_for_charts , sql = cl . sql_condition , ) for cl in self . comparison_levels ] comp_levels = \"\" . join ( comp_levels ) return comp_levels @property def _human_readable_description_succinct ( self ): input_cols = join_list_with_commas_final_and ( [ c . name () for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = ( f \"of { input_cols } \\n Description: ' { self . _comparison_description } '\" ) else : main_desc = f \"of { input_cols } \" desc = f \"Comparison { main_desc } \\n Comparison levels: \\n { comp_levels } \" return desc @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name () for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = f \"' { self . _comparison_description } ' of { input_cols } \" else : main_desc = f \"of { input_cols } \" desc = ( f \"Comparison { main_desc } . \\n \" \"Similarity is assessed using the following \" f \"ComparisonLevels: \\n { comp_levels } \" ) return desc def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict )","title":"Documentation for Comparison object"},{"location":"comparison.html#splink.comparison.Comparison.human_readable_description","text":"","title":"human_readable_description"},{"location":"comparison.html#splink.comparison.Comparison.__init__","text":"Source code in splink/comparison.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , comparison_dict , settings_obj : Settings = None ): # Protected because we don't want to modify self . _comparison_dict = comparison_dict comparison_level_list = comparison_dict [ \"comparison_levels\" ] self . comparison_levels : list [ ComparisonLevel ] = [] # If comparison_levels are already of type ComparisonLevel, register # the settings object on them # otherwise turn the dictionaries into ComparisonLevel for cl in comparison_level_list : if isinstance ( cl , ComparisonLevel ): cl . comparison = self elif settings_obj is None : cl = ComparisonLevel ( cl , self ) else : cl = ComparisonLevel ( cl , self , sql_dialect = settings_obj . _sql_dialect ) self . comparison_levels . append ( cl ) self . _settings_obj : Settings = settings_obj # Assign comparison vector values starting at highest level, count down to 0 num_levels = self . _num_levels counter = num_levels - 1 for level in self . comparison_levels : if level . is_null_level : level . _comparison_vector_value = - 1 level . _max_level = False else : level . _comparison_vector_value = counter if counter == num_levels - 1 : level . _max_level = True else : level . _max_level = False counter -= 1","title":"__init__()"},{"location":"comparison.html#splink.comparison.Comparison.match_weights_chart","text":"Display a chart of comparison levels of the comparison Source code in splink/comparison.py 484 485 486 487 488 489 def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict )","title":"match_weights_chart()"},{"location":"comparison_level.html","tags":["API","comparisons"],"text":"Documentation for ComparisonLevel object \u00b6 Each ComparisonLevel defines a gradation (category) of similarity within a Comparison . For example, a Comparison that uses the first_name and surname columns may define three ComparisonLevel s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. Source code in splink/comparison_level.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 class ComparisonLevel : \"\"\"Each ComparisonLevel defines a gradation (category) of similarity within a `Comparison`. For example, a `Comparison` that uses the first_name and surname columns may define three `ComparisonLevel`s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: ``` Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. ``` \"\"\" def __init__ ( self , level_dict , comparison : Comparison = None , sql_dialect : str = None , ): # Protected, because we don't want to modify the original dict self . _level_dict = level_dict self . comparison : Comparison = comparison if not hasattr ( self , \"_sql_dialect\" ): self . _sql_dialect = sql_dialect self . _sql_condition = self . _level_dict [ \"sql_condition\" ] self . _is_null_level = self . _level_dict_val_else_default ( \"is_null_level\" ) self . _tf_adjustment_weight = self . _level_dict_val_else_default ( \"tf_adjustment_weight\" ) self . _tf_minimum_u_value = self . _level_dict_val_else_default ( \"tf_minimum_u_value\" ) # Private values controlled with getter/setter self . _m_probability = self . _level_dict . get ( \"m_probability\" ) self . _u_probability = self . _level_dict . get ( \"u_probability\" ) # These will be set when the ComparisonLevel is passed into a Comparison self . _comparison_vector_value : int = None self . _max_level : bool = None # Enable the level to 'know' when it's been trained self . _trained_m_probabilities : list = [] self . _trained_u_probabilities : list = [] self . _validate () @property def is_null_level ( self ) -> bool : return self . _is_null_level @property def sql_condition ( self ) -> str : return self . _sql_condition def _level_dict_val_else_default ( self , key ): val = self . _level_dict . get ( key ) if not val : val = default_value_from_schema ( key , \"comparison_level\" ) return val @property def _tf_adjustment_input_column ( self ): val = self . _level_dict_val_else_default ( \"tf_adjustment_column\" ) if val : return InputColumn ( val , sql_dialect = self . _sql_dialect ) else : return None @property def _tf_adjustment_input_column_name ( self ): input_column = self . _tf_adjustment_input_column if input_column : return input_column . unquote () . name () @property def _has_comparison ( self ): from .comparison import Comparison return isinstance ( self . comparison , Comparison ) @property def m_probability ( self ): if self . is_null_level : return None if self . _m_probability == LEVEL_NOT_OBSERVED_TEXT : return 1e-6 if self . _m_probability is None and self . _has_comparison : vals = _default_m_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _m_probability @m_probability . setter def m_probability ( self , value ): if self . is_null_level : raise AttributeError ( \"Cannot set m_probability when is_null_level is true\" ) if value == LEVEL_NOT_OBSERVED_TEXT : cc_n = self . comparison . _output_column_name cl_n = self . label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train m value\" ) self . _m_probability = value @property def u_probability ( self ): if self . is_null_level : return None if self . _u_probability == LEVEL_NOT_OBSERVED_TEXT : return 1e-6 if self . _u_probability is None : vals = _default_u_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _u_probability @u_probability . setter def u_probability ( self , value ): if self . is_null_level : raise AttributeError ( \"Cannot set u_probability when is_null_level is true\" ) if value == LEVEL_NOT_OBSERVED_TEXT : cc_n = self . comparison . _output_column_name cl_n = self . label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train u value\" ) self . _u_probability = value @property def _m_probability_description ( self ): if self . m_probability is not None : return ( \"Amongst matching record comparisons, \" f \" { self . m_probability : .2% } of records are in the \" f \" { self . label_for_charts . lower () } comparison level\" ) @property def _u_probability_description ( self ): if self . u_probability is not None : return ( \"Amongst non-matching record comparisons, \" f \" { self . u_probability : .2% } of records are in the \" f \" { self . label_for_charts . lower () } comparison level\" ) def _add_trained_u_probability ( self , val , desc = \"no description given\" ): self . _trained_u_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"u\" } ) def _add_trained_m_probability ( self , val , desc = \"no description given\" ): self . _trained_m_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"m\" } ) @property def _has_estimated_u_values ( self ): if self . is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_m_values ( self ): if self . is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_values ( self ): return self . _has_estimated_m_values and self . _has_estimated_u_values @property def _trained_m_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _trained_u_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _m_is_trained ( self ): if self . is_null_level : return True if self . _m_probability == \"level not observed in data\" : return False if self . _m_probability is None : return False return True @property def _u_is_trained ( self ): if self . is_null_level : return True if self . _u_probability == \"level not observed in data\" : return False if self . _u_probability is None : return False return True @property def _is_trained ( self ): return self . _m_is_trained and self . _u_is_trained @property def _bayes_factor ( self ): if self . is_null_level : return 1.0 if self . m_probability is None or self . u_probability is None : return None elif self . u_probability == 0 : return math . inf else : return self . m_probability / self . u_probability @property def _log2_bayes_factor ( self ): if self . is_null_level : return 0.0 else : return math . log2 ( self . _bayes_factor ) @property def _bayes_factor_description ( self ): text = ( f \"If comparison level is ` { self . label_for_charts . lower () } ` \" \"then comparison is\" ) if self . _bayes_factor == math . inf : return f \" { text } certain to be a match\" elif self . _bayes_factor == 0.0 : return f \" { text } impossible to be a match\" elif self . _bayes_factor >= 1.0 : return f \" { text } { self . _bayes_factor : ,.2f } times more likely to be a match\" else : mult = 1 / self . _bayes_factor return f \" { text } { mult : ,.2f } times less likely to be a match\" @property def label_for_charts ( self ): return self . _level_dict . get ( \"label_for_charts\" , str ( self . _comparison_vector_value ) ) @property def _label_for_charts_no_duplicates ( self ): if self . _has_comparison : labels = [] for cl in self . comparison . comparison_levels : labels . append ( cl . label_for_charts ) if len ( labels ) == len ( set ( labels )): return self . label_for_charts # Make label unique cvv = str ( self . _comparison_vector_value ) label = self . _level_dict [ \"label_for_charts\" ] return f \" { cvv } . { label } \" @property def _is_else_level ( self ): if self . sql_condition . strip () . upper () == \"ELSE\" : return True @property def _has_tf_adjustments ( self ): col = self . _level_dict . get ( \"tf_adjustment_column\" ) return col is not None def _validate_sql ( self ): sql = self . sql_condition if self . _is_else_level : return True dialect = self . _sql_dialect # TODO: really self._sql_dialect should always be set, something gets # messed up during the deepcopy()ing of a Comparison if dialect is None : dialect = \"spark\" try : sqlglot . parse_one ( sql , read = dialect ) except sqlglot . ParseError as e : raise ValueError ( f \"Error parsing sql_statement: \\n { sql } \" ) from e return True @property def _input_columns_used_by_sql_condition ( self ) -> list [ InputColumn ]: # returns e.g. InputColumn(first_name), InputColumn(surname) if self . _is_else_level : return [] cols = get_columns_used_from_sql ( self . sql_condition , dialect = self . _sql_dialect ) # Parsed order seems to be roughly in reverse order of apearance cols = cols [:: - 1 ] cols = [ re . sub ( r \"_L$|_R$\" , \"\" , c , flags = re . IGNORECASE ) for c in cols ] cols = dedupe_preserving_order ( cols ) input_cols = [] for c in cols : # We could have tf adjustments for surname on a dmeta_surname column # If so, we want to set the tf adjustments against the surname col, # not the dmeta_surname one input_cols . append ( InputColumn ( c , sql_dialect = self . _sql_dialect )) return input_cols @property def _columns_to_select_for_blocking ( self ): # e.g. l.first_name as first_name_l, r.first_name as first_name_r output_cols = [] cols = self . _input_columns_used_by_sql_condition for c in cols : output_cols . extend ( c . l_r_names_as_l_r ()) if self . _tf_adjustment_input_column : output_cols . extend ( self . _tf_adjustment_input_column . l_r_tf_names_as_l_r () ) return dedupe_preserving_order ( output_cols ) @property def _when_then_comparison_vector_value_sql ( self ): # e.g. when first_name_l = first_name_r then 1 if not hasattr ( self , \"_comparison_vector_value\" ): raise ValueError ( \"Cannot get the 'when .. then ...' sql expression because \" \"this comparison level does not belong to a parent Comparison. \" \"The comparison_vector_value is only defined in the \" \"context of a list of ComparisonLevels within a Comparison.\" ) if self . _is_else_level : return f \" { self . sql_condition } { self . _comparison_vector_value } \" else : return f \"WHEN { self . sql_condition } THEN { self . _comparison_vector_value } \" @property def _is_exact_match ( self ): if self . _is_else_level : return False sql_syntax_tree = sqlglot . parse_one ( self . sql_condition . lower (), read = self . _sql_dialect ) sql_cnf = normalize ( sql_syntax_tree ) exprs = _get_and_subclauses ( sql_cnf ) for expr in exprs : if not _is_exact_match ( expr ): return False return True @property def _exact_match_colnames ( self ): sql_syntax_tree = sqlglot . parse_one ( self . sql_condition . lower (), read = self . _sql_dialect ) sql_cnf = normalize ( sql_syntax_tree ) exprs = _get_and_subclauses ( sql_cnf ) for expr in exprs : if not _is_exact_match ( expr ): raise ValueError ( \"sql_cond not an exact match so can't get exact match column name\" ) cols = [] for expr in exprs : col = _exact_match_colname ( expr ) cols . append ( col ) return cols @property def _u_probability_corresponding_to_exact_match ( self ): levels = self . comparison . comparison_levels # Find a level with a single exact match colname # which is equal to the tf adjustment input colname for level in levels : if not level . _is_exact_match : continue colnames = level . _exact_match_colnames if len ( colnames ) != 1 : continue if colnames [ 0 ] == self . _tf_adjustment_input_column_name . lower (): return level . u_probability raise ValueError ( \"Could not find an exact match level for \" f \" { self . _tf_adjustment_input_column_name } .\" \" \\n An exact match level is required to make a term frequency adjustment \" \"on a comparison level that is not an exact match.\" ) @property def _bayes_factor_sql ( self ): bayes_factor = ( self . _bayes_factor if self . _bayes_factor != math . inf else \"'Infinity'\" ) sql = f \"\"\" WHEN { self . comparison . _gamma_column_name } = { self . _comparison_vector_value } THEN cast( { bayes_factor } as double) \"\"\" return dedent ( sql ) @property def _tf_adjustment_sql ( self ): gamma_column_name = self . comparison . _gamma_column_name gamma_colname_value_is_this_level = ( f \" { gamma_column_name } = { self . _comparison_vector_value } \" ) # A tf adjustment of 1D is a multiplier of 1.0, i.e. no adjustment if self . _comparison_vector_value == - 1 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif not self . _has_tf_adjustments : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _tf_adjustment_weight == 0 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _is_else_level : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" else : tf_adj_col = self . _tf_adjustment_input_column coalesce_l_r = ( f \"coalesce( { tf_adj_col . tf_name_l () } , { tf_adj_col . tf_name_r () } )\" ) coalesce_r_l = ( f \"coalesce( { tf_adj_col . tf_name_r () } , { tf_adj_col . tf_name_l () } )\" ) tf_adjustment_exists = f \" { coalesce_l_r } is not null\" u_prob_exact_match = self . _u_probability_corresponding_to_exact_match # Using coalesce protects against one of the tf adjustments being null # Which would happen if the user provided their own tf adjustment table # That didn't contain some of the values in this data # In this case rather than taking the greater of the two, we take # whichever value exists if self . _tf_minimum_u_value == 0.0 : divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } THEN { coalesce_l_r } ELSE { coalesce_r_l } END) \"\"\" else : # This sql works correctly even when the tf_minimum_u_value is 0.0 # but is less efficient to execute, hence the above if statement divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } AND { coalesce_l_r } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_l_r } WHEN { coalesce_r_l } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_r_l } ELSE cast( { self . _tf_minimum_u_value } as double) END) \"\"\" sql = f \"\"\" WHEN { gamma_colname_value_is_this_level } then (CASE WHEN { tf_adjustment_exists } THEN POW( cast( { u_prob_exact_match } as double) / { divisor_sql } , cast( { self . _tf_adjustment_weight } as double) ) ELSE cast(1 as double) END) \"\"\" return dedent ( sql ) . strip () def as_dict ( self ): \"The minimal representation of this level to use as an input to Splink\" output = {} output [ \"sql_condition\" ] = self . sql_condition if self . _level_dict . get ( \"label_for_charts\" ): output [ \"label_for_charts\" ] = self . label_for_charts if self . _m_probability and self . _m_is_trained : output [ \"m_probability\" ] = self . m_probability if self . _u_probability and self . _u_is_trained : output [ \"u_probability\" ] = self . u_probability if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name if self . _tf_adjustment_weight != 0 : output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight if self . is_null_level : output [ \"is_null_level\" ] = True return output def _as_completed_dict ( self ): comp_dict = self . as_dict () comp_dict [ \"comparison_vector_value\" ] = self . _comparison_vector_value return comp_dict @property def _as_detailed_record ( self ): \"A detailed representation of this level to describe it in charting outputs\" output = {} output [ \"sql_condition\" ] = self . sql_condition output [ \"label_for_charts\" ] = self . _label_for_charts_no_duplicates output [ \"m_probability\" ] = self . m_probability output [ \"u_probability\" ] = self . u_probability output [ \"m_probability_description\" ] = self . _m_probability_description output [ \"u_probability_description\" ] = self . _u_probability_description output [ \"has_tf_adjustments\" ] = self . _has_tf_adjustments if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name else : output [ \"tf_adjustment_column\" ] = None output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight output [ \"is_null_level\" ] = self . is_null_level output [ \"bayes_factor\" ] = self . _bayes_factor output [ \"log2_bayes_factor\" ] = self . _log2_bayes_factor output [ \"comparison_vector_value\" ] = self . _comparison_vector_value output [ \"max_comparison_vector_value\" ] = self . comparison . _num_levels - 1 output [ \"bayes_factor_description\" ] = self . _bayes_factor_description return output @property def _parameter_estimates_as_records ( self ): output_records = [] cl_record = self . _as_detailed_record trained_values = self . _trained_u_probabilities + self . _trained_m_probabilities for trained_value in trained_values : record = {} record [ \"m_or_u\" ] = trained_value [ \"m_or_u\" ] p = trained_value [ \"probability\" ] record [ \"estimated_probability\" ] = p record [ \"estimate_description\" ] = trained_value [ \"description\" ] if p is not None and p != LEVEL_NOT_OBSERVED_TEXT and p > 0.0 and p < 1.0 : record [ \"estimated_probability_as_log_odds\" ] = math . log2 ( p / ( 1 - p )) else : record [ \"estimated_probability_as_log_odds\" ] = None record [ \"sql_condition\" ] = cl_record [ \"sql_condition\" ] record [ \"comparison_level_label\" ] = cl_record [ \"label_for_charts\" ] record [ \"comparison_vector_value\" ] = cl_record [ \"comparison_vector_value\" ] output_records . append ( record ) return output_records def _validate ( self ): self . _validate_sql () def _abbreviated_sql ( self , cutoff = 75 ): sql = self . sql_condition return ( sql [: 75 ] + \"...\" ) if len ( sql ) > 75 else sql def __repr__ ( self ): return f \"< { self . _human_readable_succinct } >\" @property def _human_readable_succinct ( self ): sql = self . _abbreviated_sql ( 75 ) return f \"Comparison level ' { self . label_for_charts } ' using SQL rule: { sql } \" @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name () for c in self . _input_columns_used_by_sql_condition ] ) desc = ( f \"Comparison level: { self . label_for_charts } of { input_cols } \\n \" \"Assesses similarity between pairwise comparisons of the input columns \" f \"using the following rule \\n { self . sql_condition } \" ) return desc m_probability writable property \u00b6 u_probability writable property \u00b6 __init__ ( level_dict , comparison = None , sql_dialect = None ) \u00b6 Source code in splink/comparison_level.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , level_dict , comparison : Comparison = None , sql_dialect : str = None , ): # Protected, because we don't want to modify the original dict self . _level_dict = level_dict self . comparison : Comparison = comparison if not hasattr ( self , \"_sql_dialect\" ): self . _sql_dialect = sql_dialect self . _sql_condition = self . _level_dict [ \"sql_condition\" ] self . _is_null_level = self . _level_dict_val_else_default ( \"is_null_level\" ) self . _tf_adjustment_weight = self . _level_dict_val_else_default ( \"tf_adjustment_weight\" ) self . _tf_minimum_u_value = self . _level_dict_val_else_default ( \"tf_minimum_u_value\" ) # Private values controlled with getter/setter self . _m_probability = self . _level_dict . get ( \"m_probability\" ) self . _u_probability = self . _level_dict . get ( \"u_probability\" ) # These will be set when the ComparisonLevel is passed into a Comparison self . _comparison_vector_value : int = None self . _max_level : bool = None # Enable the level to 'know' when it's been trained self . _trained_m_probabilities : list = [] self . _trained_u_probabilities : list = [] self . _validate ()","title":"Comparison Level"},{"location":"comparison_level.html#documentation-for-comparisonlevel-object","text":"Each ComparisonLevel defines a gradation (category) of similarity within a Comparison . For example, a Comparison that uses the first_name and surname columns may define three ComparisonLevel s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. Source code in splink/comparison_level.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 class ComparisonLevel : \"\"\"Each ComparisonLevel defines a gradation (category) of similarity within a `Comparison`. For example, a `Comparison` that uses the first_name and surname columns may define three `ComparisonLevel`s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: ``` Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. ``` \"\"\" def __init__ ( self , level_dict , comparison : Comparison = None , sql_dialect : str = None , ): # Protected, because we don't want to modify the original dict self . _level_dict = level_dict self . comparison : Comparison = comparison if not hasattr ( self , \"_sql_dialect\" ): self . _sql_dialect = sql_dialect self . _sql_condition = self . _level_dict [ \"sql_condition\" ] self . _is_null_level = self . _level_dict_val_else_default ( \"is_null_level\" ) self . _tf_adjustment_weight = self . _level_dict_val_else_default ( \"tf_adjustment_weight\" ) self . _tf_minimum_u_value = self . _level_dict_val_else_default ( \"tf_minimum_u_value\" ) # Private values controlled with getter/setter self . _m_probability = self . _level_dict . get ( \"m_probability\" ) self . _u_probability = self . _level_dict . get ( \"u_probability\" ) # These will be set when the ComparisonLevel is passed into a Comparison self . _comparison_vector_value : int = None self . _max_level : bool = None # Enable the level to 'know' when it's been trained self . _trained_m_probabilities : list = [] self . _trained_u_probabilities : list = [] self . _validate () @property def is_null_level ( self ) -> bool : return self . _is_null_level @property def sql_condition ( self ) -> str : return self . _sql_condition def _level_dict_val_else_default ( self , key ): val = self . _level_dict . get ( key ) if not val : val = default_value_from_schema ( key , \"comparison_level\" ) return val @property def _tf_adjustment_input_column ( self ): val = self . _level_dict_val_else_default ( \"tf_adjustment_column\" ) if val : return InputColumn ( val , sql_dialect = self . _sql_dialect ) else : return None @property def _tf_adjustment_input_column_name ( self ): input_column = self . _tf_adjustment_input_column if input_column : return input_column . unquote () . name () @property def _has_comparison ( self ): from .comparison import Comparison return isinstance ( self . comparison , Comparison ) @property def m_probability ( self ): if self . is_null_level : return None if self . _m_probability == LEVEL_NOT_OBSERVED_TEXT : return 1e-6 if self . _m_probability is None and self . _has_comparison : vals = _default_m_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _m_probability @m_probability . setter def m_probability ( self , value ): if self . is_null_level : raise AttributeError ( \"Cannot set m_probability when is_null_level is true\" ) if value == LEVEL_NOT_OBSERVED_TEXT : cc_n = self . comparison . _output_column_name cl_n = self . label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train m value\" ) self . _m_probability = value @property def u_probability ( self ): if self . is_null_level : return None if self . _u_probability == LEVEL_NOT_OBSERVED_TEXT : return 1e-6 if self . _u_probability is None : vals = _default_u_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _u_probability @u_probability . setter def u_probability ( self , value ): if self . is_null_level : raise AttributeError ( \"Cannot set u_probability when is_null_level is true\" ) if value == LEVEL_NOT_OBSERVED_TEXT : cc_n = self . comparison . _output_column_name cl_n = self . label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train u value\" ) self . _u_probability = value @property def _m_probability_description ( self ): if self . m_probability is not None : return ( \"Amongst matching record comparisons, \" f \" { self . m_probability : .2% } of records are in the \" f \" { self . label_for_charts . lower () } comparison level\" ) @property def _u_probability_description ( self ): if self . u_probability is not None : return ( \"Amongst non-matching record comparisons, \" f \" { self . u_probability : .2% } of records are in the \" f \" { self . label_for_charts . lower () } comparison level\" ) def _add_trained_u_probability ( self , val , desc = \"no description given\" ): self . _trained_u_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"u\" } ) def _add_trained_m_probability ( self , val , desc = \"no description given\" ): self . _trained_m_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"m\" } ) @property def _has_estimated_u_values ( self ): if self . is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_m_values ( self ): if self . is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_values ( self ): return self . _has_estimated_m_values and self . _has_estimated_u_values @property def _trained_m_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _trained_u_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _m_is_trained ( self ): if self . is_null_level : return True if self . _m_probability == \"level not observed in data\" : return False if self . _m_probability is None : return False return True @property def _u_is_trained ( self ): if self . is_null_level : return True if self . _u_probability == \"level not observed in data\" : return False if self . _u_probability is None : return False return True @property def _is_trained ( self ): return self . _m_is_trained and self . _u_is_trained @property def _bayes_factor ( self ): if self . is_null_level : return 1.0 if self . m_probability is None or self . u_probability is None : return None elif self . u_probability == 0 : return math . inf else : return self . m_probability / self . u_probability @property def _log2_bayes_factor ( self ): if self . is_null_level : return 0.0 else : return math . log2 ( self . _bayes_factor ) @property def _bayes_factor_description ( self ): text = ( f \"If comparison level is ` { self . label_for_charts . lower () } ` \" \"then comparison is\" ) if self . _bayes_factor == math . inf : return f \" { text } certain to be a match\" elif self . _bayes_factor == 0.0 : return f \" { text } impossible to be a match\" elif self . _bayes_factor >= 1.0 : return f \" { text } { self . _bayes_factor : ,.2f } times more likely to be a match\" else : mult = 1 / self . _bayes_factor return f \" { text } { mult : ,.2f } times less likely to be a match\" @property def label_for_charts ( self ): return self . _level_dict . get ( \"label_for_charts\" , str ( self . _comparison_vector_value ) ) @property def _label_for_charts_no_duplicates ( self ): if self . _has_comparison : labels = [] for cl in self . comparison . comparison_levels : labels . append ( cl . label_for_charts ) if len ( labels ) == len ( set ( labels )): return self . label_for_charts # Make label unique cvv = str ( self . _comparison_vector_value ) label = self . _level_dict [ \"label_for_charts\" ] return f \" { cvv } . { label } \" @property def _is_else_level ( self ): if self . sql_condition . strip () . upper () == \"ELSE\" : return True @property def _has_tf_adjustments ( self ): col = self . _level_dict . get ( \"tf_adjustment_column\" ) return col is not None def _validate_sql ( self ): sql = self . sql_condition if self . _is_else_level : return True dialect = self . _sql_dialect # TODO: really self._sql_dialect should always be set, something gets # messed up during the deepcopy()ing of a Comparison if dialect is None : dialect = \"spark\" try : sqlglot . parse_one ( sql , read = dialect ) except sqlglot . ParseError as e : raise ValueError ( f \"Error parsing sql_statement: \\n { sql } \" ) from e return True @property def _input_columns_used_by_sql_condition ( self ) -> list [ InputColumn ]: # returns e.g. InputColumn(first_name), InputColumn(surname) if self . _is_else_level : return [] cols = get_columns_used_from_sql ( self . sql_condition , dialect = self . _sql_dialect ) # Parsed order seems to be roughly in reverse order of apearance cols = cols [:: - 1 ] cols = [ re . sub ( r \"_L$|_R$\" , \"\" , c , flags = re . IGNORECASE ) for c in cols ] cols = dedupe_preserving_order ( cols ) input_cols = [] for c in cols : # We could have tf adjustments for surname on a dmeta_surname column # If so, we want to set the tf adjustments against the surname col, # not the dmeta_surname one input_cols . append ( InputColumn ( c , sql_dialect = self . _sql_dialect )) return input_cols @property def _columns_to_select_for_blocking ( self ): # e.g. l.first_name as first_name_l, r.first_name as first_name_r output_cols = [] cols = self . _input_columns_used_by_sql_condition for c in cols : output_cols . extend ( c . l_r_names_as_l_r ()) if self . _tf_adjustment_input_column : output_cols . extend ( self . _tf_adjustment_input_column . l_r_tf_names_as_l_r () ) return dedupe_preserving_order ( output_cols ) @property def _when_then_comparison_vector_value_sql ( self ): # e.g. when first_name_l = first_name_r then 1 if not hasattr ( self , \"_comparison_vector_value\" ): raise ValueError ( \"Cannot get the 'when .. then ...' sql expression because \" \"this comparison level does not belong to a parent Comparison. \" \"The comparison_vector_value is only defined in the \" \"context of a list of ComparisonLevels within a Comparison.\" ) if self . _is_else_level : return f \" { self . sql_condition } { self . _comparison_vector_value } \" else : return f \"WHEN { self . sql_condition } THEN { self . _comparison_vector_value } \" @property def _is_exact_match ( self ): if self . _is_else_level : return False sql_syntax_tree = sqlglot . parse_one ( self . sql_condition . lower (), read = self . _sql_dialect ) sql_cnf = normalize ( sql_syntax_tree ) exprs = _get_and_subclauses ( sql_cnf ) for expr in exprs : if not _is_exact_match ( expr ): return False return True @property def _exact_match_colnames ( self ): sql_syntax_tree = sqlglot . parse_one ( self . sql_condition . lower (), read = self . _sql_dialect ) sql_cnf = normalize ( sql_syntax_tree ) exprs = _get_and_subclauses ( sql_cnf ) for expr in exprs : if not _is_exact_match ( expr ): raise ValueError ( \"sql_cond not an exact match so can't get exact match column name\" ) cols = [] for expr in exprs : col = _exact_match_colname ( expr ) cols . append ( col ) return cols @property def _u_probability_corresponding_to_exact_match ( self ): levels = self . comparison . comparison_levels # Find a level with a single exact match colname # which is equal to the tf adjustment input colname for level in levels : if not level . _is_exact_match : continue colnames = level . _exact_match_colnames if len ( colnames ) != 1 : continue if colnames [ 0 ] == self . _tf_adjustment_input_column_name . lower (): return level . u_probability raise ValueError ( \"Could not find an exact match level for \" f \" { self . _tf_adjustment_input_column_name } .\" \" \\n An exact match level is required to make a term frequency adjustment \" \"on a comparison level that is not an exact match.\" ) @property def _bayes_factor_sql ( self ): bayes_factor = ( self . _bayes_factor if self . _bayes_factor != math . inf else \"'Infinity'\" ) sql = f \"\"\" WHEN { self . comparison . _gamma_column_name } = { self . _comparison_vector_value } THEN cast( { bayes_factor } as double) \"\"\" return dedent ( sql ) @property def _tf_adjustment_sql ( self ): gamma_column_name = self . comparison . _gamma_column_name gamma_colname_value_is_this_level = ( f \" { gamma_column_name } = { self . _comparison_vector_value } \" ) # A tf adjustment of 1D is a multiplier of 1.0, i.e. no adjustment if self . _comparison_vector_value == - 1 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif not self . _has_tf_adjustments : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _tf_adjustment_weight == 0 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _is_else_level : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" else : tf_adj_col = self . _tf_adjustment_input_column coalesce_l_r = ( f \"coalesce( { tf_adj_col . tf_name_l () } , { tf_adj_col . tf_name_r () } )\" ) coalesce_r_l = ( f \"coalesce( { tf_adj_col . tf_name_r () } , { tf_adj_col . tf_name_l () } )\" ) tf_adjustment_exists = f \" { coalesce_l_r } is not null\" u_prob_exact_match = self . _u_probability_corresponding_to_exact_match # Using coalesce protects against one of the tf adjustments being null # Which would happen if the user provided their own tf adjustment table # That didn't contain some of the values in this data # In this case rather than taking the greater of the two, we take # whichever value exists if self . _tf_minimum_u_value == 0.0 : divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } THEN { coalesce_l_r } ELSE { coalesce_r_l } END) \"\"\" else : # This sql works correctly even when the tf_minimum_u_value is 0.0 # but is less efficient to execute, hence the above if statement divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } AND { coalesce_l_r } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_l_r } WHEN { coalesce_r_l } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_r_l } ELSE cast( { self . _tf_minimum_u_value } as double) END) \"\"\" sql = f \"\"\" WHEN { gamma_colname_value_is_this_level } then (CASE WHEN { tf_adjustment_exists } THEN POW( cast( { u_prob_exact_match } as double) / { divisor_sql } , cast( { self . _tf_adjustment_weight } as double) ) ELSE cast(1 as double) END) \"\"\" return dedent ( sql ) . strip () def as_dict ( self ): \"The minimal representation of this level to use as an input to Splink\" output = {} output [ \"sql_condition\" ] = self . sql_condition if self . _level_dict . get ( \"label_for_charts\" ): output [ \"label_for_charts\" ] = self . label_for_charts if self . _m_probability and self . _m_is_trained : output [ \"m_probability\" ] = self . m_probability if self . _u_probability and self . _u_is_trained : output [ \"u_probability\" ] = self . u_probability if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name if self . _tf_adjustment_weight != 0 : output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight if self . is_null_level : output [ \"is_null_level\" ] = True return output def _as_completed_dict ( self ): comp_dict = self . as_dict () comp_dict [ \"comparison_vector_value\" ] = self . _comparison_vector_value return comp_dict @property def _as_detailed_record ( self ): \"A detailed representation of this level to describe it in charting outputs\" output = {} output [ \"sql_condition\" ] = self . sql_condition output [ \"label_for_charts\" ] = self . _label_for_charts_no_duplicates output [ \"m_probability\" ] = self . m_probability output [ \"u_probability\" ] = self . u_probability output [ \"m_probability_description\" ] = self . _m_probability_description output [ \"u_probability_description\" ] = self . _u_probability_description output [ \"has_tf_adjustments\" ] = self . _has_tf_adjustments if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name else : output [ \"tf_adjustment_column\" ] = None output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight output [ \"is_null_level\" ] = self . is_null_level output [ \"bayes_factor\" ] = self . _bayes_factor output [ \"log2_bayes_factor\" ] = self . _log2_bayes_factor output [ \"comparison_vector_value\" ] = self . _comparison_vector_value output [ \"max_comparison_vector_value\" ] = self . comparison . _num_levels - 1 output [ \"bayes_factor_description\" ] = self . _bayes_factor_description return output @property def _parameter_estimates_as_records ( self ): output_records = [] cl_record = self . _as_detailed_record trained_values = self . _trained_u_probabilities + self . _trained_m_probabilities for trained_value in trained_values : record = {} record [ \"m_or_u\" ] = trained_value [ \"m_or_u\" ] p = trained_value [ \"probability\" ] record [ \"estimated_probability\" ] = p record [ \"estimate_description\" ] = trained_value [ \"description\" ] if p is not None and p != LEVEL_NOT_OBSERVED_TEXT and p > 0.0 and p < 1.0 : record [ \"estimated_probability_as_log_odds\" ] = math . log2 ( p / ( 1 - p )) else : record [ \"estimated_probability_as_log_odds\" ] = None record [ \"sql_condition\" ] = cl_record [ \"sql_condition\" ] record [ \"comparison_level_label\" ] = cl_record [ \"label_for_charts\" ] record [ \"comparison_vector_value\" ] = cl_record [ \"comparison_vector_value\" ] output_records . append ( record ) return output_records def _validate ( self ): self . _validate_sql () def _abbreviated_sql ( self , cutoff = 75 ): sql = self . sql_condition return ( sql [: 75 ] + \"...\" ) if len ( sql ) > 75 else sql def __repr__ ( self ): return f \"< { self . _human_readable_succinct } >\" @property def _human_readable_succinct ( self ): sql = self . _abbreviated_sql ( 75 ) return f \"Comparison level ' { self . label_for_charts } ' using SQL rule: { sql } \" @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name () for c in self . _input_columns_used_by_sql_condition ] ) desc = ( f \"Comparison level: { self . label_for_charts } of { input_cols } \\n \" \"Assesses similarity between pairwise comparisons of the input columns \" f \"using the following rule \\n { self . sql_condition } \" ) return desc","title":"Documentation for ComparisonLevel object"},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.m_probability","text":"","title":"m_probability"},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.u_probability","text":"","title":"u_probability"},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.__init__","text":"Source code in splink/comparison_level.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self , level_dict , comparison : Comparison = None , sql_dialect : str = None , ): # Protected, because we don't want to modify the original dict self . _level_dict = level_dict self . comparison : Comparison = comparison if not hasattr ( self , \"_sql_dialect\" ): self . _sql_dialect = sql_dialect self . _sql_condition = self . _level_dict [ \"sql_condition\" ] self . _is_null_level = self . _level_dict_val_else_default ( \"is_null_level\" ) self . _tf_adjustment_weight = self . _level_dict_val_else_default ( \"tf_adjustment_weight\" ) self . _tf_minimum_u_value = self . _level_dict_val_else_default ( \"tf_minimum_u_value\" ) # Private values controlled with getter/setter self . _m_probability = self . _level_dict . get ( \"m_probability\" ) self . _u_probability = self . _level_dict . get ( \"u_probability\" ) # These will be set when the ComparisonLevel is passed into a Comparison self . _comparison_vector_value : int = None self . _max_level : bool = None # Enable the level to 'know' when it's been trained self . _trained_m_probabilities : list = [] self . _trained_u_probabilities : list = [] self . _validate ()","title":"__init__()"},{"location":"comparison_level_composition.html","tags":["API","comparisons"],"text":"Documentation for comparison_level_composition functions \u00b6 comparison_composition allows the merging of existing comparison levels by a logical SQL clause - OR , AND or NOT . This extends the functionality of our base comparison levels by allowing users to \"join\" existing comparisons by various SQL clauses. For example, or_(null_level(\"first_name\"), null_level(\"surname\")) creates a check for nulls in either first_name or surname , rather than restricting the user to a single column. The detailed API for each of these are outlined below. Library comparison composition APIs \u00b6 and_ ( * clls , label_for_charts = None , m_probability = None , is_null_level = None ) \u00b6 Merge ComparisonLevels using logical \"AND\". Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"AND\". By default, we generate a new label_for_charts for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments. Parameters: Name Type Description Default *clls ComparisonLevel | dict ComparisonLevels or comparison level dictionaries to merge () label_for_charts str A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - label_1 AND label_2 None m_probability float Starting value for m probability. Defaults to None. None is_null_level bool If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None. None Examples: >>> # Simple null level composition with an `AND` clause >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> cll . and_ ( cll . null_level ( \"first_name\" ), cll . null_level ( \"surname\" )) >>> # Composing a levenshtein level with a custom `contains` level >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> misspelling = cll . levenshtein_level ( \"name\" , 1 ) >>> contains = { >>> \"sql_condition\" : \"(contains(name_l, name_r) OR \" >>> \"contains(name_r, name_l))\" >>> } >>> merged = cll . and_ ( misspelling , contains , label_for_charts = \"Spelling error\" ) >>> merged . as_dict () >>> { >>> 'sql_condition' : '(levenshtein(\"name_l\", \"name_r\") <= 1) ' >>> 'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))' , >>> 'label_for_charts' : 'Spelling error' >>> } Returns: Name Type Description ComparisonLevel ComparisonLevel A new ComparisonLevel with the merged SQL condition or_ ( * clls , label_for_charts = None , m_probability = None , is_null_level = None ) \u00b6 Merge ComparisonLevels using logical \"OR\". Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"OR\". By default, we generate a new label_for_charts for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments. Parameters: Name Type Description Default *clls ComparisonLevel | dict ComparisonLevels or comparison level dictionaries to merge () label_for_charts str A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - label_1 OR label_2 None m_probability float Starting value for m probability. Defaults to None. None is_null_level bool If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None. None Examples: >>> # Simple null level composition with an `OR` clause >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> cll . or_ ( cll . null_level ( \"first_name\" ), cll . null_level ( \"surname\" )) >>> # Composing a levenshtein level with a custom `contains` level >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> misspelling = cll . levenshtein_level ( \"name\" , 1 ) >>> contains = { >>> \"sql_condition\" : \"(contains(name_l, name_r) OR \" >>> \"contains(name_r, name_l))\" >>> } >>> merged = cll . or_ ( misspelling , contains , label_for_charts = \"Spelling error\" ) >>> merged . as_dict () >>> { >>> 'sql_condition' : '(levenshtein(\"name_l\", \"name_r\") <= 1) ' >>> 'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))' , >>> 'label_for_charts' : 'Spelling error' >>> } Returns: Name Type Description ComparisonLevel ComparisonLevel A new ComparisonLevel with the merged SQL condition not_ ( cll , label_for_charts = None , m_probability = None ) \u00b6 Negate a ComparisonLevel. Returns a ComparisonLevel with the same SQL condition as the input, but prefixed with \"NOT\". By default, we generate a new label_for_charts for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments. Parameters: Name Type Description Default cll ComparisonLevel | dict ComparisonLevel or comparison level dictionary required label_for_charts str A label for this comparson level, which will appear on charts as a reminder of what the level represents. None m_probability float Starting value for m probability. Defaults to None. None Examples: >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> # *Not* a null on first name `first_name` >>> cll . not_ ( cll . exact_match ( \"first_name\" )) >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> # Find all exact matches *not* on the first of January >>> dob_first_jan = { >>> \"sql_condition\" : \"SUBSTR(dob_std_l, -5) = '01-01'\" , >>> \"label_for_charts\" : \"Date is 1st Jan\" , >>> } >>> exact_match_not_first_jan = cll . and_ ( >>> cll . exact_match_level ( \"dob\" ), >>> cll . not_ ( dob_first_jan ), >>> label_for_charts = \"Exact match and not the 1st Jan\" >>> ) Returns: Type Description ComparisonLevel ComparisonLevel A new ComparisonLevel with the negated SQL condition and label_for_charts","title":"Comparison Composition"},{"location":"comparison_level_composition.html#documentation-for-comparison_level_composition-functions","text":"comparison_composition allows the merging of existing comparison levels by a logical SQL clause - OR , AND or NOT . This extends the functionality of our base comparison levels by allowing users to \"join\" existing comparisons by various SQL clauses. For example, or_(null_level(\"first_name\"), null_level(\"surname\")) creates a check for nulls in either first_name or surname , rather than restricting the user to a single column. The detailed API for each of these are outlined below.","title":"Documentation for comparison_level_composition functions"},{"location":"comparison_level_composition.html#library-comparison-composition-apis","text":"","title":"Library comparison composition APIs"},{"location":"comparison_level_composition.html#splink.comparison_level_composition.and_","text":"Merge ComparisonLevels using logical \"AND\". Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"AND\". By default, we generate a new label_for_charts for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments. Parameters: Name Type Description Default *clls ComparisonLevel | dict ComparisonLevels or comparison level dictionaries to merge () label_for_charts str A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - label_1 AND label_2 None m_probability float Starting value for m probability. Defaults to None. None is_null_level bool If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None. None Examples: >>> # Simple null level composition with an `AND` clause >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> cll . and_ ( cll . null_level ( \"first_name\" ), cll . null_level ( \"surname\" )) >>> # Composing a levenshtein level with a custom `contains` level >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> misspelling = cll . levenshtein_level ( \"name\" , 1 ) >>> contains = { >>> \"sql_condition\" : \"(contains(name_l, name_r) OR \" >>> \"contains(name_r, name_l))\" >>> } >>> merged = cll . and_ ( misspelling , contains , label_for_charts = \"Spelling error\" ) >>> merged . as_dict () >>> { >>> 'sql_condition' : '(levenshtein(\"name_l\", \"name_r\") <= 1) ' >>> 'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))' , >>> 'label_for_charts' : 'Spelling error' >>> } Returns: Name Type Description ComparisonLevel ComparisonLevel A new ComparisonLevel with the merged SQL condition","title":"and_()"},{"location":"comparison_level_composition.html#splink.comparison_level_composition.or_","text":"Merge ComparisonLevels using logical \"OR\". Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"OR\". By default, we generate a new label_for_charts for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments. Parameters: Name Type Description Default *clls ComparisonLevel | dict ComparisonLevels or comparison level dictionaries to merge () label_for_charts str A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - label_1 OR label_2 None m_probability float Starting value for m probability. Defaults to None. None is_null_level bool If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None. None Examples: >>> # Simple null level composition with an `OR` clause >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> cll . or_ ( cll . null_level ( \"first_name\" ), cll . null_level ( \"surname\" )) >>> # Composing a levenshtein level with a custom `contains` level >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> misspelling = cll . levenshtein_level ( \"name\" , 1 ) >>> contains = { >>> \"sql_condition\" : \"(contains(name_l, name_r) OR \" >>> \"contains(name_r, name_l))\" >>> } >>> merged = cll . or_ ( misspelling , contains , label_for_charts = \"Spelling error\" ) >>> merged . as_dict () >>> { >>> 'sql_condition' : '(levenshtein(\"name_l\", \"name_r\") <= 1) ' >>> 'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))' , >>> 'label_for_charts' : 'Spelling error' >>> } Returns: Name Type Description ComparisonLevel ComparisonLevel A new ComparisonLevel with the merged SQL condition","title":"or_()"},{"location":"comparison_level_composition.html#splink.comparison_level_composition.not_","text":"Negate a ComparisonLevel. Returns a ComparisonLevel with the same SQL condition as the input, but prefixed with \"NOT\". By default, we generate a new label_for_charts for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments. Parameters: Name Type Description Default cll ComparisonLevel | dict ComparisonLevel or comparison level dictionary required label_for_charts str A label for this comparson level, which will appear on charts as a reminder of what the level represents. None m_probability float Starting value for m probability. Defaults to None. None Examples: >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> # *Not* a null on first name `first_name` >>> cll . not_ ( cll . exact_match ( \"first_name\" )) >>> import splink.duckdb.duckdb_comparison_level_library as cll >>> # Find all exact matches *not* on the first of January >>> dob_first_jan = { >>> \"sql_condition\" : \"SUBSTR(dob_std_l, -5) = '01-01'\" , >>> \"label_for_charts\" : \"Date is 1st Jan\" , >>> } >>> exact_match_not_first_jan = cll . and_ ( >>> cll . exact_match_level ( \"dob\" ), >>> cll . not_ ( dob_first_jan ), >>> label_for_charts = \"Exact match and not the 1st Jan\" >>> ) Returns: Type Description ComparisonLevel ComparisonLevel A new ComparisonLevel with the negated SQL condition and label_for_charts","title":"not_()"},{"location":"comparison_level_library.html","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"],"text":"Documentation for comparison_level_library \u00b6 The comparison_level_library contains pre-made comparison levels available for use to construct custom comparisons as described in this topic guide . However, not every comparison level is available for every Splink-compatible SQL backend . The pre-made Splink comparison levels available for each SQL dialect are as given in this table: spark duckdb athena sqlite array_intersect_level \u2713 \u2713 \u2713 columns_reversed_level \u2713 \u2713 \u2713 \u2713 datediff_level \u2713 \u2713 distance_function_level \u2713 \u2713 \u2713 \u2713 distance_in_km_level \u2713 \u2713 \u2713 else_level \u2713 \u2713 \u2713 \u2713 exact_match_level \u2713 \u2713 \u2713 \u2713 jaccard_level \u2713 \u2713 jaro_winkler_level \u2713 \u2713 levenshtein_level \u2713 \u2713 \u2713 null_level \u2713 \u2713 \u2713 \u2713 percentage_difference_level \u2713 \u2713 \u2713 \u2713 The detailed API for each of these are outlined below. Library comparison level APIs \u00b6 splink.comparison_level_library.NullLevelBase \u00b6 Bases: ComparisonLevel __init__ ( col_name ) \u00b6 Represents comparisons where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Parameters: Name Type Description Default col_name str Input column name required Returns: Name Type Description ComparisonLevel Comparison level splink.comparison_level_library.ExactMatchLevelBase \u00b6 Bases: ComparisonLevel __init__ ( col_name , m_probability = None , term_frequency_adjustments = False , include_colname_in_charts_label = False ) \u00b6 splink.comparison_level_library.ElseLevelBase \u00b6 Bases: ComparisonLevel __init__ ( m_probability = None ) \u00b6 splink.comparison_level_library.DistanceFunctionLevelBase \u00b6 Bases: ComparisonLevel __init__ ( col_name , distance_function_name , distance_threshold , higher_is_more_similar = True , m_probability = None ) \u00b6 Represents a comparison using a user-provided distance function, where the similarity Parameters: Name Type Description Default col_name str Input column name required distance_function_name str The name of the distance function required distance_threshold Union [ int , float ] The threshold to use to assess similarity required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True m_probability float Starting value for m probability Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level for a given distance function splink.comparison_level_library.LevenshteinLevelBase \u00b6 Bases: DistanceFunctionLevelBase __init__ ( col_name , distance_threshold , m_probability = None ) \u00b6 Represents a comparison using a levenshtein distance function, Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the levenshtein similarity splink.comparison_level_library.JaroWinklerLevelBase \u00b6 Bases: DistanceFunctionLevelBase __init__ ( col_name , distance_threshold , m_probability = None ) \u00b6 Represents a comparison using the jaro winkler distance function Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the jaro winkler similarity splink.comparison_level_library.JaccardLevelBase \u00b6 Bases: DistanceFunctionLevelBase __init__ ( col_name , distance_threshold , m_probability = None ) \u00b6 Represents a comparison using a jaccard distance function Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the jaccard similarity splink.comparison_level_library.ColumnsReversedLevelBase \u00b6 Bases: ComparisonLevel __init__ ( col_name_1 , col_name_2 , m_probability = None , tf_adjustment_column = None ) \u00b6 Represents a comparison where the columns are reversed. For example, if surname is in the forename field and vice versa Parameters: Name Type Description Default col_name_1 str First column, e.g. forename required col_name_2 str Second column, e.g. surname required m_probability float Starting value for m probability. Defaults to None. None tf_adjustment_column str Column to use for term frequency adjustments if an exact match is observed. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the exact match of two columns. splink.comparison_level_library.DistanceInKMLevelBase \u00b6 Bases: ComparisonLevel __init__ ( lat_col , long_col , km_threshold , not_null = False , m_probability = None ) \u00b6 Use the haversine formula to transform comparisons of lat,lngs into distances measured in kilometers Parameters: Name Type Description Default lat_col str The name of a latitude column or the respective array or struct column column containing the information For example: long_lat['lat'] or long_lat[0] required long_col str The name of a longitudinal column or the respective array or struct column column containing the information, plus an index. For example: long_lat['long'] or long_lat[1] required km_threshold int The total distance in kilometers to evaluate your comparisons against required not_null bool If true, remove any . This is only necessary if you are not capturing nulls elsewhere in your comparison level. False m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the distance between two coordinates splink.comparison_level_library.PercentageDifferenceLevelBase \u00b6 Bases: ComparisonLevel __init__ ( col_name , percentage_distance_threshold , m_probability = None ) \u00b6 splink.comparison_level_library.ArrayIntersectLevelBase \u00b6 Bases: ComparisonLevel __init__ ( col_name , m_probability = None , term_frequency_adjustments = False , min_intersection = 1 , include_colname_in_charts_label = False ) \u00b6 Represents a comparison level based around the size of an intersection of arrays Parameters: Name Type Description Default col_name str Input column name required m_probability float Starting value for m probability. Defaults to None. None tf_adjustment_column str Column to use for term frequency adjustments if an exact match is observed. Defaults to None. required min_intersection int The minimum cardinality of the intersection of arrays for this comparison level. Defaults to 1 1 include_colname_in_charts_label bool Should the charts label contain the column name? Defaults to False False Returns: Name Type Description ComparisonLevel A comparison level that evaluates the size of intersection of arrays splink.comparison_level_library.DateDiffLevelBase \u00b6 Bases: ComparisonLevel __init__ ( date_col , date_threshold , date_metric = 'day' , m_probability = None ) \u00b6 Use the ... Parameters: Name Type Description Default date_col str Input column name required date_threshold int The total difference in time between two given dates. This is used in tandem with date_metric to determine . If you are using year as your metric, then a value of 1 would require that your dates lie within 1 year of one another. required date_metric str The unit of time with which to measure your date_threshold . Your metric should be one of day , month or year . Defaults to day . 'day' m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates whether two dates fall within a given interval.","title":"Comparison Level Library"},{"location":"comparison_level_library.html#documentation-for-comparison_level_library","text":"The comparison_level_library contains pre-made comparison levels available for use to construct custom comparisons as described in this topic guide . However, not every comparison level is available for every Splink-compatible SQL backend . The pre-made Splink comparison levels available for each SQL dialect are as given in this table: spark duckdb athena sqlite array_intersect_level \u2713 \u2713 \u2713 columns_reversed_level \u2713 \u2713 \u2713 \u2713 datediff_level \u2713 \u2713 distance_function_level \u2713 \u2713 \u2713 \u2713 distance_in_km_level \u2713 \u2713 \u2713 else_level \u2713 \u2713 \u2713 \u2713 exact_match_level \u2713 \u2713 \u2713 \u2713 jaccard_level \u2713 \u2713 jaro_winkler_level \u2713 \u2713 levenshtein_level \u2713 \u2713 \u2713 null_level \u2713 \u2713 \u2713 \u2713 percentage_difference_level \u2713 \u2713 \u2713 \u2713 The detailed API for each of these are outlined below.","title":"Documentation for comparison_level_library"},{"location":"comparison_level_library.html#library-comparison-level-apis","text":"","title":"Library comparison level APIs"},{"location":"comparison_level_library.html#splink.comparison_level_library.NullLevelBase","text":"Bases: ComparisonLevel","title":"NullLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.NullLevelBase.__init__","text":"Represents comparisons where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Parameters: Name Type Description Default col_name str Input column name required Returns: Name Type Description ComparisonLevel Comparison level","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.ExactMatchLevelBase","text":"Bases: ComparisonLevel","title":"ExactMatchLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.ExactMatchLevelBase.__init__","text":"","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.ElseLevelBase","text":"Bases: ComparisonLevel","title":"ElseLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.ElseLevelBase.__init__","text":"","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceFunctionLevelBase","text":"Bases: ComparisonLevel","title":"DistanceFunctionLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceFunctionLevelBase.__init__","text":"Represents a comparison using a user-provided distance function, where the similarity Parameters: Name Type Description Default col_name str Input column name required distance_function_name str The name of the distance function required distance_threshold Union [ int , float ] The threshold to use to assess similarity required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True m_probability float Starting value for m probability Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level for a given distance function","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.LevenshteinLevelBase","text":"Bases: DistanceFunctionLevelBase","title":"LevenshteinLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.LevenshteinLevelBase.__init__","text":"Represents a comparison using a levenshtein distance function, Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the levenshtein similarity","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroWinklerLevelBase","text":"Bases: DistanceFunctionLevelBase","title":"JaroWinklerLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroWinklerLevelBase.__init__","text":"Represents a comparison using the jaro winkler distance function Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the jaro winkler similarity","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.JaccardLevelBase","text":"Bases: DistanceFunctionLevelBase","title":"JaccardLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.JaccardLevelBase.__init__","text":"Represents a comparison using a jaccard distance function Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the jaccard similarity","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.ColumnsReversedLevelBase","text":"Bases: ComparisonLevel","title":"ColumnsReversedLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.ColumnsReversedLevelBase.__init__","text":"Represents a comparison where the columns are reversed. For example, if surname is in the forename field and vice versa Parameters: Name Type Description Default col_name_1 str First column, e.g. forename required col_name_2 str Second column, e.g. surname required m_probability float Starting value for m probability. Defaults to None. None tf_adjustment_column str Column to use for term frequency adjustments if an exact match is observed. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the exact match of two columns.","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceInKMLevelBase","text":"Bases: ComparisonLevel","title":"DistanceInKMLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceInKMLevelBase.__init__","text":"Use the haversine formula to transform comparisons of lat,lngs into distances measured in kilometers Parameters: Name Type Description Default lat_col str The name of a latitude column or the respective array or struct column column containing the information For example: long_lat['lat'] or long_lat[0] required long_col str The name of a longitudinal column or the respective array or struct column column containing the information, plus an index. For example: long_lat['long'] or long_lat[1] required km_threshold int The total distance in kilometers to evaluate your comparisons against required not_null bool If true, remove any . This is only necessary if you are not capturing nulls elsewhere in your comparison level. False m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates the distance between two coordinates","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.PercentageDifferenceLevelBase","text":"Bases: ComparisonLevel","title":"PercentageDifferenceLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.PercentageDifferenceLevelBase.__init__","text":"","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.ArrayIntersectLevelBase","text":"Bases: ComparisonLevel","title":"ArrayIntersectLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.ArrayIntersectLevelBase.__init__","text":"Represents a comparison level based around the size of an intersection of arrays Parameters: Name Type Description Default col_name str Input column name required m_probability float Starting value for m probability. Defaults to None. None tf_adjustment_column str Column to use for term frequency adjustments if an exact match is observed. Defaults to None. required min_intersection int The minimum cardinality of the intersection of arrays for this comparison level. Defaults to 1 1 include_colname_in_charts_label bool Should the charts label contain the column name? Defaults to False False Returns: Name Type Description ComparisonLevel A comparison level that evaluates the size of intersection of arrays","title":"__init__()"},{"location":"comparison_level_library.html#splink.comparison_level_library.DateDiffLevelBase","text":"Bases: ComparisonLevel","title":"DateDiffLevelBase"},{"location":"comparison_level_library.html#splink.comparison_level_library.DateDiffLevelBase.__init__","text":"Use the ... Parameters: Name Type Description Default date_col str Input column name required date_threshold int The total difference in time between two given dates. This is used in tandem with date_metric to determine . If you are using year as your metric, then a value of 1 would require that your dates lie within 1 year of one another. required date_metric str The unit of time with which to measure your date_threshold . Your metric should be one of day , month or year . Defaults to day . 'day' m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel A comparison level that evaluates whether two dates fall within a given interval.","title":"__init__()"},{"location":"comparison_library.html","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"],"text":"Documentation for comparison_library \u00b6 The comparison_library contains pre-made comparisons available for use directly as described in this topic guide . However, not every comparison is available for every Splink-compatible SQL backend . The pre-made Splink comparisons available for each SQL dialect are as given in this table: spark duckdb athena sqlite array_intersect_at_sizes \u2713 \u2713 \u2713 datediff_at_thresholds \u2713 \u2713 distance_function_at_thresholds \u2713 \u2713 \u2713 \u2713 distance_in_km_at_thresholds \u2713 \u2713 \u2713 exact_match \u2713 \u2713 \u2713 \u2713 jaccard_at_thresholds \u2713 \u2713 jaro_winkler_at_thresholds \u2713 \u2713 levenshtein_at_thresholds \u2713 \u2713 \u2713 The detailed API for each of these are outlined below. Library comparison APIs \u00b6 splink.comparison_library.ExactMatchBase \u00b6 Bases: Comparison __init__ ( col_name , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_else = None , include_colname_in_charts_label = False ) \u00b6 A comparison of the data in col_name with two levels: - Exact match - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required term_frequency_adjustments bool If True, term frequency adjustments will be made on the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None include_colname_in_charts_label If true, append col name to label for charts. Defaults to False. False Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary splink.comparison_library.DistanceFunctionAtThresholdsComparisonBase \u00b6 Bases: Comparison __init__ ( col_name , distance_function_name , distance_threshold_or_thresholds , higher_is_more_similar = True , include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with a user-provided distance function used to assess middle similarity levels. The user-provided distance function must exist in the SQL backend. An example of the output with default arguments and setting distance_function_name to jaccard and distance_threshold_or_thresholds = [0.9,0.7] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_function_name str The name of the distance function. required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison splink.comparison_library.LevenshteinAtThresholdsComparisonBase \u00b6 Bases: DistanceFunctionAtThresholdsComparisonBase __init__ ( col_name , distance_threshold_or_thresholds = [ 1 , 2 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with the levenshtein distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - levenshtein distance <= 1 - levenshtein distance <= 2 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. [1, 2] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison splink.comparison_library.JaccardAtThresholdsComparisonBase \u00b6 Bases: DistanceFunctionAtThresholdsComparisonBase __init__ ( col_name , distance_threshold_or_thresholds = [ 0.9 , 0.7 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with the jaccard distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. [0.9, 0.7] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison splink.comparison_library.JaroWinklerAtThresholdsComparisonBase \u00b6 Bases: DistanceFunctionAtThresholdsComparisonBase __init__ ( col_name , distance_threshold_or_thresholds = [ 0.9 , 0.7 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with the jaro_winkler distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - jaro_winkler distance <= 0.9 - jaro_winkler distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. [0.9, 0.7] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison splink.comparison_library.ArrayIntersectAtSizesComparisonBase \u00b6 Bases: Comparison __init__ ( col_name , size_or_sizes = [ 1 ], m_probability_or_probabilities_sizes = None , m_probability_else = None ) \u00b6 A comparison of the data in array column col_name with various intersection sizes to assess similarity levels. An example of the output with default arguments and setting size_or_sizes = [3, 1] would be - Intersection has at least 3 elements - Intersection has at least 1 element (i.e. 1 or 2) - Anything else (i.e. empty intersection) Parameters: Name Type Description Default col_name str The name of the column to compare required size_or_sizes Union [ int , list ] The size(s) of intersection to use for the non-'else' similarity level(s). Should be in descending order. Defaults to [1]. [1] m_probability_or_probabilities_sizes Union [ float , list ] description . If provided, overrides the default m probabilities for the sizes specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison splink.comparison_library.DateDiffAtThresholdsComparisonBase \u00b6 Bases: Comparison __init__ ( col_name , date_thresholds = [ 1 ], date_metrics = [ 'year' ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_sizes = None , m_probability_else = None ) \u00b6 A comparison of the data in the date column col_name with various date thresholds and metrics to assess similarity levels. An example of the output with default arguments and settings date_thresholds = [1] and date_metrics = ['day'] would be - The two input dates are within 1 day of one another - Anything else (i.e. all other dates lie outside this range) date_thresholds and date_metrics should be used in conjunction with one another. For example, date_thresholds = [10, 12, 15] with date_metrics = ['day', 'month', 'year'] would result in the following checks: - The two dates are within 10 days of one another - The two dates are within 12 months of one another - And the two dates are within 15 years of one another Parameters: Name Type Description Default col_name str The name of the date column to compare. required date_thresholds Union [ int , list ] The size(s) of given date thresholds, to assess whether two dates fall within a given time interval. These values can be any integer value and should be used in tandem with date_metrics . [1] date_metrics Union [ str , list ] The unit of time you wish your date_thresholds to be measured against. Metrics should be one of day , month or year . ['year'] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_sizes Union [ float , list ] description . If provided, overrides the default m probabilities for the sizes specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary. splink.comparison_library.DistanceInKMAtThresholdsComparisonBase \u00b6 Bases: Comparison __init__ ( lat_col , long_col , km_thresholds = [ 0.1 , 1 ], include_exact_match_level = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the coordinates defined in 'lat_col' and 'long col' giving the haversine distance between them in km. An example of the output with default arguments and settings km_thresholds = [1] would be - The two coordinates within 1 km of one another - Anything else (i.e. the distance between all coordinate lie outside this range) Parameters: Name Type Description Default col_name str The name of the date column to compare. required lat_col str The name of the column containing the lattitude of the coordinates. required long_col str The name of the column containing the longitude of the coordinates. required km_thresholds Union [ int , list ] The size(s) of given date thresholds, to assess whether two coordinates fall within a given distance. [0.1, 1] include_exact_match_level bool If True, include an exact match level. Defaults to True. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the sizes specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary.","title":"Comparison Library"},{"location":"comparison_library.html#documentation-for-comparison_library","text":"The comparison_library contains pre-made comparisons available for use directly as described in this topic guide . However, not every comparison is available for every Splink-compatible SQL backend . The pre-made Splink comparisons available for each SQL dialect are as given in this table: spark duckdb athena sqlite array_intersect_at_sizes \u2713 \u2713 \u2713 datediff_at_thresholds \u2713 \u2713 distance_function_at_thresholds \u2713 \u2713 \u2713 \u2713 distance_in_km_at_thresholds \u2713 \u2713 \u2713 exact_match \u2713 \u2713 \u2713 \u2713 jaccard_at_thresholds \u2713 \u2713 jaro_winkler_at_thresholds \u2713 \u2713 levenshtein_at_thresholds \u2713 \u2713 \u2713 The detailed API for each of these are outlined below.","title":"Documentation for comparison_library"},{"location":"comparison_library.html#library-comparison-apis","text":"","title":"Library comparison APIs"},{"location":"comparison_library.html#splink.comparison_library.ExactMatchBase","text":"Bases: Comparison","title":"ExactMatchBase"},{"location":"comparison_library.html#splink.comparison_library.ExactMatchBase.__init__","text":"A comparison of the data in col_name with two levels: - Exact match - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required term_frequency_adjustments bool If True, term frequency adjustments will be made on the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None include_colname_in_charts_label If true, append col name to label for charts. Defaults to False. False Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.DistanceFunctionAtThresholdsComparisonBase","text":"Bases: Comparison","title":"DistanceFunctionAtThresholdsComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.DistanceFunctionAtThresholdsComparisonBase.__init__","text":"A comparison of the data in col_name with a user-provided distance function used to assess middle similarity levels. The user-provided distance function must exist in the SQL backend. An example of the output with default arguments and setting distance_function_name to jaccard and distance_threshold_or_thresholds = [0.9,0.7] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_function_name str The name of the distance function. required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.LevenshteinAtThresholdsComparisonBase","text":"Bases: DistanceFunctionAtThresholdsComparisonBase","title":"LevenshteinAtThresholdsComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.LevenshteinAtThresholdsComparisonBase.__init__","text":"A comparison of the data in col_name with the levenshtein distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - levenshtein distance <= 1 - levenshtein distance <= 2 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. [1, 2] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.JaccardAtThresholdsComparisonBase","text":"Bases: DistanceFunctionAtThresholdsComparisonBase","title":"JaccardAtThresholdsComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.JaccardAtThresholdsComparisonBase.__init__","text":"A comparison of the data in col_name with the jaccard distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. [0.9, 0.7] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.JaroWinklerAtThresholdsComparisonBase","text":"Bases: DistanceFunctionAtThresholdsComparisonBase","title":"JaroWinklerAtThresholdsComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.JaroWinklerAtThresholdsComparisonBase.__init__","text":"A comparison of the data in col_name with the jaro_winkler distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - jaro_winkler distance <= 0.9 - jaro_winkler distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. [0.9, 0.7] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.ArrayIntersectAtSizesComparisonBase","text":"Bases: Comparison","title":"ArrayIntersectAtSizesComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.ArrayIntersectAtSizesComparisonBase.__init__","text":"A comparison of the data in array column col_name with various intersection sizes to assess similarity levels. An example of the output with default arguments and setting size_or_sizes = [3, 1] would be - Intersection has at least 3 elements - Intersection has at least 1 element (i.e. 1 or 2) - Anything else (i.e. empty intersection) Parameters: Name Type Description Default col_name str The name of the column to compare required size_or_sizes Union [ int , list ] The size(s) of intersection to use for the non-'else' similarity level(s). Should be in descending order. Defaults to [1]. [1] m_probability_or_probabilities_sizes Union [ float , list ] description . If provided, overrides the default m probabilities for the sizes specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.DateDiffAtThresholdsComparisonBase","text":"Bases: Comparison","title":"DateDiffAtThresholdsComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.DateDiffAtThresholdsComparisonBase.__init__","text":"A comparison of the data in the date column col_name with various date thresholds and metrics to assess similarity levels. An example of the output with default arguments and settings date_thresholds = [1] and date_metrics = ['day'] would be - The two input dates are within 1 day of one another - Anything else (i.e. all other dates lie outside this range) date_thresholds and date_metrics should be used in conjunction with one another. For example, date_thresholds = [10, 12, 15] with date_metrics = ['day', 'month', 'year'] would result in the following checks: - The two dates are within 10 days of one another - The two dates are within 12 months of one another - And the two dates are within 15 years of one another Parameters: Name Type Description Default col_name str The name of the date column to compare. required date_thresholds Union [ int , list ] The size(s) of given date thresholds, to assess whether two dates fall within a given time interval. These values can be any integer value and should be used in tandem with date_metrics . [1] date_metrics Union [ str , list ] The unit of time you wish your date_thresholds to be measured against. Metrics should be one of day , month or year . ['year'] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_sizes Union [ float , list ] description . If provided, overrides the default m probabilities for the sizes specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary.","title":"__init__()"},{"location":"comparison_library.html#splink.comparison_library.DistanceInKMAtThresholdsComparisonBase","text":"Bases: Comparison","title":"DistanceInKMAtThresholdsComparisonBase"},{"location":"comparison_library.html#splink.comparison_library.DistanceInKMAtThresholdsComparisonBase.__init__","text":"A comparison of the coordinates defined in 'lat_col' and 'long col' giving the haversine distance between them in km. An example of the output with default arguments and settings km_thresholds = [1] would be - The two coordinates within 1 km of one another - Anything else (i.e. the distance between all coordinate lie outside this range) Parameters: Name Type Description Default col_name str The name of the date column to compare. required lat_col str The name of the column containing the lattitude of the coordinates. required long_col str The name of the column containing the longitude of the coordinates. required km_thresholds Union [ int , list ] The size(s) of given date thresholds, to assess whether two coordinates fall within a given distance. [0.1, 1] include_exact_match_level bool If True, include an exact match level. Defaults to True. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the sizes specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary.","title":"__init__()"},{"location":"comparison_template_library.html","tags":["API","comparisons","Date Comparison"],"text":"Documentation for comparison_template_library \u00b6 The comparison_template_library contains pre-made comparisons with pre-defined parameters available for use directly as described in this topic guide . However, not every comparison is available for every Splink-compatible SQL backend . More detail on creating comparisons for specific data types is also included in the topic guide. The detailed API for each of these are outlined below. Library comparison APIs \u00b6 splink.comparison_template_library.DateComparisonBase \u00b6 Bases: Comparison __init__ ( col_name , include_exact_match_level = True , term_frequency_adjustments = False , separate_1st_january = False , levenshtein_thresholds = [ 1 , 2 ], jaro_winkler_thresholds = [], datediff_thresholds = [ 1 , 10 ], datediff_metrics = [ 'year' , 'year' ], m_probability_exact_match = None , m_probability_1st_january = None , m_probability_or_probabilities_lev = None , m_probability_or_probabilities_jw = None , m_probability_or_probabilities_datediff = None , m_probability_else = None ) \u00b6 A wrapper to generate a comparison for a date column the data in col_name with preselected defaults. The default arguments will give a comparison with comparison levels: Exact match (1st of January only) Exact match (all other dates) Levenshtein distance <= 2 Date difference <= 1 year Date difference <= 10 years Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False separate_1st_january bool If True, include a separate exact match comparison level when date is 1st January. False levenshtein_thresholds Union [ int , list ] The thresholds to use for levenshtein similarity level(s). We recommend use of either levenshtein or jaro_winkler for fuzzy matching, but not both. Defaults to [2] [1, 2] jaro_winkler_thresholds Union [ int , list ] The thresholds to use for jaro_winkler similarity level(s). We recommend use of either levenshtein or jaro_winkler for fuzzy matching, but not both. Defaults to [] [] datediff_thresholds Union [ int , list ] The thresholds to use for datediff similarity level(s). Defaults to [1, 1]. [1, 10] datediff_metrics Union [ str , list ] The metrics to apply thresholds to for datediff similarity level(s). Defaults to [\"month\", \"year\"]. ['year', 'year'] m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the levenshtein thresholds specified. Defaults to None. None m_probability_or_probabilities_jw Union [ float , list ] description . If provided, overrides the default m probabilities for the jaro winkler thresholds specified. Defaults to None. None m_probability_or_probabilities_datediff Union [ float , list ] description . If provided, overrides the default m probabilities for the datediff thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary. splink.comparison_template_library.NameComparisonBase \u00b6 Bases: Comparison __init__ ( col_name , include_exact_match_level = True , phonetic_col_name = None , term_frequency_adjustments_name = False , term_frequency_adjustments_phonetic_name = False , levenshtein_thresholds = [], jaro_winkler_thresholds = [ 0.95 , 0.88 ], jaccard_thresholds = [], m_probability_exact_match_name = None , m_probability_exact_match_phonetic_name = None , m_probability_or_probabilities_lev = None , m_probability_or_probabilities_jw = None , m_probability_or_probabilities_jac = None , m_probability_else = None ) \u00b6 A wrapper to generate a comparison for a name column the data in col_name with preselected defaults. The default arguments will give a comparison with comparison levels: Exact match Jaro Winkler similarity >= 0.95 Jaro Winkler similarity >= 0.88 Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required include_exact_match_level bool If True, include an exact match level for col_name. Defaults to True. True phonetic_col_name str The name of the column with phonetic reduction (such as dmetaphone) of col_name. Including parameter will create an exact match level for phonetic_col_name. The phonetic column must be present in the dataset to use this parameter. Defaults to None None term_frequency_adjustments_name bool If True, apply term frequency adjustments to the exact match level for \"col_name\". Defaults to False. False term_frequency_adjustments_phonetic_name bool If True, apply term frequency adjustments to the exact match level for \"phonetic_col_name\". Defaults to False. False levenshtein_thresholds Union [ int , list ] The thresholds to use for levenshtein similarity level(s). Defaults to [] [] jaro_winkler_thresholds Union [ int , list ] The thresholds to use for jaro_winkler similarity level(s). Defaults to [0.88] [0.95, 0.88] jaccard_thresholds Union [ int , list ] The thresholds to use for jaccard similarity level(s). Defaults to [] [] m_probability_exact_match_name _type_ If provided, overrides the default m probability for the exact match level for col_name. Defaults to None. None m_probability_exact_match_phonetic_name _type_ If provided, overrides the default m probability for the exact match level for phonetic_col_name. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_or_probabilities_datediff Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. required m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be included in the Splink settings dictionary.","title":"Comparison Template Library"},{"location":"comparison_template_library.html#documentation-for-comparison_template_library","text":"The comparison_template_library contains pre-made comparisons with pre-defined parameters available for use directly as described in this topic guide . However, not every comparison is available for every Splink-compatible SQL backend . More detail on creating comparisons for specific data types is also included in the topic guide. The detailed API for each of these are outlined below.","title":"Documentation for comparison_template_library"},{"location":"comparison_template_library.html#library-comparison-apis","text":"","title":"Library comparison APIs"},{"location":"comparison_template_library.html#splink.comparison_template_library.DateComparisonBase","text":"Bases: Comparison","title":"DateComparisonBase"},{"location":"comparison_template_library.html#splink.comparison_template_library.DateComparisonBase.__init__","text":"A wrapper to generate a comparison for a date column the data in col_name with preselected defaults. The default arguments will give a comparison with comparison levels: Exact match (1st of January only) Exact match (all other dates) Levenshtein distance <= 2 Date difference <= 1 year Date difference <= 10 years Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False separate_1st_january bool If True, include a separate exact match comparison level when date is 1st January. False levenshtein_thresholds Union [ int , list ] The thresholds to use for levenshtein similarity level(s). We recommend use of either levenshtein or jaro_winkler for fuzzy matching, but not both. Defaults to [2] [1, 2] jaro_winkler_thresholds Union [ int , list ] The thresholds to use for jaro_winkler similarity level(s). We recommend use of either levenshtein or jaro_winkler for fuzzy matching, but not both. Defaults to [] [] datediff_thresholds Union [ int , list ] The thresholds to use for datediff similarity level(s). Defaults to [1, 1]. [1, 10] datediff_metrics Union [ str , list ] The metrics to apply thresholds to for datediff similarity level(s). Defaults to [\"month\", \"year\"]. ['year', 'year'] m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the levenshtein thresholds specified. Defaults to None. None m_probability_or_probabilities_jw Union [ float , list ] description . If provided, overrides the default m probabilities for the jaro winkler thresholds specified. Defaults to None. None m_probability_or_probabilities_datediff Union [ float , list ] description . If provided, overrides the default m probabilities for the datediff thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be inclued in the Splink settings dictionary.","title":"__init__()"},{"location":"comparison_template_library.html#splink.comparison_template_library.NameComparisonBase","text":"Bases: Comparison","title":"NameComparisonBase"},{"location":"comparison_template_library.html#splink.comparison_template_library.NameComparisonBase.__init__","text":"A wrapper to generate a comparison for a name column the data in col_name with preselected defaults. The default arguments will give a comparison with comparison levels: Exact match Jaro Winkler similarity >= 0.95 Jaro Winkler similarity >= 0.88 Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required include_exact_match_level bool If True, include an exact match level for col_name. Defaults to True. True phonetic_col_name str The name of the column with phonetic reduction (such as dmetaphone) of col_name. Including parameter will create an exact match level for phonetic_col_name. The phonetic column must be present in the dataset to use this parameter. Defaults to None None term_frequency_adjustments_name bool If True, apply term frequency adjustments to the exact match level for \"col_name\". Defaults to False. False term_frequency_adjustments_phonetic_name bool If True, apply term frequency adjustments to the exact match level for \"phonetic_col_name\". Defaults to False. False levenshtein_thresholds Union [ int , list ] The thresholds to use for levenshtein similarity level(s). Defaults to [] [] jaro_winkler_thresholds Union [ int , list ] The thresholds to use for jaro_winkler similarity level(s). Defaults to [0.88] [0.95, 0.88] jaccard_thresholds Union [ int , list ] The thresholds to use for jaccard similarity level(s). Defaults to [] [] m_probability_exact_match_name _type_ If provided, overrides the default m probability for the exact match level for col_name. Defaults to None. None m_probability_exact_match_phonetic_name _type_ If provided, overrides the default m probability for the exact match level for phonetic_col_name. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_or_probabilities_datediff Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. required m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison A comparison that can be included in the Splink settings dictionary.","title":"__init__()"},{"location":"em_training_session.html","tags":["API","Expectation Maximisation","Model Training"],"text":"Documentation for EMTrainingSession object \u00b6 Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts Source code in splink/em_training_session.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 class EMTrainingSession : \"\"\"Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts \"\"\" def __init__ ( self , linker : Linker , blocking_rule_for_training : str , fix_u_probabilities : bool = False , fix_m_probabilities : bool = False , fix_probability_two_random_records_match : bool = False , comparisons_to_deactivate : list [ Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , ): logger . info ( \" \\n ----- Starting EM training session ----- \\n \" ) self . _original_settings_obj = linker . _settings_obj self . _original_linker = linker self . _training_linker = deepcopy ( linker ) self . _settings_obj = self . _training_linker . _settings_obj self . _settings_obj . _retain_matching_columns = False self . _settings_obj . _retain_intermediate_calculation_columns = False self . _settings_obj . _training_mode = True if not isinstance ( blocking_rule_for_training , BlockingRule ): blocking_rule = BlockingRule ( blocking_rule_for_training ) self . _settings_obj . _blocking_rule_for_training = blocking_rule self . _blocking_rule_for_training = blocking_rule if comparison_levels_to_reverse_blocking_rule : self . _comparison_levels_to_reverse_blocking_rule = ( comparison_levels_to_reverse_blocking_rule ) else : self . _comparison_levels_to_reverse_blocking_rule = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa blocking_rule_for_training ) self . _settings_obj . _probability_two_random_records_match = ( self . _blocking_adjusted_probability_two_random_records_match ) self . _training_fix_u_probabilities = fix_u_probabilities self . _training_fix_m_probabilities = fix_m_probabilities self . _training_fix_probability_two_random_records_match = ( fix_probability_two_random_records_match ) # Remove comparison columns which are either 'used up' by the blocking rules # or alternatively, if the user has manually provided a list to remove, # use this instead if not comparisons_to_deactivate : comparisons_to_deactivate = [] br_cols = get_columns_used_from_sql ( blocking_rule_for_training , self . _settings_obj . _sql_dialect ) for cc in self . _settings_obj . comparisons : cc_cols = cc . _input_columns_used_by_case_statement cc_cols = [ c . input_name for c in cc_cols ] if set ( br_cols ) . intersection ( cc_cols ): comparisons_to_deactivate . append ( cc ) cc_names_to_deactivate = [ cc . _output_column_name for cc in comparisons_to_deactivate ] self . _comparisons_that_cannot_be_estimated : list [ Comparison ] = comparisons_to_deactivate filtered_ccs = [ cc for cc in self . _settings_obj . comparisons if cc . _output_column_name not in cc_names_to_deactivate ] self . _settings_obj . comparisons = filtered_ccs self . _comparisons_that_can_be_estimated = filtered_ccs self . _settings_obj_history = [] # Add iteration 0 i.e. the starting parameters self . _add_iteration () def _training_log_message ( self ): not_estimated = [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] not_estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in not_estimated ]) estimated = [ cc . _output_column_name for cc in self . _comparisons_that_can_be_estimated ] estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in estimated ]) if self . _training_fix_m_probabilities and self . _training_fix_u_probabilities : raise ValueError ( \"Can't train model if you fix both m and u probabilites\" ) elif self . _training_fix_u_probabilities : mu = \"m probabilities\" elif self . _training_fix_m_probabilities : mu = \"u probabilities\" else : mu = \"m and u probabilities\" blocking_rule = self . _blocking_rule_for_training . blocking_rule logger . info ( f \"Estimating the { mu } of the model by blocking on: \\n \" f \" { blocking_rule } \\n\\n \" \"Parameter estimates will be made for the following comparison(s):\" f \" { estimated } \\n \" \" \\n Parameter estimates cannot be made for the following comparison(s)\" f \" since they are used in the blocking rules: { not_estimated } \" ) def _comparison_vectors ( self ): self . _training_log_message () nodes_with_tf = self . _original_linker . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self . _training_linker ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_blocked\" ) # repartition after blocking only exists on the SparkLinker repartition_after_blocking = getattr ( self . _original_linker , \"repartition_after_blocking\" , False ) if repartition_after_blocking : df_blocked = self . _training_linker . _execute_sql_pipeline ([ nodes_with_tf ]) input_dataframes = [ nodes_with_tf , df_blocked ] else : input_dataframes = [ nodes_with_tf ] sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) return self . _training_linker . _execute_sql_pipeline ( input_dataframes ) def _train ( self ): cvv = self . _comparison_vectors () # check that the blocking rule actually generates _some_ record pairs, # if not give the user a helpful message if not cvv . as_record_dict ( limit = 1 ): br_sql = f \"` { self . _blocking_rule_for_training . blocking_rule } `\" raise EMTrainingException ( f \"Training rule { br_sql } resulted in no record pairs. \" \"This means that in the supplied data set \" f \"there were no pairs of records for which { br_sql } was `true`. \\n \" \"Expectation maximisation requires a substantial number of record \" \"comparisons to produce accurate parameter estimates - usually \" \"at least a few hundred, but preferably at least a few thousand. \\n \" \"You must revise your training blocking rule so that the set of \" \"generated comparisons is not empty. You can use \" \"`linker.count_num_comparisons_from_blocking_rule()` to compute \" \"the number of comparisons that will be generated by a blocking rule.\" ) # Compute the new params, populating the paramters in the copied settings object # At this stage, we do not overwrite any of the parameters # in the original (main) setting object expectation_maximisation ( self , cvv ) rule = self . _blocking_rule_for_training . blocking_rule training_desc = f \"EM, blocked on: { rule } \" # Add m and u values to original settings for cc in self . _settings_obj . comparisons : orig_cc = self . _original_settings_obj . _get_comparison_by_output_column_name ( cc . _output_column_name ) for cl in cc . _comparison_levels_excluding_null : orig_cl = orig_cc . _get_comparison_level_by_comparison_vector_value ( cl . _comparison_vector_value ) if not self . _training_fix_m_probabilities : not_observed = LEVEL_NOT_OBSERVED_TEXT if cl . _m_probability == not_observed : orig_cl . _add_trained_m_probability ( not_observed , training_desc ) logger . info ( f \"m probability not trained for { cc . _output_column_name } - \" f \" { cl . label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_m_probability ( cl . m_probability , training_desc ) if not self . _training_fix_u_probabilities : not_observed = LEVEL_NOT_OBSERVED_TEXT if cl . _u_probability == not_observed : orig_cl . _add_trained_u_probability ( not_observed , training_desc ) logger . info ( f \"u probability not trained for { cc . _output_column_name } - \" f \" { cl . label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_u_probability ( cl . u_probability , training_desc ) self . _original_linker . _em_training_sessions . append ( self ) def _add_iteration ( self ): self . _settings_obj_history . append ( deepcopy ( self . _settings_obj )) @property def _blocking_adjusted_probability_two_random_records_match ( self ): orig_prop_m = self . _original_settings_obj . _probability_two_random_records_match adj_bayes_factor = prob_to_bayes_factor ( orig_prop_m ) logger . log ( 15 , f \"Original prob two random records match: { orig_prop_m : .3f } \" ) comp_levels = self . _comparison_levels_to_reverse_blocking_rule if not comp_levels : comp_levels = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa self . _blocking_rule_for_training . blocking_rule ) for cl in comp_levels : adj_bayes_factor = cl . _bayes_factor * adj_bayes_factor logger . log ( 15 , f \"Increasing prob two random records match using \" f \" { cl . comparison . _output_column_name } - { cl . label_for_charts } \" f \" using bayes factor { cl . _bayes_factor : ,.3f } \" , ) adjusted_prop_m = bayes_factor_to_prob ( adj_bayes_factor ) logger . log ( 15 , f \" \\n Prob two random records match adjusted for blocking on \" f \" { self . _blocking_rule_for_training . blocking_rule } : \" f \" { adjusted_prop_m : .3f } \" , ) return adjusted_prop_m @property def _iteration_history_records ( self ): output_records = [] for iteration , settings_obj in enumerate ( self . _settings_obj_history ): records = settings_obj . _parameters_as_detailed_records for r in records : r [ \"iteration\" ] = iteration r [ \"probability_two_random_records_match\" ] = self . _settings_obj . _probability_two_random_records_match output_records . extend ( records ) return output_records @property def _lambda_history_records ( self ): output_records = [] for i , s in enumerate ( self . _settings_obj_history ): lam = s . _probability_two_random_records_match r = { \"probability_two_random_records_match\" : lam , \"probability_two_random_records_match_reciprocal\" : 1 / lam , \"iteration\" : i , } output_records . append ( r ) return output_records def probability_two_random_records_match_iteration_chart ( self ): records = self . _lambda_history_records return probability_two_random_records_match_iteration_chart ( records ) def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training ) def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records ) def _max_change_message ( self , max_change_dict ): message = \"Largest change in params was\" if max_change_dict [ \"max_change_type\" ] == \"probability_two_random_records_match\" : message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" \"probability_two_random_records_match\" ) else : cl = max_change_dict [ \"current_comparison_level\" ] m_u = max_change_dict [ \"max_change_type\" ] cc_name = cl . comparison . _output_column_name cl_label = cl . label_for_charts level_text = f \" { cc_name } , level ` { cl_label } `\" message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" f \"the { m_u } of { level_text } \" ) return message def _max_change_in_parameters_comparison_levels ( self ): previous_iteration = self . _settings_obj_history [ - 2 ] this_iteration = self . _settings_obj_history [ - 1 ] max_change = - 0.1 max_change_levels = { \"previous_iteration\" : None , \"this_iteration\" : None , \"max_change_type\" : None , \"max_change_value\" : None , } comparisons = zip ( previous_iteration . comparisons , this_iteration . comparisons ) for comparison in comparisons : prev_cc = comparison [ 0 ] this_cc = comparison [ 1 ] z_cls = zip ( prev_cc . comparison_levels , this_cc . comparison_levels ) for z_cl in z_cls : if z_cl [ 0 ] . is_null_level : continue prev_cl = z_cl [ 0 ] this_cl = z_cl [ 1 ] change_m = this_cl . m_probability - prev_cl . m_probability change_u = this_cl . u_probability - prev_cl . u_probability change = max ( abs ( change_m ), abs ( change_u )) change_type = ( \"m_probability\" if abs ( change_m ) > abs ( change_u ) else \"u_probability\" ) change_value = change_m if abs ( change_m ) > abs ( change_u ) else change_u if change > max_change : max_change = change max_change_levels [ \"prev_comparison_level\" ] = prev_cl max_change_levels [ \"current_comparison_level\" ] = this_cl max_change_levels [ \"max_change_type\" ] = change_type max_change_levels [ \"max_change_value\" ] = change_value max_change_levels [ \"max_abs_change_value\" ] = abs ( change_value ) change_probability_two_random_records_match = ( this_iteration . _probability_two_random_records_match - previous_iteration . _probability_two_random_records_match ) if abs ( change_probability_two_random_records_match ) > max_change : max_change = abs ( change_probability_two_random_records_match ) max_change_levels [ \"prev_comparison_level\" ] = None max_change_levels [ \"current_comparison_level\" ] = None max_change_levels [ \"max_change_type\" ] = \"probability_two_random_records_match\" max_change_levels [ \"max_change_value\" ] = change_probability_two_random_records_match max_change_levels [ \"max_abs_change_value\" ] = abs ( change_probability_two_random_records_match ) max_change_levels [ \"message\" ] = self . _max_change_message ( max_change_levels ) return max_change_levels def __repr__ ( self ): deactivated_cols = \", \" . join ( [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] ) blocking_rule = self . _blocking_rule_for_training . blocking_rule return ( f \"<EMTrainingSession, blocking on { blocking_rule } , \" f \"deactivating comparisons { deactivated_cols } >\" ) match_weights_interactive_history_chart () \u00b6 Source code in splink/em_training_session.py 308 309 310 311 312 def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training ) m_u_values_interactive_history_chart () \u00b6 Source code in splink/em_training_session.py 314 315 316 def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records )","title":"EM Training Session API"},{"location":"em_training_session.html#documentation-for-emtrainingsession-object","text":"Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts Source code in splink/em_training_session.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 class EMTrainingSession : \"\"\"Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts \"\"\" def __init__ ( self , linker : Linker , blocking_rule_for_training : str , fix_u_probabilities : bool = False , fix_m_probabilities : bool = False , fix_probability_two_random_records_match : bool = False , comparisons_to_deactivate : list [ Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , ): logger . info ( \" \\n ----- Starting EM training session ----- \\n \" ) self . _original_settings_obj = linker . _settings_obj self . _original_linker = linker self . _training_linker = deepcopy ( linker ) self . _settings_obj = self . _training_linker . _settings_obj self . _settings_obj . _retain_matching_columns = False self . _settings_obj . _retain_intermediate_calculation_columns = False self . _settings_obj . _training_mode = True if not isinstance ( blocking_rule_for_training , BlockingRule ): blocking_rule = BlockingRule ( blocking_rule_for_training ) self . _settings_obj . _blocking_rule_for_training = blocking_rule self . _blocking_rule_for_training = blocking_rule if comparison_levels_to_reverse_blocking_rule : self . _comparison_levels_to_reverse_blocking_rule = ( comparison_levels_to_reverse_blocking_rule ) else : self . _comparison_levels_to_reverse_blocking_rule = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa blocking_rule_for_training ) self . _settings_obj . _probability_two_random_records_match = ( self . _blocking_adjusted_probability_two_random_records_match ) self . _training_fix_u_probabilities = fix_u_probabilities self . _training_fix_m_probabilities = fix_m_probabilities self . _training_fix_probability_two_random_records_match = ( fix_probability_two_random_records_match ) # Remove comparison columns which are either 'used up' by the blocking rules # or alternatively, if the user has manually provided a list to remove, # use this instead if not comparisons_to_deactivate : comparisons_to_deactivate = [] br_cols = get_columns_used_from_sql ( blocking_rule_for_training , self . _settings_obj . _sql_dialect ) for cc in self . _settings_obj . comparisons : cc_cols = cc . _input_columns_used_by_case_statement cc_cols = [ c . input_name for c in cc_cols ] if set ( br_cols ) . intersection ( cc_cols ): comparisons_to_deactivate . append ( cc ) cc_names_to_deactivate = [ cc . _output_column_name for cc in comparisons_to_deactivate ] self . _comparisons_that_cannot_be_estimated : list [ Comparison ] = comparisons_to_deactivate filtered_ccs = [ cc for cc in self . _settings_obj . comparisons if cc . _output_column_name not in cc_names_to_deactivate ] self . _settings_obj . comparisons = filtered_ccs self . _comparisons_that_can_be_estimated = filtered_ccs self . _settings_obj_history = [] # Add iteration 0 i.e. the starting parameters self . _add_iteration () def _training_log_message ( self ): not_estimated = [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] not_estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in not_estimated ]) estimated = [ cc . _output_column_name for cc in self . _comparisons_that_can_be_estimated ] estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in estimated ]) if self . _training_fix_m_probabilities and self . _training_fix_u_probabilities : raise ValueError ( \"Can't train model if you fix both m and u probabilites\" ) elif self . _training_fix_u_probabilities : mu = \"m probabilities\" elif self . _training_fix_m_probabilities : mu = \"u probabilities\" else : mu = \"m and u probabilities\" blocking_rule = self . _blocking_rule_for_training . blocking_rule logger . info ( f \"Estimating the { mu } of the model by blocking on: \\n \" f \" { blocking_rule } \\n\\n \" \"Parameter estimates will be made for the following comparison(s):\" f \" { estimated } \\n \" \" \\n Parameter estimates cannot be made for the following comparison(s)\" f \" since they are used in the blocking rules: { not_estimated } \" ) def _comparison_vectors ( self ): self . _training_log_message () nodes_with_tf = self . _original_linker . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self . _training_linker ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_blocked\" ) # repartition after blocking only exists on the SparkLinker repartition_after_blocking = getattr ( self . _original_linker , \"repartition_after_blocking\" , False ) if repartition_after_blocking : df_blocked = self . _training_linker . _execute_sql_pipeline ([ nodes_with_tf ]) input_dataframes = [ nodes_with_tf , df_blocked ] else : input_dataframes = [ nodes_with_tf ] sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) return self . _training_linker . _execute_sql_pipeline ( input_dataframes ) def _train ( self ): cvv = self . _comparison_vectors () # check that the blocking rule actually generates _some_ record pairs, # if not give the user a helpful message if not cvv . as_record_dict ( limit = 1 ): br_sql = f \"` { self . _blocking_rule_for_training . blocking_rule } `\" raise EMTrainingException ( f \"Training rule { br_sql } resulted in no record pairs. \" \"This means that in the supplied data set \" f \"there were no pairs of records for which { br_sql } was `true`. \\n \" \"Expectation maximisation requires a substantial number of record \" \"comparisons to produce accurate parameter estimates - usually \" \"at least a few hundred, but preferably at least a few thousand. \\n \" \"You must revise your training blocking rule so that the set of \" \"generated comparisons is not empty. You can use \" \"`linker.count_num_comparisons_from_blocking_rule()` to compute \" \"the number of comparisons that will be generated by a blocking rule.\" ) # Compute the new params, populating the paramters in the copied settings object # At this stage, we do not overwrite any of the parameters # in the original (main) setting object expectation_maximisation ( self , cvv ) rule = self . _blocking_rule_for_training . blocking_rule training_desc = f \"EM, blocked on: { rule } \" # Add m and u values to original settings for cc in self . _settings_obj . comparisons : orig_cc = self . _original_settings_obj . _get_comparison_by_output_column_name ( cc . _output_column_name ) for cl in cc . _comparison_levels_excluding_null : orig_cl = orig_cc . _get_comparison_level_by_comparison_vector_value ( cl . _comparison_vector_value ) if not self . _training_fix_m_probabilities : not_observed = LEVEL_NOT_OBSERVED_TEXT if cl . _m_probability == not_observed : orig_cl . _add_trained_m_probability ( not_observed , training_desc ) logger . info ( f \"m probability not trained for { cc . _output_column_name } - \" f \" { cl . label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_m_probability ( cl . m_probability , training_desc ) if not self . _training_fix_u_probabilities : not_observed = LEVEL_NOT_OBSERVED_TEXT if cl . _u_probability == not_observed : orig_cl . _add_trained_u_probability ( not_observed , training_desc ) logger . info ( f \"u probability not trained for { cc . _output_column_name } - \" f \" { cl . label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_u_probability ( cl . u_probability , training_desc ) self . _original_linker . _em_training_sessions . append ( self ) def _add_iteration ( self ): self . _settings_obj_history . append ( deepcopy ( self . _settings_obj )) @property def _blocking_adjusted_probability_two_random_records_match ( self ): orig_prop_m = self . _original_settings_obj . _probability_two_random_records_match adj_bayes_factor = prob_to_bayes_factor ( orig_prop_m ) logger . log ( 15 , f \"Original prob two random records match: { orig_prop_m : .3f } \" ) comp_levels = self . _comparison_levels_to_reverse_blocking_rule if not comp_levels : comp_levels = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa self . _blocking_rule_for_training . blocking_rule ) for cl in comp_levels : adj_bayes_factor = cl . _bayes_factor * adj_bayes_factor logger . log ( 15 , f \"Increasing prob two random records match using \" f \" { cl . comparison . _output_column_name } - { cl . label_for_charts } \" f \" using bayes factor { cl . _bayes_factor : ,.3f } \" , ) adjusted_prop_m = bayes_factor_to_prob ( adj_bayes_factor ) logger . log ( 15 , f \" \\n Prob two random records match adjusted for blocking on \" f \" { self . _blocking_rule_for_training . blocking_rule } : \" f \" { adjusted_prop_m : .3f } \" , ) return adjusted_prop_m @property def _iteration_history_records ( self ): output_records = [] for iteration , settings_obj in enumerate ( self . _settings_obj_history ): records = settings_obj . _parameters_as_detailed_records for r in records : r [ \"iteration\" ] = iteration r [ \"probability_two_random_records_match\" ] = self . _settings_obj . _probability_two_random_records_match output_records . extend ( records ) return output_records @property def _lambda_history_records ( self ): output_records = [] for i , s in enumerate ( self . _settings_obj_history ): lam = s . _probability_two_random_records_match r = { \"probability_two_random_records_match\" : lam , \"probability_two_random_records_match_reciprocal\" : 1 / lam , \"iteration\" : i , } output_records . append ( r ) return output_records def probability_two_random_records_match_iteration_chart ( self ): records = self . _lambda_history_records return probability_two_random_records_match_iteration_chart ( records ) def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training ) def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records ) def _max_change_message ( self , max_change_dict ): message = \"Largest change in params was\" if max_change_dict [ \"max_change_type\" ] == \"probability_two_random_records_match\" : message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" \"probability_two_random_records_match\" ) else : cl = max_change_dict [ \"current_comparison_level\" ] m_u = max_change_dict [ \"max_change_type\" ] cc_name = cl . comparison . _output_column_name cl_label = cl . label_for_charts level_text = f \" { cc_name } , level ` { cl_label } `\" message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" f \"the { m_u } of { level_text } \" ) return message def _max_change_in_parameters_comparison_levels ( self ): previous_iteration = self . _settings_obj_history [ - 2 ] this_iteration = self . _settings_obj_history [ - 1 ] max_change = - 0.1 max_change_levels = { \"previous_iteration\" : None , \"this_iteration\" : None , \"max_change_type\" : None , \"max_change_value\" : None , } comparisons = zip ( previous_iteration . comparisons , this_iteration . comparisons ) for comparison in comparisons : prev_cc = comparison [ 0 ] this_cc = comparison [ 1 ] z_cls = zip ( prev_cc . comparison_levels , this_cc . comparison_levels ) for z_cl in z_cls : if z_cl [ 0 ] . is_null_level : continue prev_cl = z_cl [ 0 ] this_cl = z_cl [ 1 ] change_m = this_cl . m_probability - prev_cl . m_probability change_u = this_cl . u_probability - prev_cl . u_probability change = max ( abs ( change_m ), abs ( change_u )) change_type = ( \"m_probability\" if abs ( change_m ) > abs ( change_u ) else \"u_probability\" ) change_value = change_m if abs ( change_m ) > abs ( change_u ) else change_u if change > max_change : max_change = change max_change_levels [ \"prev_comparison_level\" ] = prev_cl max_change_levels [ \"current_comparison_level\" ] = this_cl max_change_levels [ \"max_change_type\" ] = change_type max_change_levels [ \"max_change_value\" ] = change_value max_change_levels [ \"max_abs_change_value\" ] = abs ( change_value ) change_probability_two_random_records_match = ( this_iteration . _probability_two_random_records_match - previous_iteration . _probability_two_random_records_match ) if abs ( change_probability_two_random_records_match ) > max_change : max_change = abs ( change_probability_two_random_records_match ) max_change_levels [ \"prev_comparison_level\" ] = None max_change_levels [ \"current_comparison_level\" ] = None max_change_levels [ \"max_change_type\" ] = \"probability_two_random_records_match\" max_change_levels [ \"max_change_value\" ] = change_probability_two_random_records_match max_change_levels [ \"max_abs_change_value\" ] = abs ( change_probability_two_random_records_match ) max_change_levels [ \"message\" ] = self . _max_change_message ( max_change_levels ) return max_change_levels def __repr__ ( self ): deactivated_cols = \", \" . join ( [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] ) blocking_rule = self . _blocking_rule_for_training . blocking_rule return ( f \"<EMTrainingSession, blocking on { blocking_rule } , \" f \"deactivating comparisons { deactivated_cols } >\" )","title":"Documentation for EMTrainingSession object"},{"location":"em_training_session.html#splink.em_training_session.EMTrainingSession.match_weights_interactive_history_chart","text":"Source code in splink/em_training_session.py 308 309 310 311 312 def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training )","title":"match_weights_interactive_history_chart()"},{"location":"em_training_session.html#splink.em_training_session.EMTrainingSession.m_u_values_interactive_history_chart","text":"Source code in splink/em_training_session.py 314 315 316 def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records )","title":"m_u_values_interactive_history_chart()"},{"location":"examples_index.html","tags":["Examples","DuckDB","Spark"],"text":"Examples \u00b6 This page provides a series of examples to help you get started with splink. You can find the underlying notebooks at the splink_demos repo. You can try these demos live in your web browser using the following link: DuckDB examples \u00b6 Entity type: Persons \u00b6 Deduplicating 50,000 records of realistic data based on historical persons Using the link_only setting to link, but not dedupe, two datasets Real time record linkage Accuracy analysis and ROC charts using a ground truth (cluster) column Estimating m probabilities from pairwise labels Deduplicating the febrl3 dataset . Note this dataset comes from febrl , as referenced in A.2 here and replicated here . Linking the febrl4 datasets . As above, these datasets are from febrl , replicated here . Entity type: Financial transactions \u00b6 Linking financial transactions PySpark examples \u00b6 Deduplication of a small dataset using Pyspark. Entity type is persons.","title":"Examples index"},{"location":"examples_index.html#examples","text":"This page provides a series of examples to help you get started with splink. You can find the underlying notebooks at the splink_demos repo. You can try these demos live in your web browser using the following link:","title":"Examples"},{"location":"examples_index.html#duckdb-examples","text":"","title":"DuckDB examples"},{"location":"examples_index.html#entity-type-persons","text":"Deduplicating 50,000 records of realistic data based on historical persons Using the link_only setting to link, but not dedupe, two datasets Real time record linkage Accuracy analysis and ROC charts using a ground truth (cluster) column Estimating m probabilities from pairwise labels Deduplicating the febrl3 dataset . Note this dataset comes from febrl , as referenced in A.2 here and replicated here . Linking the febrl4 datasets . As above, these datasets are from febrl , replicated here .","title":"Entity type: Persons"},{"location":"examples_index.html#entity-type-financial-transactions","text":"Linking financial transactions","title":"Entity type: Financial transactions"},{"location":"examples_index.html#pyspark-examples","text":"Deduplication of a small dataset using Pyspark. Entity type is persons.","title":"PySpark examples"},{"location":"linker.html","tags":["API"],"text":"Documentation for Linker object \u00b6 The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . Source code in splink/linker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 class Linker : \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a `DuckDBLinker`. \"\"\" def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False @property def _cache_uid ( self ): if self . _settings_dict : return self . _settings_obj . _cache_uid else : return self . _cache_uid_no_settings @_cache_uid . setter def _cache_uid ( self , value ): if self . _settings_dict : self . _settings_obj . _cache_uid = value else : self . _cache_uid_no_settings = value @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `load_settings()` method on your linker \" \"object. i.e. linker.load_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink__df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True # in u-train sample mode we are joining the concatenated table mixing # both data sets - hence if we inner join on True we will end up with # samples which both originate from the same dataset if self . _train_u_using_random_sample_mode : return False if self . _analyse_blocking_mode : return False if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False @property def _sql_dialect ( self ): if self . _sql_dialect_ is None : raise NotImplementedError ( f \"No SQL dialect set on object of type { type ( self ) } . \" \"Did you make sure to create a dialect-specific Linker?\" ) return self . _sql_dialect_ @property def _infinity_expression ( self ): raise NotImplementedError ( f \"infinity sql expression not available for { type ( self ) } \" ) def _setup_settings_objs ( self , settings_dict ): # Setup the linker class's required settings self . _settings_dict = settings_dict # if settings_dict is passed, set sql_dialect on it if missing, and make sure # incompatible dialect not passed if settings_dict is not None and settings_dict . get ( \"sql_dialect\" , None ) is None : settings_dict [ \"sql_dialect\" ] = self . _sql_dialect if settings_dict is None : self . _cache_uid_no_settings = ascii_uid ( 8 ) else : uid = settings_dict . get ( \"linker_uid\" , ascii_uid ( 8 )) settings_dict [ \"linker_uid\" ] = uid if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _validate_dialect () def _initialise_df_concat ( self , materialise = False ): cache = self . _intermediate_table_cache concat_df = None if \"__splink__df_concat\" in cache : concat_df = cache [ \"__splink__df_concat\" ] elif \"__splink__df_concat_with_tf\" in cache : concat_df = cache [ \"__splink__df_concat_with_tf\" ] concat_df . templated_name = \"__splink__df_concat\" else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) if materialise : concat_df = self . _execute_sql_pipeline () cache [ \"__splink__df_concat\" ] = concat_df return concat_df def _initialise_df_concat_with_tf ( self , materialise = True ): cache = self . _intermediate_table_cache nodes_with_tf = None if \"__splink__df_concat_with_tf\" in cache : nodes_with_tf = cache [ \"__splink__df_concat_with_tf\" ] else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if materialise : nodes_with_tf = self . _execute_sql_pipeline () cache [ \"__splink__df_concat_with_tf\" ] = nodes_with_tf return nodes_with_tf def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : list [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name try : dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , ) except Exception as e : raise e finally : self . _pipeline . reset () return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename , materialise_as_hash = False , use_cache = False , ) self . _pipeline . reset () return dataframe def _execute_sql_against_backend ( self , sql , templated_name , physical_name ): raise NotImplementedError ( f \"_execute_sql_against_backend not implemented for { type ( self ) } \" ) def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) def _sql_to_splink_dataframe_checking_cache ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute sql, or if identical sql has been run before, return cached results. This function - is used by _execute_sql_pipeline to to execute SQL - or can be used directly if you have a single SQL statement that's not in a pipeline Return a SplinkDataFrame representing the results of the SQL \"\"\" to_hash = ( sql + self . _cache_uid ) . encode ( \"utf-8\" ) hash = hashlib . sha256 ( to_hash ) . hexdigest ()[: 9 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , table_name_hash ) else : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , output_tablename_templated , ) self . _names_of_tables_created_by_splink . add ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj_ ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain ' \"only a single input table\" , ) def _validate_dialect ( self ): settings_dialect = self . _settings_obj . _sql_dialect if settings_dialect != self . _sql_dialect : raise ValueError ( f \"Incompatible SQL dialect! `settings` dictionary uses \" f \"dialect { settings_dialect } , but expecting \" f \"' { self . _sql_dialect } ' for Linker of type { type ( self ) } \" ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training . blocking_rule } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): to_remove = set () for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : continue if name == \"__splink__df_concat_with_tf\" : if not retain_df_concat_with_tf : self . _delete_table_from_database ( name ) to_remove . add ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if not retain_term_frequency : self . _delete_table_from_database ( name ) to_remove . add ( name ) else : self . _delete_table_from_database ( name ) to_remove . add ( name ) self . _names_of_tables_created_by_splink = ( self . _names_of_tables_created_by_splink - to_remove ) def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def _raise_error_if_necessary_accuracy_columns_not_computed ( self ): rmc = self . _settings_obj . _retain_matching_columns if not ( rmc ): raise ValueError ( \"retain_matching_columns must be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Its current value is { rmc } . \" \"Please re-run your linkage with it set to True.\" ) def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ BlockingRule ( f \" { uid_l } = { uid_r } \" ) ] nodes_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : output_table_name = sql [ \"output_table_name\" ] output_table_name = output_table_name . replace ( \"predict\" , \"self_link\" ) self . _enqueue_sql ( sql [ \"sql\" ], output_table_name ) predictions = self . _execute_sql_pipeline ( input_dataframes = [ nodes_with_tf ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def _get_labels_tablename_from_input ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame ): if isinstance ( labels_splinkdataframe_or_table_name , SplinkDataFrame ): labels_tablename = labels_splinkdataframe_or_table_name . physical_name elif isinstance ( labels_splinkdataframe_or_table_name , str ): labels_tablename = labels_splinkdataframe_or_table_name else : raise ValueError ( \"The 'labels_splinkdataframe_or_table_name' argument\" \" must be of type SplinkDataframe or a string representing a tablename\" \" in the input database\" ) return labels_tablename def estimate_m_from_pairwise_labels ( self , labels_splinkdataframe_or_table_name ): \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. Note that at the moment, this method does not respect values in a `clerical_match_score` column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Args: labels_splinkdataframe_or_table_name (str): Name of table containing labels in the database or SplinkDataframe Examples: >>> pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\") >>> linker.register_table(pairwise_labels, \"labels\", overwrite=True) >>> linker.estimate_m_from_pairwise_labels(\"labels\") \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) estimate_m_from_pairwise_labels ( self , labels_tablename ) def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_column ( self , label_colname , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column. A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs Args: label_colname (str): Name of labels column in input data include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" return prediction_errors_from_label_column ( self , label_colname , include_false_positives , include_false_negatives , threshold , ) def match_weights_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weights_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def completeness_chart ( self , input_dataset : str = None , cols : list [ str ] = None ): \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data. Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, completeness will be computed for this table alone. Defaults to None. cols (List[str], optional): List of column names to calculate completeness. Default to None. Examples: >>> linker.completeness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.completeness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = completeness_data ( self , input_dataset , cols ) return completeness_chart ( records , input_dataset ) def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) def invalidate_cache ( self ): \"\"\"Invalidate the Splink cache. Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed. \"\"\" # Before Splink executes a SQL command, it checks the cache to see # whether a table already exists with the name of the output table # This function has the effect of changing the names of the output tables # to include a different unique id # As a result, any previously cached tables will not be found self . _cache_uid = ascii_uid ( 8 ) # As a result, any previously cached tables will not be found self . _intermediate_table_cache . invalidate_cache () # Also drop any existing splink tables from the database # Note, this is not actually necessary, it's just good housekeeping self . _delete_tables_created_by_splink_from_db () def register_table_input_nodes_concat_with_tf ( self , input_data , overwrite = False ): \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew. Args: input_data: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. overwrite (bool): Overwrite the table in the underlying database if it exists \"\"\" table_name_physical = \"__splink__df_concat_with_tf_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_concat_with_tf\" ] = splink_dataframe return splink_dataframe def register_table_predict ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_predict_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_predict\" ] = splink_dataframe return splink_dataframe def register_term_frequency_lookup ( self , input_data , col_name , overwrite = False ): input_col = InputColumn ( col_name , settings_obj = self . _settings_obj ) table_name_templated = colname_to_tf_tablename ( input_col ) table_name_physical = f \" { table_name_templated } _ { self . _cache_uid } \" splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ table_name_templated ] = splink_dataframe return splink_dataframe def register_labels_table ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_labels_\" + ascii_uid ( 8 ) splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) return splink_dataframe __init__ ( input_table_or_tables , settings_dict , set_up_basic_logging = True , input_table_aliases = None ) \u00b6 Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd . read_csv ( \"data_to_dedupe.csv\" ) >>> linker = DuckDBLinker ( df , settings_dict ) >>> # Example 2: Spark >>> df_1 = spark . read . parquet ( \"table_1/\" ) >>> df_2 = spark . read . parquet ( \"table_2/\" ) >>> linker = SparkLinker ( >>> [ df_1 , df_2 ], >>> settings_dict , >>> input_table_aliases = [ \"customers\" , \"contact_center_callers\" ] >>> ) Parameters: Name Type Description Default input_table_or_tables Union [ str , list ] Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. required settings_dict dict A Splink settings dictionary. If not provided when the object is created, can later be added using linker.load_settings() Defaults to None. required set_up_basic_logging bool If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. True input_table_aliases Union [ str , list ] Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. None Source code in splink/linker.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False cluster_pairwise_predictions_at_threshold ( df_predict , threshold_match_probability , pairwise_formatting = False , filter_pairwise_format_for_clusters = True ) \u00b6 Clusters the pairwise match predictions that result from linker.predict() into groups of connected record using the connected components graph clustering algorithm Records with an estimated match_probability above threshold_match_probability are considered to be a match (i.e. they represent the same entity). Parameters: Name Type Description Default df_predict SplinkDataFrame The results of linker.predict() required threshold_match_probability float Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. required pairwise_formatting bool Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. False filter_pairwise_format_for_clusters bool If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. True Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. Source code in splink/linker.py 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc cluster_studio_dashboard ( df_predict , df_clustered , out_path , sampling_method = 'random' , sample_size = 10 , cluster_ids = None , cluster_names = None , overwrite = False , return_html_as_string = False ) \u00b6 Generate an interactive html visualization of the predicted cluster and save to out_path . Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required df_clustered SplinkDataFrame The outputs of linker.cluster_pairwise_predictions_at_threshold() required out_path str The path (including filename) to save the html file to. required sampling_method str random or by_cluster_size . Defaults to random . 'random' sample_size int Number of clusters to show in the dahboard. Defaults to 10. 10 cluster_ids list The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the sampling_method and sample_size arguments. Defaults to None. None overwrite bool Overwrite the html file if it already exists? Defaults to False. False cluster_names list If provided, the dashboard will display these names in the selection box. Ony works in conjunction with cluster_ids . Defaults to None. None return_html_as_string If True, return the html as a string False Examples: >>> df_p = linker . predict () >>> df_c = linker . cluster_pairwise_predictions_at_threshold ( df_p , 0.5 ) >>> linker . cluster_studio_dashboard ( >>> df_p , df_c , [ 0 , 4 , 7 ], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered compare_two_records ( record_1 , record_2 ) \u00b6 Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Parameters: Name Type Description Default record_1 dict dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object required record_2 dict dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object required Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compare_two_records ( record_left , record_right ) Returns: Name Type Description SplinkDataFrame Pairwise comparison with scored prediction Source code in splink/linker.py 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions comparison_viewer_dashboard ( df_predict , out_path , overwrite = False , num_example_rows = 2 , return_html_as_string = False ) \u00b6 Generate an interactive html visualization of the linker's predictions and save to out_path . For more information see this video Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required out_path str The path (including filename) to save the html file to. required overwrite bool Overwrite the html file if it already exists? Defaults to False. False num_example_rows int Number of example rows per comparison vector. Defaults to 2. 2 return_html_as_string If True, return the html as a string False Examples: >>> df_predictions = linker . predict () >>> linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered count_num_comparisons_from_blocking_rule ( blocking_rule ) \u00b6 Compute the number of pairwise record comparisons that would be generated by a blocking rule Parameters: Name Type Description Default blocking_rule str The blocking rule to analyse required link_type str The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required unique_id_column_name str This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required Examples: >>> br = \"l.first_name = r.first_name\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 394 Returns: Name Type Description int int The number of comparisons generated by the blocking rule Source code in splink/linker.py 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] count_num_comparisons_from_blocking_rules_for_prediction ( df_predict ) \u00b6 Counts the maginal number of edges created from each of the blocking rules in blocking_rules_to_generate_predictions This is different to count_num_comparisons_from_blocking_rule because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in blocking_rules_to_generate_predictions Parameters: Name Type Description Default df_predict SplinkDataFrame SplinkDataFrame with match weights required Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> df_predict = linker . predict ( threshold_match_probability = 0.95 ) >>> count_pairwise = linker . count_num_comparisons_from_blocking_rules_for_prediction ( df_predict ) >>> count_pairwise . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. Source code in splink/linker.py 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis compute_tf_table ( column_name ) \u00b6 Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compute_tf_table ( \"surname\" ) >>> linker . compare_two_records ( record_left , record_right ) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker ( df ) >>> df_first_name_tf = linker . compute_tf_table ( \"first_name\" ) >>> df_first_name_tf . write . parquet ( \"folder/first_name_tf\" ) >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark . read . parquet ( \"folder/first_name_tf\" ) >>> df_first_name_tf . createOrReplaceTempView ( \"__splink__df_tf_first_name\" ) Parameters: Name Type Description Default column_name str The column name in the input table required Returns: Name Type Description SplinkDataFrame SplinkDataFrame The resultant table as a splink data frame Source code in splink/linker.py 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df cumulative_comparisons_from_blocking_rules_records ( blocking_rules = None ) \u00b6 Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_comparisons_from_blocking_rules_records () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_comparisons_from_blocking_rules_records ( >>> blocking_rules >>> ) Returns: Name Type Description List A list of blocking rules and the corresponding number of comparisons it is forecast to generate. Source code in splink/linker.py 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records cumulative_num_comparisons_from_blocking_rules_chart ( blocking_rules = None ) \u00b6 Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart ( >>> blocking_rules >>> ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) deterministic_link () \u00b6 Uses the blocking rules specified by blocking_rules_to_generate_predictions in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker ( df ) >>> >>> settings = { >>> \"link_type\" : \"dedupe_only\" , >>> \"blocking_rules_to_generate_predictions\" : [ >>> \"l.first_name = r.first_name\" , >>> \"l.surname = r.surname\" , >>> ], >>> \"comparisons\" : [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker ( df , settings ) >>> df = linker . deterministic_link () Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) estimate_m_from_label_column ( label_colname ) \u00b6 Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Parameters: Name Type Description Default label_colname str The name of the column containing the ground truth label in the input data. required Examples: >>> linker . estimate_m_from_label_column ( \"social_security_number\" ) Returns: Type Description Updates the estimated m parameters within the linker object and returns nothing. Source code in splink/linker.py 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () estimate_parameters_using_expectation_maximisation ( blocking_rule , comparisons_to_deactivate = None , comparison_levels_to_reverse_blocking_rule = None , fix_probability_two_random_records_match = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False ) \u00b6 Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from linker.estimate_u_using_random_sampling() . You can change this by setting fix_u_probabilities to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is l.first_name = r.first_name , then parameter esimates will be made for all comparison except those which use first_name in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify comparisons_to_deactivate and comparison_levels_to_reverse_blocking_rule . This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( br_training ) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker . _settings_obj >>> comp = settings_obj . _get_comparison_by_output_column_name ( \"first_name\" ) >>> dmeta_level = comp . _get_comparison_level_by_comparison_vector_value ( 1 ) >>> linker . estimate_parameters_using_expectation_maximisation ( >>> br_training , >>> comparisons_to_deactivate = [ \"first_name\" ], >>> comparison_levels_to_reverse_blocking_rule = [ dmeta_level ], >>> ) Parameters: Name Type Description Default blocking_rule str The blocking rule used to generate pairwise record comparisons. required comparisons_to_deactivate list By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. None comparison_levels_to_reverse_blocking_rule list By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. None fix_probability_two_random_records_match bool If True, do not update the probability two random records match after each iteration. Defaults to False. False fix_m_probabilities bool If True, do not update the m probabilities after each iteration. Defaults to False. False fix_u_probabilities bool If True, do not update the u probabilities after each iteration. Defaults to True. True Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) Returns: Name Type Description EMTrainingSession EMTrainingSession An object containing information about the training session such as how parameters changed during the iteration history Source code in splink/linker.py 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session estimate_probability_two_random_records_match ( deterministic_matching_rules , recall ) \u00b6 Estimate the model parameter probability_two_random_records_match using a direct estimation approach. See here for discussion of methodology Parameters: Name Type Description Default deterministic_matching_rules list A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives required recall float A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules required Source code in splink/linker.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) estimate_u_using_random_sampling ( max_pairs = None , * , target_rows = None ) \u00b6 Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Parameters: Name Type Description Default max_pairs int The maximum number of pairwise record comparisons to None Examples: >>> linker . estimate_u_using_random_sampling ( 1e8 ) Returns: Name Type Description None Updates the estimated u parameters within the linker object and returns nothing. Source code in splink/linker.py 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () find_matches_to_new_records ( records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 ) \u00b6 Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Parameters: Name Type Description Default records_or_tablename List [ dict ] Input search record(s) as list of dict, or a table registered to the database. required blocking_rules list Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. [] match_weight_threshold int Return matches with a match weight above this threshold. Defaults to -4. -4 Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker . compute_tf_table ( \"first_name\" ) >>> record = { 'unique_id' : 1 , >>> 'first_name' : \"John\" , >>> 'surname' : \"Smith\" , >>> 'dob' : \"1971-05-24\" , >>> 'city' : \"London\" , >>> 'email' : \"john@smith.net\" >>> } >>> df = linker . find_matches_to_new_records ([ record ], blocking_rules = []) Returns: Name Type Description SplinkDataFrame SplinkDataFrame The pairwise comparisons. Source code in splink/linker.py 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions load_settings ( settings_dict ) \u00b6 Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ([ \"first_name\" , \"surname\" ]) >>> linker . load_settings ( settings_dict ) >>> linker . load_settings ( \"my_settings.json\" ) Parameters: Name Type Description Default settings_dict dict | str | Path A Splink settings dictionary or the path to your settings json file. required Source code in splink/linker.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () initialise_settings ( settings_dict ) \u00b6 This method is now deprecated. Please use load_settings when loading existing settings or a pre-trained model. Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ([ \"first_name\" , \"surname\" ]) >>> linker . initialise_settings ( settings_dict ) Parameters: Name Type Description Default settings_dict dict A Splink settings dictionary required Source code in splink/linker.py 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) load_settings_from_json ( in_path ) \u00b6 This method is now deprecated. Please use load_settings when loading existing settings or a pre-trained model. Load settings from a .json file. This .json file would usually be the output of linker.save_settings_to_json() Examples: >>> linker . load_settings_from_json ( \"my_settings.json\" ) Parameters: Name Type Description Default in_path str Path to settings json file required Source code in splink/linker.py 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) m_u_parameters_chart () \u00b6 Display a chart of the m and u parameters of the linkage model Examples: >>> linker . m_u_parameters_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () match_weights_chart () \u00b6 Display a chart of the (partial) match weights of the linkage model Examples: >>> linker . match_weights_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () missingness_chart ( input_dataset = None ) \u00b6 Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Parameters: Name Type Description Default input_dataset str Name of one of the input tables in the None Examples: >>> linker . missingness_chart () >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . missingness_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 Source code in splink/linker.py 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) parameter_estimate_comparisons_chart ( include_m = True , include_u = True ) \u00b6 Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Parameters: Name Type Description Default include_m bool Show different estimates of m values. Defaults to True. True include_u bool Show different estimates of u values. Defaults to True. True Source code in splink/linker.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) precision_recall_chart_from_labels_column ( labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . precision_recall_chart_from_labels_column ( \"ground_truth\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) precision_recall_chart_from_labels_table ( labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) predict ( threshold_match_probability = None , threshold_match_weight = None , materialise_after_computing_term_frequencies = True ) \u00b6 Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the blocking_rules_to_generate_predictions of the settings dictionary to generate the pairwise comparisons. Parameters: Name Type Description Default threshold_match_probability float If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. None threshold_match_weight float If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. None materialise_after_computing_term_frequencies bool If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True True Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> df = linker . predict ( threshold_match_probability = 0.95 ) >>> df . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions prediction_errors_from_labels_table ( labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 ) \u00b6 Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required include_false_positives bool Defaults to True. True include_false_negatives bool Defaults to True. True threshold float Threshold above which a score is considered to be a match. Defaults to 0.5. 0.5 Returns: Name Type Description SplinkDataFrame Table containing false positives and negatives Source code in splink/linker.py 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) profile_columns ( column_expressions , top_n = 10 , bottom_n = 10 ) \u00b6 Source code in splink/linker.py 1630 1631 1632 1633 def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) query_sql ( sql , output_type = 'pandas' ) \u00b6 Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker ( df , settings ) >>> df_predict = linker . predict () >>> linker . query_sql ( f \"select * from { df_predict . physical_name } limit 10\" ) Parameters: Name Type Description Default sql str The SQL to be queried. required output_type str One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. 'pandas' Source code in splink/linker.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) register_table ( input , table_name , overwrite = False ) \u00b6 Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = { \"a\" : [ 666 , 777 , 888 ], \"b\" : [ 4 , 5 , 6 ]} >>> linker . register_table ( test_dict , \"test_dict\" ) >>> linker . query_sql ( \"select * from test_dict\" ) Parameters: Name Type Description Default input The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. required table_name str The name you wish to assign to the table. required overwrite bool Overwrite the table in the underlying database if it exists False Returns: Name Type Description SplinkDataFrame An abstraction representing the table created by the sql pipeline Source code in splink/linker.py 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) roc_chart_from_labels_column ( labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . roc_chart_from_labels_column ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) roc_chart_from_labels_table ( labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) save_settings_to_json ( out_path = None , overwrite = False ) \u00b6 Save the configuration and parameters of the linkage model to a .json file. The model can later be loaded back in using linker.load_settings() . The settings dict is also returned in case you want to save it a different way. Examples: >>> linker . save_settings_to_json ( \"my_settings.json\" , overwrite = True ) Parameters: Name Type Description Default out_path str File path for json file. If None, don't save to file. Defaults to None. None overwrite bool Overwrite if already exists? Defaults to False. False Returns: Name Type Description dict dict The settings as a dictionary. Source code in splink/linker.py 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict truth_space_table_from_labels_column ( labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . truth_space_table_from_labels_column ( \"cluster\" ) Returns: Name Type Description SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) truth_space_table_from_labels_table ( labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) unlinkables_chart ( x_col = 'match_weight' , source_dataset = None , as_dict = False ) \u00b6 Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) waterfall_chart ( records , filter_nulls = True ) \u00b6 Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. Examples: >>> df = linker . predict ( threshold_match_weight = 2 ) >>> records = df . as_record_dict ( limit = 10 ) >>> linker . waterfall_chart ( records ) Parameters: Name Type Description Default records List [ dict ] Usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. required filter_nulls bool Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. True Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls )","title":"Full API"},{"location":"linker.html#documentation-for-linker-object","text":"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . Source code in splink/linker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 class Linker : \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a `DuckDBLinker`. \"\"\" def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False @property def _cache_uid ( self ): if self . _settings_dict : return self . _settings_obj . _cache_uid else : return self . _cache_uid_no_settings @_cache_uid . setter def _cache_uid ( self , value ): if self . _settings_dict : self . _settings_obj . _cache_uid = value else : self . _cache_uid_no_settings = value @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `load_settings()` method on your linker \" \"object. i.e. linker.load_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink__df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True # in u-train sample mode we are joining the concatenated table mixing # both data sets - hence if we inner join on True we will end up with # samples which both originate from the same dataset if self . _train_u_using_random_sample_mode : return False if self . _analyse_blocking_mode : return False if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False @property def _sql_dialect ( self ): if self . _sql_dialect_ is None : raise NotImplementedError ( f \"No SQL dialect set on object of type { type ( self ) } . \" \"Did you make sure to create a dialect-specific Linker?\" ) return self . _sql_dialect_ @property def _infinity_expression ( self ): raise NotImplementedError ( f \"infinity sql expression not available for { type ( self ) } \" ) def _setup_settings_objs ( self , settings_dict ): # Setup the linker class's required settings self . _settings_dict = settings_dict # if settings_dict is passed, set sql_dialect on it if missing, and make sure # incompatible dialect not passed if settings_dict is not None and settings_dict . get ( \"sql_dialect\" , None ) is None : settings_dict [ \"sql_dialect\" ] = self . _sql_dialect if settings_dict is None : self . _cache_uid_no_settings = ascii_uid ( 8 ) else : uid = settings_dict . get ( \"linker_uid\" , ascii_uid ( 8 )) settings_dict [ \"linker_uid\" ] = uid if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _validate_dialect () def _initialise_df_concat ( self , materialise = False ): cache = self . _intermediate_table_cache concat_df = None if \"__splink__df_concat\" in cache : concat_df = cache [ \"__splink__df_concat\" ] elif \"__splink__df_concat_with_tf\" in cache : concat_df = cache [ \"__splink__df_concat_with_tf\" ] concat_df . templated_name = \"__splink__df_concat\" else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) if materialise : concat_df = self . _execute_sql_pipeline () cache [ \"__splink__df_concat\" ] = concat_df return concat_df def _initialise_df_concat_with_tf ( self , materialise = True ): cache = self . _intermediate_table_cache nodes_with_tf = None if \"__splink__df_concat_with_tf\" in cache : nodes_with_tf = cache [ \"__splink__df_concat_with_tf\" ] else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if materialise : nodes_with_tf = self . _execute_sql_pipeline () cache [ \"__splink__df_concat_with_tf\" ] = nodes_with_tf return nodes_with_tf def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : list [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name try : dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , ) except Exception as e : raise e finally : self . _pipeline . reset () return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename , materialise_as_hash = False , use_cache = False , ) self . _pipeline . reset () return dataframe def _execute_sql_against_backend ( self , sql , templated_name , physical_name ): raise NotImplementedError ( f \"_execute_sql_against_backend not implemented for { type ( self ) } \" ) def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) def _sql_to_splink_dataframe_checking_cache ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute sql, or if identical sql has been run before, return cached results. This function - is used by _execute_sql_pipeline to to execute SQL - or can be used directly if you have a single SQL statement that's not in a pipeline Return a SplinkDataFrame representing the results of the SQL \"\"\" to_hash = ( sql + self . _cache_uid ) . encode ( \"utf-8\" ) hash = hashlib . sha256 ( to_hash ) . hexdigest ()[: 9 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , table_name_hash ) else : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , output_tablename_templated , ) self . _names_of_tables_created_by_splink . add ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj_ ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain ' \"only a single input table\" , ) def _validate_dialect ( self ): settings_dialect = self . _settings_obj . _sql_dialect if settings_dialect != self . _sql_dialect : raise ValueError ( f \"Incompatible SQL dialect! `settings` dictionary uses \" f \"dialect { settings_dialect } , but expecting \" f \"' { self . _sql_dialect } ' for Linker of type { type ( self ) } \" ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training . blocking_rule } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): to_remove = set () for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : continue if name == \"__splink__df_concat_with_tf\" : if not retain_df_concat_with_tf : self . _delete_table_from_database ( name ) to_remove . add ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if not retain_term_frequency : self . _delete_table_from_database ( name ) to_remove . add ( name ) else : self . _delete_table_from_database ( name ) to_remove . add ( name ) self . _names_of_tables_created_by_splink = ( self . _names_of_tables_created_by_splink - to_remove ) def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def _raise_error_if_necessary_accuracy_columns_not_computed ( self ): rmc = self . _settings_obj . _retain_matching_columns if not ( rmc ): raise ValueError ( \"retain_matching_columns must be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Its current value is { rmc } . \" \"Please re-run your linkage with it set to True.\" ) def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ BlockingRule ( f \" { uid_l } = { uid_r } \" ) ] nodes_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : output_table_name = sql [ \"output_table_name\" ] output_table_name = output_table_name . replace ( \"predict\" , \"self_link\" ) self . _enqueue_sql ( sql [ \"sql\" ], output_table_name ) predictions = self . _execute_sql_pipeline ( input_dataframes = [ nodes_with_tf ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def _get_labels_tablename_from_input ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame ): if isinstance ( labels_splinkdataframe_or_table_name , SplinkDataFrame ): labels_tablename = labels_splinkdataframe_or_table_name . physical_name elif isinstance ( labels_splinkdataframe_or_table_name , str ): labels_tablename = labels_splinkdataframe_or_table_name else : raise ValueError ( \"The 'labels_splinkdataframe_or_table_name' argument\" \" must be of type SplinkDataframe or a string representing a tablename\" \" in the input database\" ) return labels_tablename def estimate_m_from_pairwise_labels ( self , labels_splinkdataframe_or_table_name ): \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. Note that at the moment, this method does not respect values in a `clerical_match_score` column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Args: labels_splinkdataframe_or_table_name (str): Name of table containing labels in the database or SplinkDataframe Examples: >>> pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\") >>> linker.register_table(pairwise_labels, \"labels\", overwrite=True) >>> linker.estimate_m_from_pairwise_labels(\"labels\") \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) estimate_m_from_pairwise_labels ( self , labels_tablename ) def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_column ( self , label_colname , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column. A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs Args: label_colname (str): Name of labels column in input data include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" return prediction_errors_from_label_column ( self , label_colname , include_false_positives , include_false_negatives , threshold , ) def match_weights_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weights_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def completeness_chart ( self , input_dataset : str = None , cols : list [ str ] = None ): \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data. Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, completeness will be computed for this table alone. Defaults to None. cols (List[str], optional): List of column names to calculate completeness. Default to None. Examples: >>> linker.completeness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.completeness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = completeness_data ( self , input_dataset , cols ) return completeness_chart ( records , input_dataset ) def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) def invalidate_cache ( self ): \"\"\"Invalidate the Splink cache. Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed. \"\"\" # Before Splink executes a SQL command, it checks the cache to see # whether a table already exists with the name of the output table # This function has the effect of changing the names of the output tables # to include a different unique id # As a result, any previously cached tables will not be found self . _cache_uid = ascii_uid ( 8 ) # As a result, any previously cached tables will not be found self . _intermediate_table_cache . invalidate_cache () # Also drop any existing splink tables from the database # Note, this is not actually necessary, it's just good housekeeping self . _delete_tables_created_by_splink_from_db () def register_table_input_nodes_concat_with_tf ( self , input_data , overwrite = False ): \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew. Args: input_data: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. overwrite (bool): Overwrite the table in the underlying database if it exists \"\"\" table_name_physical = \"__splink__df_concat_with_tf_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_concat_with_tf\" ] = splink_dataframe return splink_dataframe def register_table_predict ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_predict_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_predict\" ] = splink_dataframe return splink_dataframe def register_term_frequency_lookup ( self , input_data , col_name , overwrite = False ): input_col = InputColumn ( col_name , settings_obj = self . _settings_obj ) table_name_templated = colname_to_tf_tablename ( input_col ) table_name_physical = f \" { table_name_templated } _ { self . _cache_uid } \" splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ table_name_templated ] = splink_dataframe return splink_dataframe def register_labels_table ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_labels_\" + ascii_uid ( 8 ) splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) return splink_dataframe","title":"Documentation for Linker object"},{"location":"linker.html#splink.linker.Linker.__init__","text":"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd . read_csv ( \"data_to_dedupe.csv\" ) >>> linker = DuckDBLinker ( df , settings_dict ) >>> # Example 2: Spark >>> df_1 = spark . read . parquet ( \"table_1/\" ) >>> df_2 = spark . read . parquet ( \"table_2/\" ) >>> linker = SparkLinker ( >>> [ df_1 , df_2 ], >>> settings_dict , >>> input_table_aliases = [ \"customers\" , \"contact_center_callers\" ] >>> ) Parameters: Name Type Description Default input_table_or_tables Union [ str , list ] Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. required settings_dict dict A Splink settings dictionary. If not provided when the object is created, can later be added using linker.load_settings() Defaults to None. required set_up_basic_logging bool If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. True input_table_aliases Union [ str , list ] Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. None Source code in splink/linker.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False","title":"__init__()"},{"location":"linker.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","text":"Clusters the pairwise match predictions that result from linker.predict() into groups of connected record using the connected components graph clustering algorithm Records with an estimated match_probability above threshold_match_probability are considered to be a match (i.e. they represent the same entity). Parameters: Name Type Description Default df_predict SplinkDataFrame The results of linker.predict() required threshold_match_probability float Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. required pairwise_formatting bool Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. False filter_pairwise_format_for_clusters bool If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. True Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. Source code in splink/linker.py 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc","title":"cluster_pairwise_predictions_at_threshold()"},{"location":"linker.html#splink.linker.Linker.cluster_studio_dashboard","text":"Generate an interactive html visualization of the predicted cluster and save to out_path . Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required df_clustered SplinkDataFrame The outputs of linker.cluster_pairwise_predictions_at_threshold() required out_path str The path (including filename) to save the html file to. required sampling_method str random or by_cluster_size . Defaults to random . 'random' sample_size int Number of clusters to show in the dahboard. Defaults to 10. 10 cluster_ids list The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the sampling_method and sample_size arguments. Defaults to None. None overwrite bool Overwrite the html file if it already exists? Defaults to False. False cluster_names list If provided, the dashboard will display these names in the selection box. Ony works in conjunction with cluster_ids . Defaults to None. None return_html_as_string If True, return the html as a string False Examples: >>> df_p = linker . predict () >>> df_c = linker . cluster_pairwise_predictions_at_threshold ( df_p , 0.5 ) >>> linker . cluster_studio_dashboard ( >>> df_p , df_c , [ 0 , 4 , 7 ], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered","title":"cluster_studio_dashboard()"},{"location":"linker.html#splink.linker.Linker.compare_two_records","text":"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Parameters: Name Type Description Default record_1 dict dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object required record_2 dict dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object required Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compare_two_records ( record_left , record_right ) Returns: Name Type Description SplinkDataFrame Pairwise comparison with scored prediction Source code in splink/linker.py 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions","title":"compare_two_records()"},{"location":"linker.html#splink.linker.Linker.comparison_viewer_dashboard","text":"Generate an interactive html visualization of the linker's predictions and save to out_path . For more information see this video Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required out_path str The path (including filename) to save the html file to. required overwrite bool Overwrite the html file if it already exists? Defaults to False. False num_example_rows int Number of example rows per comparison vector. Defaults to 2. 2 return_html_as_string If True, return the html as a string False Examples: >>> df_predictions = linker . predict () >>> linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered","title":"comparison_viewer_dashboard()"},{"location":"linker.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","text":"Compute the number of pairwise record comparisons that would be generated by a blocking rule Parameters: Name Type Description Default blocking_rule str The blocking rule to analyse required link_type str The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required unique_id_column_name str This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required Examples: >>> br = \"l.first_name = r.first_name\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 394 Returns: Name Type Description int int The number of comparisons generated by the blocking rule Source code in splink/linker.py 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ]","title":"count_num_comparisons_from_blocking_rule()"},{"location":"linker.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","text":"Counts the maginal number of edges created from each of the blocking rules in blocking_rules_to_generate_predictions This is different to count_num_comparisons_from_blocking_rule because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in blocking_rules_to_generate_predictions Parameters: Name Type Description Default df_predict SplinkDataFrame SplinkDataFrame with match weights required Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> df_predict = linker . predict ( threshold_match_probability = 0.95 ) >>> count_pairwise = linker . count_num_comparisons_from_blocking_rules_for_prediction ( df_predict ) >>> count_pairwise . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. Source code in splink/linker.py 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis","title":"count_num_comparisons_from_blocking_rules_for_prediction()"},{"location":"linker.html#splink.linker.Linker.compute_tf_table","text":"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compute_tf_table ( \"surname\" ) >>> linker . compare_two_records ( record_left , record_right ) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker ( df ) >>> df_first_name_tf = linker . compute_tf_table ( \"first_name\" ) >>> df_first_name_tf . write . parquet ( \"folder/first_name_tf\" ) >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark . read . parquet ( \"folder/first_name_tf\" ) >>> df_first_name_tf . createOrReplaceTempView ( \"__splink__df_tf_first_name\" ) Parameters: Name Type Description Default column_name str The column name in the input table required Returns: Name Type Description SplinkDataFrame SplinkDataFrame The resultant table as a splink data frame Source code in splink/linker.py 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df","title":"compute_tf_table()"},{"location":"linker.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","text":"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_comparisons_from_blocking_rules_records () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_comparisons_from_blocking_rules_records ( >>> blocking_rules >>> ) Returns: Name Type Description List A list of blocking rules and the corresponding number of comparisons it is forecast to generate. Source code in splink/linker.py 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records","title":"cumulative_comparisons_from_blocking_rules_records()"},{"location":"linker.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","text":"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart ( >>> blocking_rules >>> ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records )","title":"cumulative_num_comparisons_from_blocking_rules_chart()"},{"location":"linker.html#splink.linker.Linker.deterministic_link","text":"Uses the blocking rules specified by blocking_rules_to_generate_predictions in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker ( df ) >>> >>> settings = { >>> \"link_type\" : \"dedupe_only\" , >>> \"blocking_rules_to_generate_predictions\" : [ >>> \"l.first_name = r.first_name\" , >>> \"l.surname = r.surname\" , >>> ], >>> \"comparisons\" : [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker ( df , settings ) >>> df = linker . deterministic_link () Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ])","title":"deterministic_link()"},{"location":"linker.html#splink.linker.Linker.estimate_m_from_label_column","text":"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Parameters: Name Type Description Default label_colname str The name of the column containing the ground truth label in the input data. required Examples: >>> linker . estimate_m_from_label_column ( \"social_security_number\" ) Returns: Type Description Updates the estimated m parameters within the linker object and returns nothing. Source code in splink/linker.py 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message ()","title":"estimate_m_from_label_column()"},{"location":"linker.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","text":"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from linker.estimate_u_using_random_sampling() . You can change this by setting fix_u_probabilities to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is l.first_name = r.first_name , then parameter esimates will be made for all comparison except those which use first_name in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify comparisons_to_deactivate and comparison_levels_to_reverse_blocking_rule . This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( br_training ) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker . _settings_obj >>> comp = settings_obj . _get_comparison_by_output_column_name ( \"first_name\" ) >>> dmeta_level = comp . _get_comparison_level_by_comparison_vector_value ( 1 ) >>> linker . estimate_parameters_using_expectation_maximisation ( >>> br_training , >>> comparisons_to_deactivate = [ \"first_name\" ], >>> comparison_levels_to_reverse_blocking_rule = [ dmeta_level ], >>> ) Parameters: Name Type Description Default blocking_rule str The blocking rule used to generate pairwise record comparisons. required comparisons_to_deactivate list By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. None comparison_levels_to_reverse_blocking_rule list By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. None fix_probability_two_random_records_match bool If True, do not update the probability two random records match after each iteration. Defaults to False. False fix_m_probabilities bool If True, do not update the m probabilities after each iteration. Defaults to False. False fix_u_probabilities bool If True, do not update the u probabilities after each iteration. Defaults to True. True Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) Returns: Name Type Description EMTrainingSession EMTrainingSession An object containing information about the training session such as how parameters changed during the iteration history Source code in splink/linker.py 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session","title":"estimate_parameters_using_expectation_maximisation()"},{"location":"linker.html#splink.linker.Linker.estimate_probability_two_random_records_match","text":"Estimate the model parameter probability_two_random_records_match using a direct estimation approach. See here for discussion of methodology Parameters: Name Type Description Default deterministic_matching_rules list A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives required recall float A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules required Source code in splink/linker.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" )","title":"estimate_probability_two_random_records_match()"},{"location":"linker.html#splink.linker.Linker.estimate_u_using_random_sampling","text":"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Parameters: Name Type Description Default max_pairs int The maximum number of pairwise record comparisons to None Examples: >>> linker . estimate_u_using_random_sampling ( 1e8 ) Returns: Name Type Description None Updates the estimated u parameters within the linker object and returns nothing. Source code in splink/linker.py 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message ()","title":"estimate_u_using_random_sampling()"},{"location":"linker.html#splink.linker.Linker.find_matches_to_new_records","text":"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Parameters: Name Type Description Default records_or_tablename List [ dict ] Input search record(s) as list of dict, or a table registered to the database. required blocking_rules list Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. [] match_weight_threshold int Return matches with a match weight above this threshold. Defaults to -4. -4 Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker . compute_tf_table ( \"first_name\" ) >>> record = { 'unique_id' : 1 , >>> 'first_name' : \"John\" , >>> 'surname' : \"Smith\" , >>> 'dob' : \"1971-05-24\" , >>> 'city' : \"London\" , >>> 'email' : \"john@smith.net\" >>> } >>> df = linker . find_matches_to_new_records ([ record ], blocking_rules = []) Returns: Name Type Description SplinkDataFrame SplinkDataFrame The pairwise comparisons. Source code in splink/linker.py 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions","title":"find_matches_to_new_records()"},{"location":"linker.html#splink.linker.Linker.load_settings","text":"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ([ \"first_name\" , \"surname\" ]) >>> linker . load_settings ( settings_dict ) >>> linker . load_settings ( \"my_settings.json\" ) Parameters: Name Type Description Default settings_dict dict | str | Path A Splink settings dictionary or the path to your settings json file. required Source code in splink/linker.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect ()","title":"load_settings()"},{"location":"linker.html#splink.linker.Linker.initialise_settings","text":"This method is now deprecated. Please use load_settings when loading existing settings or a pre-trained model. Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ([ \"first_name\" , \"surname\" ]) >>> linker . initialise_settings ( settings_dict ) Parameters: Name Type Description Default settings_dict dict A Splink settings dictionary required Source code in splink/linker.py 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , )","title":"initialise_settings()"},{"location":"linker.html#splink.linker.Linker.load_settings_from_json","text":"This method is now deprecated. Please use load_settings when loading existing settings or a pre-trained model. Load settings from a .json file. This .json file would usually be the output of linker.save_settings_to_json() Examples: >>> linker . load_settings_from_json ( \"my_settings.json\" ) Parameters: Name Type Description Default in_path str Path to settings json file required Source code in splink/linker.py 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , )","title":"load_settings_from_json()"},{"location":"linker.html#splink.linker.Linker.m_u_parameters_chart","text":"Display a chart of the m and u parameters of the linkage model Examples: >>> linker . m_u_parameters_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart ()","title":"m_u_parameters_chart()"},{"location":"linker.html#splink.linker.Linker.match_weights_chart","text":"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker . match_weights_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart ()","title":"match_weights_chart()"},{"location":"linker.html#splink.linker.Linker.missingness_chart","text":"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Parameters: Name Type Description Default input_dataset str Name of one of the input tables in the None Examples: >>> linker . missingness_chart () >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . missingness_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 Source code in splink/linker.py 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset )","title":"missingness_chart()"},{"location":"linker.html#splink.linker.Linker.parameter_estimate_comparisons_chart","text":"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Parameters: Name Type Description Default include_m bool Show different estimates of m values. Defaults to True. True include_u bool Show different estimates of u values. Defaults to True. True Source code in splink/linker.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records )","title":"parameter_estimate_comparisons_chart()"},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels_column","text":"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . precision_recall_chart_from_labels_column ( \"ground_truth\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs )","title":"precision_recall_chart_from_labels_column()"},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels_table","text":"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs )","title":"precision_recall_chart_from_labels_table()"},{"location":"linker.html#splink.linker.Linker.predict","text":"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the blocking_rules_to_generate_predictions of the settings dictionary to generate the pairwise comparisons. Parameters: Name Type Description Default threshold_match_probability float If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. None threshold_match_weight float If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. None materialise_after_computing_term_frequencies bool If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True True Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> df = linker . predict ( threshold_match_probability = 0.95 ) >>> df . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions","title":"predict()"},{"location":"linker.html#splink.linker.Linker.prediction_errors_from_labels_table","text":"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required include_false_positives bool Defaults to True. True include_false_negatives bool Defaults to True. True threshold float Threshold above which a score is considered to be a match. Defaults to 0.5. 0.5 Returns: Name Type Description SplinkDataFrame Table containing false positives and negatives Source code in splink/linker.py 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , )","title":"prediction_errors_from_labels_table()"},{"location":"linker.html#splink.linker.Linker.profile_columns","text":"Source code in splink/linker.py 1630 1631 1632 1633 def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n )","title":"profile_columns()"},{"location":"linker.html#splink.linker.Linker.query_sql","text":"Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker ( df , settings ) >>> df_predict = linker . predict () >>> linker . query_sql ( f \"select * from { df_predict . physical_name } limit 10\" ) Parameters: Name Type Description Default sql str The SQL to be queried. required output_type str One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. 'pandas' Source code in splink/linker.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , )","title":"query_sql()"},{"location":"linker.html#splink.linker.Linker.register_table","text":"Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = { \"a\" : [ 666 , 777 , 888 ], \"b\" : [ 4 , 5 , 6 ]} >>> linker . register_table ( test_dict , \"test_dict\" ) >>> linker . query_sql ( \"select * from test_dict\" ) Parameters: Name Type Description Default input The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. required table_name str The name you wish to assign to the table. required overwrite bool Overwrite the table in the underlying database if it exists False Returns: Name Type Description SplinkDataFrame An abstraction representing the table created by the sql pipeline Source code in splink/linker.py 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" )","title":"register_table()"},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels_column","text":"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . roc_chart_from_labels_column ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs )","title":"roc_chart_from_labels_column()"},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels_table","text":"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs )","title":"roc_chart_from_labels_table()"},{"location":"linker.html#splink.linker.Linker.save_settings_to_json","text":"Save the configuration and parameters of the linkage model to a .json file. The model can later be loaded back in using linker.load_settings() . The settings dict is also returned in case you want to save it a different way. Examples: >>> linker . save_settings_to_json ( \"my_settings.json\" , overwrite = True ) Parameters: Name Type Description Default out_path str File path for json file. If None, don't save to file. Defaults to None. None overwrite bool Overwrite if already exists? Defaults to False. False Returns: Name Type Description dict dict The settings as a dictionary. Source code in splink/linker.py 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict","title":"save_settings_to_json()"},{"location":"linker.html#splink.linker.Linker.truth_space_table_from_labels_column","text":"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . truth_space_table_from_labels_column ( \"cluster\" ) Returns: Name Type Description SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest )","title":"truth_space_table_from_labels_column()"},{"location":"linker.html#splink.linker.Linker.truth_space_table_from_labels_table","text":"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , )","title":"truth_space_table_from_labels_table()"},{"location":"linker.html#splink.linker.Linker.unlinkables_chart","text":"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict )","title":"unlinkables_chart()"},{"location":"linker.html#splink.linker.Linker.waterfall_chart","text":"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. Examples: >>> df = linker . predict ( threshold_match_weight = 2 ) >>> records = df . as_record_dict ( limit = 10 ) >>> linker . waterfall_chart ( records ) Parameters: Name Type Description Default records List [ dict ] Usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. required filter_nulls bool Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. True Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls )","title":"waterfall_chart()"},{"location":"linkerest.html","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"],"text":"Documentation for Linker object methods related to parameter estimation \u00b6 The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . estimate_m_from_label_column ( label_colname ) \u00b6 Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Parameters: Name Type Description Default label_colname str The name of the column containing the ground truth label in the input data. required Examples: >>> linker . estimate_m_from_label_column ( \"social_security_number\" ) Returns: Type Description Updates the estimated m parameters within the linker object and returns nothing. estimate_parameters_using_expectation_maximisation ( blocking_rule , comparisons_to_deactivate = None , comparison_levels_to_reverse_blocking_rule = None , fix_probability_two_random_records_match = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False ) \u00b6 Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from linker.estimate_u_using_random_sampling() . You can change this by setting fix_u_probabilities to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is l.first_name = r.first_name , then parameter esimates will be made for all comparison except those which use first_name in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify comparisons_to_deactivate and comparison_levels_to_reverse_blocking_rule . This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( br_training ) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker . _settings_obj >>> comp = settings_obj . _get_comparison_by_output_column_name ( \"first_name\" ) >>> dmeta_level = comp . _get_comparison_level_by_comparison_vector_value ( 1 ) >>> linker . estimate_parameters_using_expectation_maximisation ( >>> br_training , >>> comparisons_to_deactivate = [ \"first_name\" ], >>> comparison_levels_to_reverse_blocking_rule = [ dmeta_level ], >>> ) Parameters: Name Type Description Default blocking_rule str The blocking rule used to generate pairwise record comparisons. required comparisons_to_deactivate list By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. None comparison_levels_to_reverse_blocking_rule list By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. None fix_probability_two_random_records_match bool If True, do not update the probability two random records match after each iteration. Defaults to False. False fix_m_probabilities bool If True, do not update the m probabilities after each iteration. Defaults to False. False fix_u_probabilities bool If True, do not update the u probabilities after each iteration. Defaults to True. True Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) Returns: Name Type Description EMTrainingSession EMTrainingSession An object containing information about the training session such as how parameters changed during the iteration history estimate_u_using_random_sampling ( max_pairs = None , * , target_rows = None ) \u00b6 Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Parameters: Name Type Description Default max_pairs int The maximum number of pairwise record comparisons to None Examples: >>> linker . estimate_u_using_random_sampling ( 1e8 ) Returns: Name Type Description None Updates the estimated u parameters within the linker object and returns nothing. save_settings_to_json ( out_path = None , overwrite = False ) \u00b6 Save the configuration and parameters of the linkage model to a .json file. The model can later be loaded back in using linker.load_settings() . The settings dict is also returned in case you want to save it a different way. Examples: >>> linker . save_settings_to_json ( \"my_settings.json\" , overwrite = True ) Parameters: Name Type Description Default out_path str File path for json file. If None, don't save to file. Defaults to None. None overwrite bool Overwrite if already exists? Defaults to False. False Returns: Name Type Description dict dict The settings as a dictionary. estimate_m_from_pairwise_labels ( labels_splinkdataframe_or_table_name ) \u00b6 Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. Note that at the moment, this method does not respect values in a clerical_match_score column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str Name of table containing labels in the database or SplinkDataframe required Examples: >>> pairwise_labels = pd . read_csv ( \"./data/pairwise_labels_to_estimate_m.csv\" ) >>> linker . register_table ( pairwise_labels , \"labels\" , overwrite = True ) >>> linker . estimate_m_from_pairwise_labels ( \"labels\" ) estimate_probability_two_random_records_match ( deterministic_matching_rules , recall ) \u00b6 Estimate the model parameter probability_two_random_records_match using a direct estimation approach. See here for discussion of methodology Parameters: Name Type Description Default deterministic_matching_rules list A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives required recall float A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules required","title":"Estimating model parameters"},{"location":"linkerest.html#documentation-for-linker-object-methods-related-to-parameter-estimation","text":"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker .","title":"Documentation for Linker object methods related to parameter estimation"},{"location":"linkerest.html#splink.linker.Linker.estimate_m_from_label_column","text":"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Parameters: Name Type Description Default label_colname str The name of the column containing the ground truth label in the input data. required Examples: >>> linker . estimate_m_from_label_column ( \"social_security_number\" ) Returns: Type Description Updates the estimated m parameters within the linker object and returns nothing.","title":"estimate_m_from_label_column()"},{"location":"linkerest.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","text":"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from linker.estimate_u_using_random_sampling() . You can change this by setting fix_u_probabilities to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is l.first_name = r.first_name , then parameter esimates will be made for all comparison except those which use first_name in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify comparisons_to_deactivate and comparison_levels_to_reverse_blocking_rule . This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( br_training ) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker . _settings_obj >>> comp = settings_obj . _get_comparison_by_output_column_name ( \"first_name\" ) >>> dmeta_level = comp . _get_comparison_level_by_comparison_vector_value ( 1 ) >>> linker . estimate_parameters_using_expectation_maximisation ( >>> br_training , >>> comparisons_to_deactivate = [ \"first_name\" ], >>> comparison_levels_to_reverse_blocking_rule = [ dmeta_level ], >>> ) Parameters: Name Type Description Default blocking_rule str The blocking rule used to generate pairwise record comparisons. required comparisons_to_deactivate list By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. None comparison_levels_to_reverse_blocking_rule list By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. None fix_probability_two_random_records_match bool If True, do not update the probability two random records match after each iteration. Defaults to False. False fix_m_probabilities bool If True, do not update the m probabilities after each iteration. Defaults to False. False fix_u_probabilities bool If True, do not update the u probabilities after each iteration. Defaults to True. True Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) Returns: Name Type Description EMTrainingSession EMTrainingSession An object containing information about the training session such as how parameters changed during the iteration history","title":"estimate_parameters_using_expectation_maximisation()"},{"location":"linkerest.html#splink.linker.Linker.estimate_u_using_random_sampling","text":"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Parameters: Name Type Description Default max_pairs int The maximum number of pairwise record comparisons to None Examples: >>> linker . estimate_u_using_random_sampling ( 1e8 ) Returns: Name Type Description None Updates the estimated u parameters within the linker object and returns nothing.","title":"estimate_u_using_random_sampling()"},{"location":"linkerest.html#splink.linker.Linker.save_settings_to_json","text":"Save the configuration and parameters of the linkage model to a .json file. The model can later be loaded back in using linker.load_settings() . The settings dict is also returned in case you want to save it a different way. Examples: >>> linker . save_settings_to_json ( \"my_settings.json\" , overwrite = True ) Parameters: Name Type Description Default out_path str File path for json file. If None, don't save to file. Defaults to None. None overwrite bool Overwrite if already exists? Defaults to False. False Returns: Name Type Description dict dict The settings as a dictionary.","title":"save_settings_to_json()"},{"location":"linkerest.html#splink.linker.Linker.estimate_m_from_pairwise_labels","text":"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. Note that at the moment, this method does not respect values in a clerical_match_score column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str Name of table containing labels in the database or SplinkDataframe required Examples: >>> pairwise_labels = pd . read_csv ( \"./data/pairwise_labels_to_estimate_m.csv\" ) >>> linker . register_table ( pairwise_labels , \"labels\" , overwrite = True ) >>> linker . estimate_m_from_pairwise_labels ( \"labels\" )","title":"estimate_m_from_pairwise_labels()"},{"location":"linkerest.html#splink.linker.Linker.estimate_probability_two_random_records_match","text":"Estimate the model parameter probability_two_random_records_match using a direct estimation approach. See here for discussion of methodology Parameters: Name Type Description Default deterministic_matching_rules list A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives required recall float A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules required","title":"estimate_probability_two_random_records_match()"},{"location":"linkerexp.html","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"],"text":"Documentation for Linker object methods related to exploratory analysis \u00b6 The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . count_num_comparisons_from_blocking_rule ( blocking_rule ) \u00b6 Compute the number of pairwise record comparisons that would be generated by a blocking rule Parameters: Name Type Description Default blocking_rule str The blocking rule to analyse required link_type str The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required unique_id_column_name str This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required Examples: >>> br = \"l.first_name = r.first_name\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 394 Returns: Name Type Description int int The number of comparisons generated by the blocking rule cumulative_comparisons_from_blocking_rules_records ( blocking_rules = None ) \u00b6 Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_comparisons_from_blocking_rules_records () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_comparisons_from_blocking_rules_records ( >>> blocking_rules >>> ) Returns: Name Type Description List A list of blocking rules and the corresponding number of comparisons it is forecast to generate. cumulative_num_comparisons_from_blocking_rules_chart ( blocking_rules = None ) \u00b6 Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart ( >>> blocking_rules >>> ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. missingness_chart ( input_dataset = None ) \u00b6 Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Parameters: Name Type Description Default input_dataset str Name of one of the input tables in the None Examples: >>> linker . missingness_chart () >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . missingness_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 profile_columns ( column_expressions , top_n = 10 , bottom_n = 10 ) \u00b6 unlinkables_chart ( x_col = 'match_weight' , source_dataset = None , as_dict = False ) \u00b6 Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute.","title":"Exploratory analysis"},{"location":"linkerexp.html#documentation-for-linker-object-methods-related-to-exploratory-analysis","text":"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker .","title":"Documentation for Linker object methods related to exploratory analysis"},{"location":"linkerexp.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","text":"Compute the number of pairwise record comparisons that would be generated by a blocking rule Parameters: Name Type Description Default blocking_rule str The blocking rule to analyse required link_type str The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required unique_id_column_name str This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. required Examples: >>> br = \"l.first_name = r.first_name\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker . count_num_comparisons_from_blocking_rule ( br ) 394 Returns: Name Type Description int int The number of comparisons generated by the blocking rule","title":"count_num_comparisons_from_blocking_rule()"},{"location":"linkerexp.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","text":"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_comparisons_from_blocking_rules_records () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_comparisons_from_blocking_rules_records ( >>> blocking_rules >>> ) Returns: Name Type Description List A list of blocking rules and the corresponding number of comparisons it is forecast to generate.","title":"cumulative_comparisons_from_blocking_rules_records()"},{"location":"linkerexp.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","text":"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Parameters: Name Type Description Default blocking_rules str or list The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. None Examples: >>> linker_settings = DuckDBLinker ( df , settings ) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart () >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\" , >>> \"l.first_name = r.first_name >>> and substr ( l . dob , 1 , 4 ) = substr ( r . dob , 1 , 4 ) \" >>> ] >>> >>> linker_settings . cumulative_num_comparisons_from_blocking_rules_chart ( >>> blocking_rules >>> ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute.","title":"cumulative_num_comparisons_from_blocking_rules_chart()"},{"location":"linkerexp.html#splink.linker.Linker.missingness_chart","text":"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Parameters: Name Type Description Default input_dataset str Name of one of the input tables in the None Examples: >>> linker . missingness_chart () >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . missingness_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500","title":"missingness_chart()"},{"location":"linkerexp.html#splink.linker.Linker.profile_columns","text":"","title":"profile_columns()"},{"location":"linkerexp.html#splink.linker.Linker.unlinkables_chart","text":"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute.","title":"unlinkables_chart()"},{"location":"linkerpred.html","tags":["API","Prediction"],"text":"Documentation for Linker object methods related to link prediction \u00b6 The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . Source code in splink/linker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 class Linker : \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a `DuckDBLinker`. \"\"\" def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False @property def _cache_uid ( self ): if self . _settings_dict : return self . _settings_obj . _cache_uid else : return self . _cache_uid_no_settings @_cache_uid . setter def _cache_uid ( self , value ): if self . _settings_dict : self . _settings_obj . _cache_uid = value else : self . _cache_uid_no_settings = value @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `load_settings()` method on your linker \" \"object. i.e. linker.load_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink__df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True # in u-train sample mode we are joining the concatenated table mixing # both data sets - hence if we inner join on True we will end up with # samples which both originate from the same dataset if self . _train_u_using_random_sample_mode : return False if self . _analyse_blocking_mode : return False if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False @property def _sql_dialect ( self ): if self . _sql_dialect_ is None : raise NotImplementedError ( f \"No SQL dialect set on object of type { type ( self ) } . \" \"Did you make sure to create a dialect-specific Linker?\" ) return self . _sql_dialect_ @property def _infinity_expression ( self ): raise NotImplementedError ( f \"infinity sql expression not available for { type ( self ) } \" ) def _setup_settings_objs ( self , settings_dict ): # Setup the linker class's required settings self . _settings_dict = settings_dict # if settings_dict is passed, set sql_dialect on it if missing, and make sure # incompatible dialect not passed if settings_dict is not None and settings_dict . get ( \"sql_dialect\" , None ) is None : settings_dict [ \"sql_dialect\" ] = self . _sql_dialect if settings_dict is None : self . _cache_uid_no_settings = ascii_uid ( 8 ) else : uid = settings_dict . get ( \"linker_uid\" , ascii_uid ( 8 )) settings_dict [ \"linker_uid\" ] = uid if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _validate_dialect () def _initialise_df_concat ( self , materialise = False ): cache = self . _intermediate_table_cache concat_df = None if \"__splink__df_concat\" in cache : concat_df = cache [ \"__splink__df_concat\" ] elif \"__splink__df_concat_with_tf\" in cache : concat_df = cache [ \"__splink__df_concat_with_tf\" ] concat_df . templated_name = \"__splink__df_concat\" else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) if materialise : concat_df = self . _execute_sql_pipeline () cache [ \"__splink__df_concat\" ] = concat_df return concat_df def _initialise_df_concat_with_tf ( self , materialise = True ): cache = self . _intermediate_table_cache nodes_with_tf = None if \"__splink__df_concat_with_tf\" in cache : nodes_with_tf = cache [ \"__splink__df_concat_with_tf\" ] else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if materialise : nodes_with_tf = self . _execute_sql_pipeline () cache [ \"__splink__df_concat_with_tf\" ] = nodes_with_tf return nodes_with_tf def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : list [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name try : dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , ) except Exception as e : raise e finally : self . _pipeline . reset () return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename , materialise_as_hash = False , use_cache = False , ) self . _pipeline . reset () return dataframe def _execute_sql_against_backend ( self , sql , templated_name , physical_name ): raise NotImplementedError ( f \"_execute_sql_against_backend not implemented for { type ( self ) } \" ) def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) def _sql_to_splink_dataframe_checking_cache ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute sql, or if identical sql has been run before, return cached results. This function - is used by _execute_sql_pipeline to to execute SQL - or can be used directly if you have a single SQL statement that's not in a pipeline Return a SplinkDataFrame representing the results of the SQL \"\"\" to_hash = ( sql + self . _cache_uid ) . encode ( \"utf-8\" ) hash = hashlib . sha256 ( to_hash ) . hexdigest ()[: 9 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , table_name_hash ) else : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , output_tablename_templated , ) self . _names_of_tables_created_by_splink . add ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj_ ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain ' \"only a single input table\" , ) def _validate_dialect ( self ): settings_dialect = self . _settings_obj . _sql_dialect if settings_dialect != self . _sql_dialect : raise ValueError ( f \"Incompatible SQL dialect! `settings` dictionary uses \" f \"dialect { settings_dialect } , but expecting \" f \"' { self . _sql_dialect } ' for Linker of type { type ( self ) } \" ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training . blocking_rule } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): to_remove = set () for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : continue if name == \"__splink__df_concat_with_tf\" : if not retain_df_concat_with_tf : self . _delete_table_from_database ( name ) to_remove . add ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if not retain_term_frequency : self . _delete_table_from_database ( name ) to_remove . add ( name ) else : self . _delete_table_from_database ( name ) to_remove . add ( name ) self . _names_of_tables_created_by_splink = ( self . _names_of_tables_created_by_splink - to_remove ) def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def _raise_error_if_necessary_accuracy_columns_not_computed ( self ): rmc = self . _settings_obj . _retain_matching_columns if not ( rmc ): raise ValueError ( \"retain_matching_columns must be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Its current value is { rmc } . \" \"Please re-run your linkage with it set to True.\" ) def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ BlockingRule ( f \" { uid_l } = { uid_r } \" ) ] nodes_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : output_table_name = sql [ \"output_table_name\" ] output_table_name = output_table_name . replace ( \"predict\" , \"self_link\" ) self . _enqueue_sql ( sql [ \"sql\" ], output_table_name ) predictions = self . _execute_sql_pipeline ( input_dataframes = [ nodes_with_tf ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def _get_labels_tablename_from_input ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame ): if isinstance ( labels_splinkdataframe_or_table_name , SplinkDataFrame ): labels_tablename = labels_splinkdataframe_or_table_name . physical_name elif isinstance ( labels_splinkdataframe_or_table_name , str ): labels_tablename = labels_splinkdataframe_or_table_name else : raise ValueError ( \"The 'labels_splinkdataframe_or_table_name' argument\" \" must be of type SplinkDataframe or a string representing a tablename\" \" in the input database\" ) return labels_tablename def estimate_m_from_pairwise_labels ( self , labels_splinkdataframe_or_table_name ): \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. Note that at the moment, this method does not respect values in a `clerical_match_score` column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Args: labels_splinkdataframe_or_table_name (str): Name of table containing labels in the database or SplinkDataframe Examples: >>> pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\") >>> linker.register_table(pairwise_labels, \"labels\", overwrite=True) >>> linker.estimate_m_from_pairwise_labels(\"labels\") \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) estimate_m_from_pairwise_labels ( self , labels_tablename ) def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_column ( self , label_colname , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column. A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs Args: label_colname (str): Name of labels column in input data include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" return prediction_errors_from_label_column ( self , label_colname , include_false_positives , include_false_negatives , threshold , ) def match_weights_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weights_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def completeness_chart ( self , input_dataset : str = None , cols : list [ str ] = None ): \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data. Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, completeness will be computed for this table alone. Defaults to None. cols (List[str], optional): List of column names to calculate completeness. Default to None. Examples: >>> linker.completeness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.completeness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = completeness_data ( self , input_dataset , cols ) return completeness_chart ( records , input_dataset ) def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) def invalidate_cache ( self ): \"\"\"Invalidate the Splink cache. Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed. \"\"\" # Before Splink executes a SQL command, it checks the cache to see # whether a table already exists with the name of the output table # This function has the effect of changing the names of the output tables # to include a different unique id # As a result, any previously cached tables will not be found self . _cache_uid = ascii_uid ( 8 ) # As a result, any previously cached tables will not be found self . _intermediate_table_cache . invalidate_cache () # Also drop any existing splink tables from the database # Note, this is not actually necessary, it's just good housekeeping self . _delete_tables_created_by_splink_from_db () def register_table_input_nodes_concat_with_tf ( self , input_data , overwrite = False ): \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew. Args: input_data: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. overwrite (bool): Overwrite the table in the underlying database if it exists \"\"\" table_name_physical = \"__splink__df_concat_with_tf_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_concat_with_tf\" ] = splink_dataframe return splink_dataframe def register_table_predict ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_predict_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_predict\" ] = splink_dataframe return splink_dataframe def register_term_frequency_lookup ( self , input_data , col_name , overwrite = False ): input_col = InputColumn ( col_name , settings_obj = self . _settings_obj ) table_name_templated = colname_to_tf_tablename ( input_col ) table_name_physical = f \" { table_name_templated } _ { self . _cache_uid } \" splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ table_name_templated ] = splink_dataframe return splink_dataframe def register_labels_table ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_labels_\" + ascii_uid ( 8 ) splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) return splink_dataframe cluster_pairwise_predictions_at_threshold ( df_predict , threshold_match_probability , pairwise_formatting = False , filter_pairwise_format_for_clusters = True ) \u00b6 Clusters the pairwise match predictions that result from linker.predict() into groups of connected record using the connected components graph clustering algorithm Records with an estimated match_probability above threshold_match_probability are considered to be a match (i.e. they represent the same entity). Parameters: Name Type Description Default df_predict SplinkDataFrame The results of linker.predict() required threshold_match_probability float Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. required pairwise_formatting bool Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. False filter_pairwise_format_for_clusters bool If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. True Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. Source code in splink/linker.py 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc compare_two_records ( record_1 , record_2 ) \u00b6 Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Parameters: Name Type Description Default record_1 dict dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object required record_2 dict dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object required Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compare_two_records ( record_left , record_right ) Returns: Name Type Description SplinkDataFrame Pairwise comparison with scored prediction Source code in splink/linker.py 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions compute_tf_table ( column_name ) \u00b6 Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compute_tf_table ( \"surname\" ) >>> linker . compare_two_records ( record_left , record_right ) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker ( df ) >>> df_first_name_tf = linker . compute_tf_table ( \"first_name\" ) >>> df_first_name_tf . write . parquet ( \"folder/first_name_tf\" ) >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark . read . parquet ( \"folder/first_name_tf\" ) >>> df_first_name_tf . createOrReplaceTempView ( \"__splink__df_tf_first_name\" ) Parameters: Name Type Description Default column_name str The column name in the input table required Returns: Name Type Description SplinkDataFrame SplinkDataFrame The resultant table as a splink data frame Source code in splink/linker.py 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df deterministic_link () \u00b6 Uses the blocking rules specified by blocking_rules_to_generate_predictions in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker ( df ) >>> >>> settings = { >>> \"link_type\" : \"dedupe_only\" , >>> \"blocking_rules_to_generate_predictions\" : [ >>> \"l.first_name = r.first_name\" , >>> \"l.surname = r.surname\" , >>> ], >>> \"comparisons\" : [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker ( df , settings ) >>> df = linker . deterministic_link () Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) find_matches_to_new_records ( records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 ) \u00b6 Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Parameters: Name Type Description Default records_or_tablename List [ dict ] Input search record(s) as list of dict, or a table registered to the database. required blocking_rules list Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. [] match_weight_threshold int Return matches with a match weight above this threshold. Defaults to -4. -4 Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker . compute_tf_table ( \"first_name\" ) >>> record = { 'unique_id' : 1 , >>> 'first_name' : \"John\" , >>> 'surname' : \"Smith\" , >>> 'dob' : \"1971-05-24\" , >>> 'city' : \"London\" , >>> 'email' : \"john@smith.net\" >>> } >>> df = linker . find_matches_to_new_records ([ record ], blocking_rules = []) Returns: Name Type Description SplinkDataFrame SplinkDataFrame The pairwise comparisons. Source code in splink/linker.py 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions load_settings ( settings_dict ) \u00b6 Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ([ \"first_name\" , \"surname\" ]) >>> linker . load_settings ( settings_dict ) >>> linker . load_settings ( \"my_settings.json\" ) Parameters: Name Type Description Default settings_dict dict | str | Path A Splink settings dictionary or the path to your settings json file. required Source code in splink/linker.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () load_settings_from_json ( in_path ) \u00b6 This method is now deprecated. Please use load_settings when loading existing settings or a pre-trained model. Load settings from a .json file. This .json file would usually be the output of linker.save_settings_to_json() Examples: >>> linker . load_settings_from_json ( \"my_settings.json\" ) Parameters: Name Type Description Default in_path str Path to settings json file required Source code in splink/linker.py 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) predict ( threshold_match_probability = None , threshold_match_weight = None , materialise_after_computing_term_frequencies = True ) \u00b6 Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the blocking_rules_to_generate_predictions of the settings dictionary to generate the pairwise comparisons. Parameters: Name Type Description Default threshold_match_probability float If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. None threshold_match_weight float If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. None materialise_after_computing_term_frequencies bool If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True True Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> df = linker . predict ( threshold_match_probability = 0.95 ) >>> df . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions","title":"Predicting results"},{"location":"linkerpred.html#documentation-for-linker-object-methods-related-to-link-prediction","text":"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . Source code in splink/linker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 class Linker : \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a `DuckDBLinker`. \"\"\" def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False @property def _cache_uid ( self ): if self . _settings_dict : return self . _settings_obj . _cache_uid else : return self . _cache_uid_no_settings @_cache_uid . setter def _cache_uid ( self , value ): if self . _settings_dict : self . _settings_obj . _cache_uid = value else : self . _cache_uid_no_settings = value @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `load_settings()` method on your linker \" \"object. i.e. linker.load_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink__df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True # in u-train sample mode we are joining the concatenated table mixing # both data sets - hence if we inner join on True we will end up with # samples which both originate from the same dataset if self . _train_u_using_random_sample_mode : return False if self . _analyse_blocking_mode : return False if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False @property def _sql_dialect ( self ): if self . _sql_dialect_ is None : raise NotImplementedError ( f \"No SQL dialect set on object of type { type ( self ) } . \" \"Did you make sure to create a dialect-specific Linker?\" ) return self . _sql_dialect_ @property def _infinity_expression ( self ): raise NotImplementedError ( f \"infinity sql expression not available for { type ( self ) } \" ) def _setup_settings_objs ( self , settings_dict ): # Setup the linker class's required settings self . _settings_dict = settings_dict # if settings_dict is passed, set sql_dialect on it if missing, and make sure # incompatible dialect not passed if settings_dict is not None and settings_dict . get ( \"sql_dialect\" , None ) is None : settings_dict [ \"sql_dialect\" ] = self . _sql_dialect if settings_dict is None : self . _cache_uid_no_settings = ascii_uid ( 8 ) else : uid = settings_dict . get ( \"linker_uid\" , ascii_uid ( 8 )) settings_dict [ \"linker_uid\" ] = uid if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _validate_dialect () def _initialise_df_concat ( self , materialise = False ): cache = self . _intermediate_table_cache concat_df = None if \"__splink__df_concat\" in cache : concat_df = cache [ \"__splink__df_concat\" ] elif \"__splink__df_concat_with_tf\" in cache : concat_df = cache [ \"__splink__df_concat_with_tf\" ] concat_df . templated_name = \"__splink__df_concat\" else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) if materialise : concat_df = self . _execute_sql_pipeline () cache [ \"__splink__df_concat\" ] = concat_df return concat_df def _initialise_df_concat_with_tf ( self , materialise = True ): cache = self . _intermediate_table_cache nodes_with_tf = None if \"__splink__df_concat_with_tf\" in cache : nodes_with_tf = cache [ \"__splink__df_concat_with_tf\" ] else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if materialise : nodes_with_tf = self . _execute_sql_pipeline () cache [ \"__splink__df_concat_with_tf\" ] = nodes_with_tf return nodes_with_tf def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : list [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name try : dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , ) except Exception as e : raise e finally : self . _pipeline . reset () return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename , materialise_as_hash = False , use_cache = False , ) self . _pipeline . reset () return dataframe def _execute_sql_against_backend ( self , sql , templated_name , physical_name ): raise NotImplementedError ( f \"_execute_sql_against_backend not implemented for { type ( self ) } \" ) def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) def _sql_to_splink_dataframe_checking_cache ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute sql, or if identical sql has been run before, return cached results. This function - is used by _execute_sql_pipeline to to execute SQL - or can be used directly if you have a single SQL statement that's not in a pipeline Return a SplinkDataFrame representing the results of the SQL \"\"\" to_hash = ( sql + self . _cache_uid ) . encode ( \"utf-8\" ) hash = hashlib . sha256 ( to_hash ) . hexdigest ()[: 9 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , table_name_hash ) else : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , output_tablename_templated , ) self . _names_of_tables_created_by_splink . add ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj_ ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain ' \"only a single input table\" , ) def _validate_dialect ( self ): settings_dialect = self . _settings_obj . _sql_dialect if settings_dialect != self . _sql_dialect : raise ValueError ( f \"Incompatible SQL dialect! `settings` dictionary uses \" f \"dialect { settings_dialect } , but expecting \" f \"' { self . _sql_dialect } ' for Linker of type { type ( self ) } \" ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training . blocking_rule } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): to_remove = set () for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : continue if name == \"__splink__df_concat_with_tf\" : if not retain_df_concat_with_tf : self . _delete_table_from_database ( name ) to_remove . add ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if not retain_term_frequency : self . _delete_table_from_database ( name ) to_remove . add ( name ) else : self . _delete_table_from_database ( name ) to_remove . add ( name ) self . _names_of_tables_created_by_splink = ( self . _names_of_tables_created_by_splink - to_remove ) def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def _raise_error_if_necessary_accuracy_columns_not_computed ( self ): rmc = self . _settings_obj . _retain_matching_columns if not ( rmc ): raise ValueError ( \"retain_matching_columns must be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Its current value is { rmc } . \" \"Please re-run your linkage with it set to True.\" ) def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ BlockingRule ( f \" { uid_l } = { uid_r } \" ) ] nodes_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : output_table_name = sql [ \"output_table_name\" ] output_table_name = output_table_name . replace ( \"predict\" , \"self_link\" ) self . _enqueue_sql ( sql [ \"sql\" ], output_table_name ) predictions = self . _execute_sql_pipeline ( input_dataframes = [ nodes_with_tf ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def _get_labels_tablename_from_input ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame ): if isinstance ( labels_splinkdataframe_or_table_name , SplinkDataFrame ): labels_tablename = labels_splinkdataframe_or_table_name . physical_name elif isinstance ( labels_splinkdataframe_or_table_name , str ): labels_tablename = labels_splinkdataframe_or_table_name else : raise ValueError ( \"The 'labels_splinkdataframe_or_table_name' argument\" \" must be of type SplinkDataframe or a string representing a tablename\" \" in the input database\" ) return labels_tablename def estimate_m_from_pairwise_labels ( self , labels_splinkdataframe_or_table_name ): \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. Note that at the moment, this method does not respect values in a `clerical_match_score` column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Args: labels_splinkdataframe_or_table_name (str): Name of table containing labels in the database or SplinkDataframe Examples: >>> pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\") >>> linker.register_table(pairwise_labels, \"labels\", overwrite=True) >>> linker.estimate_m_from_pairwise_labels(\"labels\") \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) estimate_m_from_pairwise_labels ( self , labels_tablename ) def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_column ( self , label_colname , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column. A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs Args: label_colname (str): Name of labels column in input data include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" return prediction_errors_from_label_column ( self , label_colname , include_false_positives , include_false_negatives , threshold , ) def match_weights_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weights_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def completeness_chart ( self , input_dataset : str = None , cols : list [ str ] = None ): \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data. Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, completeness will be computed for this table alone. Defaults to None. cols (List[str], optional): List of column names to calculate completeness. Default to None. Examples: >>> linker.completeness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.completeness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = completeness_data ( self , input_dataset , cols ) return completeness_chart ( records , input_dataset ) def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) def invalidate_cache ( self ): \"\"\"Invalidate the Splink cache. Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed. \"\"\" # Before Splink executes a SQL command, it checks the cache to see # whether a table already exists with the name of the output table # This function has the effect of changing the names of the output tables # to include a different unique id # As a result, any previously cached tables will not be found self . _cache_uid = ascii_uid ( 8 ) # As a result, any previously cached tables will not be found self . _intermediate_table_cache . invalidate_cache () # Also drop any existing splink tables from the database # Note, this is not actually necessary, it's just good housekeeping self . _delete_tables_created_by_splink_from_db () def register_table_input_nodes_concat_with_tf ( self , input_data , overwrite = False ): \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew. Args: input_data: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. overwrite (bool): Overwrite the table in the underlying database if it exists \"\"\" table_name_physical = \"__splink__df_concat_with_tf_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_concat_with_tf\" ] = splink_dataframe return splink_dataframe def register_table_predict ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_predict_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_predict\" ] = splink_dataframe return splink_dataframe def register_term_frequency_lookup ( self , input_data , col_name , overwrite = False ): input_col = InputColumn ( col_name , settings_obj = self . _settings_obj ) table_name_templated = colname_to_tf_tablename ( input_col ) table_name_physical = f \" { table_name_templated } _ { self . _cache_uid } \" splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ table_name_templated ] = splink_dataframe return splink_dataframe def register_labels_table ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_labels_\" + ascii_uid ( 8 ) splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) return splink_dataframe","title":"Documentation for Linker object methods related to link prediction"},{"location":"linkerpred.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","text":"Clusters the pairwise match predictions that result from linker.predict() into groups of connected record using the connected components graph clustering algorithm Records with an estimated match_probability above threshold_match_probability are considered to be a match (i.e. they represent the same entity). Parameters: Name Type Description Default df_predict SplinkDataFrame The results of linker.predict() required threshold_match_probability float Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. required pairwise_formatting bool Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. False filter_pairwise_format_for_clusters bool If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. True Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. Source code in splink/linker.py 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc","title":"cluster_pairwise_predictions_at_threshold()"},{"location":"linkerpred.html#splink.linker.Linker.compare_two_records","text":"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Parameters: Name Type Description Default record_1 dict dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object required record_2 dict dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object required Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compare_two_records ( record_left , record_right ) Returns: Name Type Description SplinkDataFrame Pairwise comparison with scored prediction Source code in splink/linker.py 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions","title":"compare_two_records()"},{"location":"linkerpred.html#splink.linker.Linker.compute_tf_table","text":"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . compute_tf_table ( \"surname\" ) >>> linker . compare_two_records ( record_left , record_right ) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker ( df ) >>> df_first_name_tf = linker . compute_tf_table ( \"first_name\" ) >>> df_first_name_tf . write . parquet ( \"folder/first_name_tf\" ) >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark . read . parquet ( \"folder/first_name_tf\" ) >>> df_first_name_tf . createOrReplaceTempView ( \"__splink__df_tf_first_name\" ) Parameters: Name Type Description Default column_name str The column name in the input table required Returns: Name Type Description SplinkDataFrame SplinkDataFrame The resultant table as a splink data frame Source code in splink/linker.py 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df","title":"compute_tf_table()"},{"location":"linkerpred.html#splink.linker.Linker.deterministic_link","text":"Uses the blocking rules specified by blocking_rules_to_generate_predictions in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker ( df ) >>> >>> settings = { >>> \"link_type\" : \"dedupe_only\" , >>> \"blocking_rules_to_generate_predictions\" : [ >>> \"l.first_name = r.first_name\" , >>> \"l.surname = r.surname\" , >>> ], >>> \"comparisons\" : [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker ( df , settings ) >>> df = linker . deterministic_link () Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ])","title":"deterministic_link()"},{"location":"linkerpred.html#splink.linker.Linker.find_matches_to_new_records","text":"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Parameters: Name Type Description Default records_or_tablename List [ dict ] Input search record(s) as list of dict, or a table registered to the database. required blocking_rules list Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. [] match_weight_threshold int Return matches with a match weight above this threshold. Defaults to -4. -4 Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker . compute_tf_table ( \"first_name\" ) >>> record = { 'unique_id' : 1 , >>> 'first_name' : \"John\" , >>> 'surname' : \"Smith\" , >>> 'dob' : \"1971-05-24\" , >>> 'city' : \"London\" , >>> 'email' : \"john@smith.net\" >>> } >>> df = linker . find_matches_to_new_records ([ record ], blocking_rules = []) Returns: Name Type Description SplinkDataFrame SplinkDataFrame The pairwise comparisons. Source code in splink/linker.py 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions","title":"find_matches_to_new_records()"},{"location":"linkerpred.html#splink.linker.Linker.load_settings","text":"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ([ \"first_name\" , \"surname\" ]) >>> linker . load_settings ( settings_dict ) >>> linker . load_settings ( \"my_settings.json\" ) Parameters: Name Type Description Default settings_dict dict | str | Path A Splink settings dictionary or the path to your settings json file. required Source code in splink/linker.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect ()","title":"load_settings()"},{"location":"linkerpred.html#splink.linker.Linker.load_settings_from_json","text":"This method is now deprecated. Please use load_settings when loading existing settings or a pre-trained model. Load settings from a .json file. This .json file would usually be the output of linker.save_settings_to_json() Examples: >>> linker . load_settings_from_json ( \"my_settings.json\" ) Parameters: Name Type Description Default in_path str Path to settings json file required Source code in splink/linker.py 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , )","title":"load_settings_from_json()"},{"location":"linkerpred.html#splink.linker.Linker.predict","text":"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the blocking_rules_to_generate_predictions of the settings dictionary to generate the pairwise comparisons. Parameters: Name Type Description Default threshold_match_probability float If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. None threshold_match_weight float If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. None materialise_after_computing_term_frequencies bool If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True True Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> df = linker . predict ( threshold_match_probability = 0.95 ) >>> df . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions","title":"predict()"},{"location":"linkerqa.html","tags":["API","QA","Clusters","Labels"],"text":"Documentation for Linker object methods related to QA \u00b6 The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . Source code in splink/linker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 class Linker : \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a `DuckDBLinker`. \"\"\" def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False @property def _cache_uid ( self ): if self . _settings_dict : return self . _settings_obj . _cache_uid else : return self . _cache_uid_no_settings @_cache_uid . setter def _cache_uid ( self , value ): if self . _settings_dict : self . _settings_obj . _cache_uid = value else : self . _cache_uid_no_settings = value @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `load_settings()` method on your linker \" \"object. i.e. linker.load_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink__df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True # in u-train sample mode we are joining the concatenated table mixing # both data sets - hence if we inner join on True we will end up with # samples which both originate from the same dataset if self . _train_u_using_random_sample_mode : return False if self . _analyse_blocking_mode : return False if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False @property def _sql_dialect ( self ): if self . _sql_dialect_ is None : raise NotImplementedError ( f \"No SQL dialect set on object of type { type ( self ) } . \" \"Did you make sure to create a dialect-specific Linker?\" ) return self . _sql_dialect_ @property def _infinity_expression ( self ): raise NotImplementedError ( f \"infinity sql expression not available for { type ( self ) } \" ) def _setup_settings_objs ( self , settings_dict ): # Setup the linker class's required settings self . _settings_dict = settings_dict # if settings_dict is passed, set sql_dialect on it if missing, and make sure # incompatible dialect not passed if settings_dict is not None and settings_dict . get ( \"sql_dialect\" , None ) is None : settings_dict [ \"sql_dialect\" ] = self . _sql_dialect if settings_dict is None : self . _cache_uid_no_settings = ascii_uid ( 8 ) else : uid = settings_dict . get ( \"linker_uid\" , ascii_uid ( 8 )) settings_dict [ \"linker_uid\" ] = uid if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _validate_dialect () def _initialise_df_concat ( self , materialise = False ): cache = self . _intermediate_table_cache concat_df = None if \"__splink__df_concat\" in cache : concat_df = cache [ \"__splink__df_concat\" ] elif \"__splink__df_concat_with_tf\" in cache : concat_df = cache [ \"__splink__df_concat_with_tf\" ] concat_df . templated_name = \"__splink__df_concat\" else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) if materialise : concat_df = self . _execute_sql_pipeline () cache [ \"__splink__df_concat\" ] = concat_df return concat_df def _initialise_df_concat_with_tf ( self , materialise = True ): cache = self . _intermediate_table_cache nodes_with_tf = None if \"__splink__df_concat_with_tf\" in cache : nodes_with_tf = cache [ \"__splink__df_concat_with_tf\" ] else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if materialise : nodes_with_tf = self . _execute_sql_pipeline () cache [ \"__splink__df_concat_with_tf\" ] = nodes_with_tf return nodes_with_tf def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : list [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name try : dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , ) except Exception as e : raise e finally : self . _pipeline . reset () return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename , materialise_as_hash = False , use_cache = False , ) self . _pipeline . reset () return dataframe def _execute_sql_against_backend ( self , sql , templated_name , physical_name ): raise NotImplementedError ( f \"_execute_sql_against_backend not implemented for { type ( self ) } \" ) def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) def _sql_to_splink_dataframe_checking_cache ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute sql, or if identical sql has been run before, return cached results. This function - is used by _execute_sql_pipeline to to execute SQL - or can be used directly if you have a single SQL statement that's not in a pipeline Return a SplinkDataFrame representing the results of the SQL \"\"\" to_hash = ( sql + self . _cache_uid ) . encode ( \"utf-8\" ) hash = hashlib . sha256 ( to_hash ) . hexdigest ()[: 9 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , table_name_hash ) else : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , output_tablename_templated , ) self . _names_of_tables_created_by_splink . add ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj_ ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain ' \"only a single input table\" , ) def _validate_dialect ( self ): settings_dialect = self . _settings_obj . _sql_dialect if settings_dialect != self . _sql_dialect : raise ValueError ( f \"Incompatible SQL dialect! `settings` dictionary uses \" f \"dialect { settings_dialect } , but expecting \" f \"' { self . _sql_dialect } ' for Linker of type { type ( self ) } \" ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training . blocking_rule } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): to_remove = set () for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : continue if name == \"__splink__df_concat_with_tf\" : if not retain_df_concat_with_tf : self . _delete_table_from_database ( name ) to_remove . add ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if not retain_term_frequency : self . _delete_table_from_database ( name ) to_remove . add ( name ) else : self . _delete_table_from_database ( name ) to_remove . add ( name ) self . _names_of_tables_created_by_splink = ( self . _names_of_tables_created_by_splink - to_remove ) def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def _raise_error_if_necessary_accuracy_columns_not_computed ( self ): rmc = self . _settings_obj . _retain_matching_columns if not ( rmc ): raise ValueError ( \"retain_matching_columns must be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Its current value is { rmc } . \" \"Please re-run your linkage with it set to True.\" ) def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ BlockingRule ( f \" { uid_l } = { uid_r } \" ) ] nodes_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : output_table_name = sql [ \"output_table_name\" ] output_table_name = output_table_name . replace ( \"predict\" , \"self_link\" ) self . _enqueue_sql ( sql [ \"sql\" ], output_table_name ) predictions = self . _execute_sql_pipeline ( input_dataframes = [ nodes_with_tf ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def _get_labels_tablename_from_input ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame ): if isinstance ( labels_splinkdataframe_or_table_name , SplinkDataFrame ): labels_tablename = labels_splinkdataframe_or_table_name . physical_name elif isinstance ( labels_splinkdataframe_or_table_name , str ): labels_tablename = labels_splinkdataframe_or_table_name else : raise ValueError ( \"The 'labels_splinkdataframe_or_table_name' argument\" \" must be of type SplinkDataframe or a string representing a tablename\" \" in the input database\" ) return labels_tablename def estimate_m_from_pairwise_labels ( self , labels_splinkdataframe_or_table_name ): \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. Note that at the moment, this method does not respect values in a `clerical_match_score` column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Args: labels_splinkdataframe_or_table_name (str): Name of table containing labels in the database or SplinkDataframe Examples: >>> pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\") >>> linker.register_table(pairwise_labels, \"labels\", overwrite=True) >>> linker.estimate_m_from_pairwise_labels(\"labels\") \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) estimate_m_from_pairwise_labels ( self , labels_tablename ) def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_column ( self , label_colname , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column. A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs Args: label_colname (str): Name of labels column in input data include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" return prediction_errors_from_label_column ( self , label_colname , include_false_positives , include_false_negatives , threshold , ) def match_weights_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weights_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def completeness_chart ( self , input_dataset : str = None , cols : list [ str ] = None ): \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data. Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, completeness will be computed for this table alone. Defaults to None. cols (List[str], optional): List of column names to calculate completeness. Default to None. Examples: >>> linker.completeness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.completeness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = completeness_data ( self , input_dataset , cols ) return completeness_chart ( records , input_dataset ) def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) def invalidate_cache ( self ): \"\"\"Invalidate the Splink cache. Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed. \"\"\" # Before Splink executes a SQL command, it checks the cache to see # whether a table already exists with the name of the output table # This function has the effect of changing the names of the output tables # to include a different unique id # As a result, any previously cached tables will not be found self . _cache_uid = ascii_uid ( 8 ) # As a result, any previously cached tables will not be found self . _intermediate_table_cache . invalidate_cache () # Also drop any existing splink tables from the database # Note, this is not actually necessary, it's just good housekeeping self . _delete_tables_created_by_splink_from_db () def register_table_input_nodes_concat_with_tf ( self , input_data , overwrite = False ): \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew. Args: input_data: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. overwrite (bool): Overwrite the table in the underlying database if it exists \"\"\" table_name_physical = \"__splink__df_concat_with_tf_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_concat_with_tf\" ] = splink_dataframe return splink_dataframe def register_table_predict ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_predict_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_predict\" ] = splink_dataframe return splink_dataframe def register_term_frequency_lookup ( self , input_data , col_name , overwrite = False ): input_col = InputColumn ( col_name , settings_obj = self . _settings_obj ) table_name_templated = colname_to_tf_tablename ( input_col ) table_name_physical = f \" { table_name_templated } _ { self . _cache_uid } \" splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ table_name_templated ] = splink_dataframe return splink_dataframe def register_labels_table ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_labels_\" + ascii_uid ( 8 ) splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) return splink_dataframe cluster_studio_dashboard ( df_predict , df_clustered , out_path , sampling_method = 'random' , sample_size = 10 , cluster_ids = None , cluster_names = None , overwrite = False , return_html_as_string = False ) \u00b6 Generate an interactive html visualization of the predicted cluster and save to out_path . Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required df_clustered SplinkDataFrame The outputs of linker.cluster_pairwise_predictions_at_threshold() required out_path str The path (including filename) to save the html file to. required sampling_method str random or by_cluster_size . Defaults to random . 'random' sample_size int Number of clusters to show in the dahboard. Defaults to 10. 10 cluster_ids list The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the sampling_method and sample_size arguments. Defaults to None. None overwrite bool Overwrite the html file if it already exists? Defaults to False. False cluster_names list If provided, the dashboard will display these names in the selection box. Ony works in conjunction with cluster_ids . Defaults to None. None return_html_as_string If True, return the html as a string False Examples: >>> df_p = linker . predict () >>> df_c = linker . cluster_pairwise_predictions_at_threshold ( df_p , 0.5 ) >>> linker . cluster_studio_dashboard ( >>> df_p , df_c , [ 0 , 4 , 7 ], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered comparison_viewer_dashboard ( df_predict , out_path , overwrite = False , num_example_rows = 2 , return_html_as_string = False ) \u00b6 Generate an interactive html visualization of the linker's predictions and save to out_path . For more information see this video Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required out_path str The path (including filename) to save the html file to. required overwrite bool Overwrite the html file if it already exists? Defaults to False. False num_example_rows int Number of example rows per comparison vector. Defaults to 2. 2 return_html_as_string If True, return the html as a string False Examples: >>> df_predictions = linker . predict () >>> linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered m_u_parameters_chart () \u00b6 Display a chart of the m and u parameters of the linkage model Examples: >>> linker . m_u_parameters_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () match_weights_chart () \u00b6 Display a chart of the (partial) match weights of the linkage model Examples: >>> linker . match_weights_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () parameter_estimate_comparisons_chart ( include_m = True , include_u = True ) \u00b6 Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Parameters: Name Type Description Default include_m bool Show different estimates of m values. Defaults to True. True include_u bool Show different estimates of u values. Defaults to True. True Source code in splink/linker.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) precision_recall_chart_from_labels_column ( labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . precision_recall_chart_from_labels_column ( \"ground_truth\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) precision_recall_chart_from_labels_table ( labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) prediction_errors_from_labels_table ( labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 ) \u00b6 Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required include_false_positives bool Defaults to True. True include_false_negatives bool Defaults to True. True threshold float Threshold above which a score is considered to be a match. Defaults to 0.5. 0.5 Returns: Name Type Description SplinkDataFrame Table containing false positives and negatives Source code in splink/linker.py 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) roc_chart_from_labels_column ( labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . roc_chart_from_labels_column ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) roc_chart_from_labels_table ( labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) truth_space_table_from_labels_column ( labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . truth_space_table_from_labels_column ( \"cluster\" ) Returns: Name Type Description SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) truth_space_table_from_labels_table ( labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) unlinkables_chart ( x_col = 'match_weight' , source_dataset = None , as_dict = False ) \u00b6 Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) waterfall_chart ( records , filter_nulls = True ) \u00b6 Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. Examples: >>> df = linker . predict ( threshold_match_weight = 2 ) >>> records = df . as_record_dict ( limit = 10 ) >>> linker . waterfall_chart ( records ) Parameters: Name Type Description Default records List [ dict ] Usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. required filter_nulls bool Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. True Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls )","title":"Visualisation and quality assurance"},{"location":"linkerqa.html#documentation-for-linker-object-methods-related-to-qa","text":"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as linker.predict() , linker.profile_columns() etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker . Source code in splink/linker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 class Linker : \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a `DuckDBLinker`. \"\"\" def __init__ ( self , input_table_or_tables : str | list , settings_dict : dict , set_up_basic_logging : bool = True , input_table_aliases : str | list = None , ): \"\"\"Initialise the linker object, which manages the data linkage process and holds the data linkage model. Examples: >>> # Example 1: DuckDB >>> df = pd.read_csv(\"data_to_dedupe.csv\") >>> linker = DuckDBLinker(df, settings_dict) >>> # Example 2: Spark >>> df_1 = spark.read.parquet(\"table_1/\") >>> df_2 = spark.read.parquet(\"table_2/\") >>> linker = SparkLinker( >>> [df_1, df_2], >>> settings_dict, >>> input_table_aliases=[\"customers\", \"contact_center_callers\"] >>> ) Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe. For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings. settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.load_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) self . _pipeline = SQLPipeline () self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) if not isinstance ( settings_dict , ( dict , type ( None ))): self . _setup_settings_objs ( None ) # feed it a blank settings dictionary self . load_settings ( settings_dict ) else : settings_dict = deepcopy ( settings_dict ) self . _setup_settings_objs ( settings_dict ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : set = set () self . _intermediate_table_cache : dict = CacheDictWithLogging () self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _analyse_blocking_mode = False self . debug_mode = False @property def _cache_uid ( self ): if self . _settings_dict : return self . _settings_obj . _cache_uid else : return self . _cache_uid_no_settings @_cache_uid . setter def _cache_uid ( self , value ): if self . _settings_dict : self . _settings_obj . _cache_uid = value else : self . _cache_uid_no_settings = value @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `load_settings()` method on your linker \" \"object. i.e. linker.load_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink__df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _analyse_blocking_mode : return \"__splink__df_concat\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True # in u-train sample mode we are joining the concatenated table mixing # both data sets - hence if we inner join on True we will end up with # samples which both originate from the same dataset if self . _train_u_using_random_sample_mode : return False if self . _analyse_blocking_mode : return False if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False @property def _sql_dialect ( self ): if self . _sql_dialect_ is None : raise NotImplementedError ( f \"No SQL dialect set on object of type { type ( self ) } . \" \"Did you make sure to create a dialect-specific Linker?\" ) return self . _sql_dialect_ @property def _infinity_expression ( self ): raise NotImplementedError ( f \"infinity sql expression not available for { type ( self ) } \" ) def _setup_settings_objs ( self , settings_dict ): # Setup the linker class's required settings self . _settings_dict = settings_dict # if settings_dict is passed, set sql_dialect on it if missing, and make sure # incompatible dialect not passed if settings_dict is not None and settings_dict . get ( \"sql_dialect\" , None ) is None : settings_dict [ \"sql_dialect\" ] = self . _sql_dialect if settings_dict is None : self . _cache_uid_no_settings = ascii_uid ( 8 ) else : uid = settings_dict . get ( \"linker_uid\" , ascii_uid ( 8 )) settings_dict [ \"linker_uid\" ] = uid if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _validate_dialect () def _initialise_df_concat ( self , materialise = False ): cache = self . _intermediate_table_cache concat_df = None if \"__splink__df_concat\" in cache : concat_df = cache [ \"__splink__df_concat\" ] elif \"__splink__df_concat_with_tf\" in cache : concat_df = cache [ \"__splink__df_concat_with_tf\" ] concat_df . templated_name = \"__splink__df_concat\" else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) if materialise : concat_df = self . _execute_sql_pipeline () cache [ \"__splink__df_concat\" ] = concat_df return concat_df def _initialise_df_concat_with_tf ( self , materialise = True ): cache = self . _intermediate_table_cache nodes_with_tf = None if \"__splink__df_concat_with_tf\" in cache : nodes_with_tf = cache [ \"__splink__df_concat_with_tf\" ] else : if materialise : # Clear the pipeline if we are materialising # There's no reason not to do this, since when # we execute the pipeline, it'll get cleared anyway self . _pipeline . reset () sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if materialise : nodes_with_tf = self . _execute_sql_pipeline () cache [ \"__splink__df_concat_with_tf\" ] = nodes_with_tf return nodes_with_tf def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : list [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name try : dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , ) except Exception as e : raise e finally : self . _pipeline . reset () return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename , materialise_as_hash = False , use_cache = False , ) self . _pipeline . reset () return dataframe def _execute_sql_against_backend ( self , sql , templated_name , physical_name ): raise NotImplementedError ( f \"_execute_sql_against_backend not implemented for { type ( self ) } \" ) def register_table ( self , input , table_name , overwrite = False ): \"\"\" Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying. Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df. Examples: >>> test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]} >>> linker.register_table(test_dict, \"test_dict\") >>> linker.query_sql(\"select * from test_dict\") Args: input: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. table_name (str): The name you wish to assign to the table. overwrite (bool): Overwrite the table in the underlying database if it exists Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" raise NotImplementedError ( f \"register_table not implemented for { type ( self ) } \" ) def query_sql ( self , sql , output_type = \"pandas\" ): \"\"\" Run a SQL query against your backend database and return the resulting output. Examples: >>> linker = DuckDBLinker(df, settings) >>> df_predict = linker.predict() >>> linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\") Args: sql (str): The SQL to be queried. output_type (str): One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in. \"\"\" output_tablename_templated = \"__splink__df_sql_query\" splink_dataframe = self . _sql_to_splink_dataframe_checking_cache ( sql , output_tablename_templated , materialise_as_hash = False , use_cache = False , ) if output_type in ( \"splink_df\" , \"splinkdf\" ): return splink_dataframe elif output_type == \"pandas\" : out = splink_dataframe . as_pandas_dataframe () # If pandas, drop the table to cleanup the db splink_dataframe . drop_table_from_database () return out else : raise ValueError ( f \"output_type ' { output_type } ' is not supported.\" , \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\" , ) def _sql_to_splink_dataframe_checking_cache ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , ) -> SplinkDataFrame : \"\"\"Execute sql, or if identical sql has been run before, return cached results. This function - is used by _execute_sql_pipeline to to execute SQL - or can be used directly if you have a single SQL statement that's not in a pipeline Return a SplinkDataFrame representing the results of the SQL \"\"\" to_hash = ( sql + self . _cache_uid ) . encode ( \"utf-8\" ) hash = hashlib . sha256 ( to_hash ) . hexdigest ()[: 9 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , table_name_hash ) else : splink_dataframe = self . _execute_sql_against_backend ( sql , output_tablename_templated , output_tablename_templated , ) self . _names_of_tables_created_by_splink . add ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj_ ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain ' \"only a single input table\" , ) def _validate_dialect ( self ): settings_dialect = self . _settings_obj . _sql_dialect if settings_dialect != self . _sql_dialect : raise ValueError ( f \"Incompatible SQL dialect! `settings` dictionary uses \" f \"dialect { settings_dialect } , but expecting \" f \"' { self . _sql_dialect } ' for Linker of type { type ( self ) } \" ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training . blocking_rule } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): to_remove = set () for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : continue if name == \"__splink__df_concat_with_tf\" : if not retain_df_concat_with_tf : self . _delete_table_from_database ( name ) to_remove . add ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if not retain_term_frequency : self . _delete_table_from_database ( name ) to_remove . add ( name ) else : self . _delete_table_from_database ( name ) to_remove . add ( name ) self . _names_of_tables_created_by_splink = ( self . _names_of_tables_created_by_splink - to_remove ) def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def _raise_error_if_necessary_accuracy_columns_not_computed ( self ): rmc = self . _settings_obj . _retain_matching_columns if not ( rmc ): raise ValueError ( \"retain_matching_columns must be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Its current value is { rmc } . \" \"Please re-run your linkage with it set to True.\" ) def load_settings ( self , settings_dict : dict | str | Path ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.load_settings(settings_dict) >>> linker.load_settings(\"my_settings.json\") Args: settings_dict (dict | str | Path): A Splink settings dictionary or the path to your settings json file. \"\"\" if not isinstance ( settings_dict , dict ): p = Path ( settings_dict ) if not p . is_file (): # check if it's a valid file/filepath raise ValueError ( \"The filepath you have provided is either not a valid file \" \"or doesn't exist along the path provided.\" ) settings_dict = json . loads ( p . read_text ()) # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () def initialise_settings ( self , settings_dict : dict ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns([\"first_name\", \"surname\"]) >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" # If a uid already exists in your settings object, prioritise this settings_dict [ \"linker_uid\" ] = settings_dict . get ( \"linker_uid\" , self . _cache_uid ) settings_dict [ \"sql_dialect\" ] = settings_dict . get ( \"sql_dialect\" , self . _sql_dialect ) self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () self . _validate_dialect () warnings . warn ( \"`initialise_settings` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def load_settings_from_json ( self , in_path : str | Path ): \"\"\"*This method is now deprecated. Please use `load_settings` when loading existing settings or a pre-trained model.* Load settings from a `.json` file. This `.json` file would usually be the output of `linker.save_settings_to_json()` Examples: >>> linker.load_settings_from_json(\"my_settings.json\") Args: in_path (str): Path to settings json file \"\"\" self . load_settings ( in_path ) warnings . warn ( \"`load_settings_from_json` is deprecated. We advise you use \" \"`linker.load_settings()` when loading in your settings or a previously \" \"trained model.\" , DeprecationWarning , stacklevel = 2 , ) def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" input_col = InputColumn ( column_name , settings_obj = self . _settings_obj ) tf_tablename = colname_to_tf_tablename ( input_col ) cache = self . _intermediate_table_cache concat_tf_tables = [ remove_quotes_from_identifiers ( tf_col . input_name_as_tree ) . sql () for tf_col in self . _settings_obj . _term_frequency_columns ] if tf_tablename in cache : tf_df = cache [ tf_tablename ] elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables : self . _pipeline . reset () # If our df_concat_with_tf table already exists, use backwards inference to # find a given tf table colname = InputColumn ( column_name ) sql = term_frequencies_from_concat_with_tf ( colname ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( colname )) tf_df = self . _execute_sql_pipeline ( [ cache [ \"__splink__df_concat_with_tf\" ]], materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df else : # Clear the pipeline if we are materialising self . _pipeline . reset () df_concat = self . _initialise_df_concat () input_dfs = [] if df_concat : input_dfs . append ( df_concat ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , tf_tablename ) tf_df = self . _execute_sql_pipeline ( input_dfs , materialise_as_hash = True ) self . _intermediate_table_cache [ tf_tablename ] = tf_df return tf_df def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df) >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings) >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" concat_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ([ concat_with_tf ]) def estimate_u_using_random_sampling ( self , max_pairs : int = None , * , target_rows = None ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: max_pairs (int): The maximum number of pairwise record comparisons to sample. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: None: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" # TODO: Remove this compatibility code in a future release once we drop # support for \"target_rows\". Deprecation warning added in 3.7.0 if max_pairs is not None and target_rows is not None : # user supplied both raise TypeError ( \"Just use max_pairs\" ) elif max_pairs is not None : # user is doing it correctly pass elif target_rows is not None : # user is using deprecated argument warnings . warn ( \"target_rows is deprecated; use max_pairs\" , DeprecationWarning , stacklevel = 2 , ) max_pairs = target_rows else : raise TypeError ( \"Missing argument max_pairs\" ) estimate_u_values ( self , max_pairs ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" # Ensure this has been run on the main linker so that it can be used by # training linked when it checks the cache self . _initialise_df_concat_with_tf () estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname , ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : list [ str | Comparison ] = None , comparison_levels_to_reverse_blocking_rule : list [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , populate_probability_two_random_records_match_from_trained_values = False , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. populate_probability_two_random_records_match_from_trained_values (bool, optional): If True, derive this parameter from the blocked value. Defaults to False. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" # Ensure this has been run on the main linker so that it's in the cache # to be used by the training linkers self . _initialise_df_concat_with_tf () if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule \" \"because each comparison to deactivate is effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () if populate_probability_two_random_records_match_from_trained_values : self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , materialise_after_computing_term_frequencies = True , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. materialise_after_computing_term_frequencies (bool): If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object. If False, this will be computed as part of one possibly gigantic CTE pipeline. Defaults to True Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If materialise_after_computing_term_frequencies=False and the user only # calls predict, it runs as a single pipeline with no materialisation # of anything. # _initialise_df_concat_with_tf returns None if the table doesn't exist # and only SQL is queued in this step. nodes_with_tf = self . _initialise_df_concat_with_tf ( materialise = materialise_after_computing_term_frequencies ) input_dataframes = [] if nodes_with_tf : input_dataframes . append ( nodes_with_tf ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) repartition_after_blocking = getattr ( self , \"repartition_after_blocking\" , False ) # repartition after blocking only exists on the SparkLinker if repartition_after_blocking : df_blocked = self . _execute_sql_pipeline ( input_dataframes ) input_dataframes . append ( df_blocked ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( input_dataframes ) self . _predict_warning () return predictions def find_matches_to_new_records ( self , records_or_tablename , blocking_rules = [], match_weight_threshold =- 4 , ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records_or_tablename (List[dict]): Input search record(s) as list of dict, or a table registered to the database. blocking_rules (list, optional): Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to []. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type if not isinstance ( records_or_tablename , str ): uid = ascii_uid ( 8 ) self . register_table ( records_or_tablename , f \"__splink__df_new_records_ { uid } \" , overwrite = True ) new_records_tablename = f \"__splink__df_new_records_ { uid } \" else : new_records_tablename = records_or_tablename cache = self . _intermediate_table_cache input_dfs = [] # If our df_concat_with_tf table already exists, use backwards inference to # find all underlying term frequency tables. if \"__splink__df_concat_with_tf\" in cache : concat_with_tf = cache [ \"__splink__df_concat_with_tf\" ] tf_tables = compute_term_frequencies_from_concat_with_tf ( self ) # This queues up our tf tables, rather materialising them for tf in tf_tables : # if tf is a SplinkDataFrame, then the table already exists if isinstance ( tf , SplinkDataFrame ): input_dfs . append ( tf ) else : self . _enqueue_sql ( tf [ \"sql\" ], tf [ \"output_table_name\" ]) else : # This queues up our cols_with_tf and df_concat_with_tf tables. concat_with_tf = self . _initialise_df_concat_with_tf ( materialise = False ) if concat_with_tf : input_dfs . append ( concat_with_tf ) rules = [] for r in blocking_rules : br_as_obj = BlockingRule ( r ) if not isinstance ( r , BlockingRule ) else r br_as_obj . preceding_rules = rules . copy () rules . append ( br_as_obj ) blocking_rules = rules self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , new_records_tablename ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink__find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( input_dataframes = input_dfs , use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] uid = ascii_uid ( 8 ) df_records_left = self . register_table ( [ record_1 ], f \"__splink__compare_two_records_left_ { uid } \" , overwrite = True ) df_records_left . templated_name = \"__splink__compare_two_records_left\" df_records_right = self . register_table ( [ record_2 ], f \"__splink__compare_two_records_right_ { uid } \" , overwrite = True ) df_records_right . templated_name = \"__splink__compare_two_records_right\" sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( [ df_records_left , df_records_right ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ BlockingRule ( f \" { uid_l } = { uid_r } \" ) ] nodes_with_tf = self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , sql_infinity_expression = self . _infinity_expression , ) for sql in sqls : output_table_name = sql [ \"output_table_name\" ] output_table_name = output_table_name . replace ( \"predict\" , \"self_link\" ) self . _enqueue_sql ( sql [ \"sql\" ], output_table_name ) predictions = self . _execute_sql_pipeline ( input_dataframes = [ nodes_with_tf ], use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float , pairwise_formatting : bool = False , filter_pairwise_format_for_clusters : bool = True , ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. pairwise_formatting (bool): Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold. filter_pairwise_format_for_clusters (bool): If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" # Feeding in df_predict forces materiailisation, if it exists in your database concat_with_tf = self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , concat_with_tf . physical_name , df_predict . physical_name , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table , df_predict , concat_with_tf , pairwise_formatting , filter_pairwise_format_for_clusters , ) return cc def profile_columns ( self , column_expressions : str | list [ str ], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def _get_labels_tablename_from_input ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame ): if isinstance ( labels_splinkdataframe_or_table_name , SplinkDataFrame ): labels_tablename = labels_splinkdataframe_or_table_name . physical_name elif isinstance ( labels_splinkdataframe_or_table_name , str ): labels_tablename = labels_splinkdataframe_or_table_name else : raise ValueError ( \"The 'labels_splinkdataframe_or_table_name' argument\" \" must be of type SplinkDataframe or a string representing a tablename\" \" in the input database\" ) return labels_tablename def estimate_m_from_pairwise_labels ( self , labels_splinkdataframe_or_table_name ): \"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise labels. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1 |1 |df_2 |2 | |df_1 |1 |df_2 |3 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. Note that at the moment, this method does not respect values in a `clerical_match_score` column. If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match. Args: labels_splinkdataframe_or_table_name (str): Name of table containing labels in the database or SplinkDataframe Examples: >>> pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\") >>> linker.register_table(pairwise_labels, \"labels\", overwrite=True) >>> linker.estimate_m_from_pairwise_labels(\"labels\") \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) estimate_m_from_pairwise_labels ( self , labels_tablename ) def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , ) def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest ) def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def prediction_errors_from_labels_column ( self , label_colname , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the splink match probability and the labels column. A label column is a column in the input dataset that contains the 'ground truth' cluster to which the record belongs Args: label_colname (str): Name of labels column in input data include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" return prediction_errors_from_label_column ( self , label_colname , include_false_positives , include_false_negatives , threshold , ) def match_weights_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weights_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.missingness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def completeness_chart ( self , input_dataset : str = None , cols : list [ str ] = None ): \"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of columns in each of the input datasets. By default, completeness is assessed for all column in the input data. Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, completeness will be computed for this table alone. Defaults to None. cols (List[str], optional): List of column names to calculate completeness. Default to None. Examples: >>> linker.completeness_chart() >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.completeness_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500 \"\"\" records = completeness_data ( self , input_dataset , cols ) return completeness_chart ( records , input_dataset ) def count_num_comparisons_from_blocking_rule ( self , blocking_rule : str , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.count_num_comparisons_from_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.count_num_comparisons_from_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def cumulative_comparisons_from_blocking_rules_records ( self , blocking_rules : str or list = None , ): \"\"\"Output the number of comparisons generated by each successive blocking rule. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_comparisons_from_blocking_rules_records() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_comparisons_from_blocking_rules_records( >>> blocking_rules >>> ) Returns: List: A list of blocking rules and the corresponding number of comparisons it is forecast to generate. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = False ) return records def cumulative_num_comparisons_from_blocking_rules_chart ( self , blocking_rules : str or list = None , ): \"\"\"Display a chart with the cumulative number of comparisons generated by a selection of blocking rules. This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total. Args: blocking_rules (str or list): The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used. Examples: >>> linker_settings = DuckDBLinker(df, settings) >>> # Compute the cumulative number of comparisons generated by the rules >>> # in your settings object. >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart() >>> >>> # Generate total comparisons with custom blocking rules. >>> blocking_rules = [ >>> \"l.surname = r.surname\", >>> \"l.first_name = r.first_name >>> and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> ] >>> >>> linker_settings.cumulative_num_comparisons_from_blocking_rules_chart( >>> blocking_rules >>> ) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" if blocking_rules : blocking_rules = ensure_is_list ( blocking_rules ) records = cumulative_comparisons_generated_by_blocking_rules ( self , blocking_rules , output_chart = True ) return cumulative_blocking_rule_comparisons_generated ( records ) def count_num_comparisons_from_blocking_rules_for_prediction ( self , df_predict ): \"\"\"Counts the maginal number of edges created from each of the blocking rules in `blocking_rules_to_generate_predictions` This is different to `count_num_comparisons_from_blocking_rule` because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in `blocking_rules_to_generate_predictions` Args: df_predict (SplinkDataFrame): SplinkDataFrame with match weights and probabilities of rows matching Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings(\"saved_settings.json\") >>> df_predict = linker.predict(threshold_match_probability=0.95) >>> count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict) >>> count_pairwise.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules. \"\"\" # noqa: E501 sql = count_num_comparisons_from_blocking_rules_for_prediction_sql ( self , df_predict ) match_key_analysis = self . _sql_to_splink_dataframe_checking_cache ( sql , \"__splink__match_key_analysis\" ) return match_key_analysis def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered def save_settings_to_json ( self , out_path : str | None = None , overwrite : bool = False ) -> dict : \"\"\"Save the configuration and parameters of the linkage model to a `.json` file. The model can later be loaded back in using `linker.load_settings()`. The settings dict is also returned in case you want to save it a different way. Examples: >>> linker.save_settings_to_json(\"my_settings.json\", overwrite=True) Args: out_path (str, optional): File path for json file. If None, don't save to file. Defaults to None. overwrite (bool, optional): Overwrite if already exists? Defaults to False. Returns: dict: The settings as a dictionary. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( model_dict , f , indent = 4 ) return model_dict def estimate_probability_two_random_records_match ( self , deterministic_matching_rules , recall ): \"\"\"Estimate the model parameter `probability_two_random_records_match` using a direct estimation approach. See [here](https://github.com/moj-analytical-services/splink/issues/462) for discussion of methodology Args: deterministic_matching_rules (list): A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives recall (float): A guess at the recall the deterministic matching rules will attain. i.e. what proportion of true matches will be recovered by these deterministic rules \"\"\" if ( recall > 1 ) or ( recall <= 0 ): raise ValueError ( f \"Estimated recall must be greater than 0 \" f \"and no more than 1. Supplied value { recall } .\" ) # If user, by error, provides a single rule as a string if isinstance ( deterministic_matching_rules , str ): deterministic_matching_rules = [ deterministic_matching_rules ] records = cumulative_comparisons_generated_by_blocking_rules ( self , deterministic_matching_rules , ) summary_record = records [ - 1 ] num_observed_matches = summary_record [ \"cumulative_rows\" ] num_total_comparisons = summary_record [ \"cartesian\" ] if num_observed_matches > num_total_comparisons * recall : raise ValueError ( f \"Deterministic matching rules led to more \" f \"observed matches than is consistent with supplied recall. \" f \"With these rules, recall must be at least \" f \" { num_observed_matches / num_total_comparisons : ,.2f } .\" ) num_expected_matches = num_observed_matches / recall prob = num_expected_matches / num_total_comparisons # warn about boundary values, as these will usually be in error if num_observed_matches == 0 : logger . warning ( f \"WARNING: Deterministic matching rules led to no observed matches! \" f \"This means that no possible record pairs are matches, \" f \"and no records are linked to one another. \\n \" f \"If this is truly the case then you do not need \" f \"to run the linkage model. \\n \" f \"However this is usually in error; \" f \"expected rules to have recall of { 100 * recall : ,.0f } %. \" f \"Consider revising rules as they may have an error.\" ) if prob == 1 : logger . warning ( \"WARNING: Probability two random records match is estimated to be 1. \\n \" \"This means that all possible record pairs are matches, \" \"and all records are linked to one another. \\n \" \"If this is truly the case then you do not need \" \"to run the linkage model. \\n \" \"However, it is more likely that this estimate is faulty. \" \"Perhaps your deterministic matching rules include \" \"too many false positives?\" ) self . _settings_obj . _probability_two_random_records_match = prob reciprocal_prob = \"Infinity\" if prob == 0 else f \" { 1 / prob : ,.2f } \" logger . info ( f \"Probability two random records match is estimated to be { prob : .3g } . \\n \" f \"This means that amongst all possible pairwise record comparisons, one in \" f \" { reciprocal_prob } are expected to match. \" f \"With { num_total_comparisons : ,.0f } total\" \" possible comparisons, we expect a total of around \" f \" { num_expected_matches : ,.2f } matching pairs\" ) def invalidate_cache ( self ): \"\"\"Invalidate the Splink cache. Any previously-computed tables will be recomputed. This is useful, for example, if the input data tables have changed. \"\"\" # Before Splink executes a SQL command, it checks the cache to see # whether a table already exists with the name of the output table # This function has the effect of changing the names of the output tables # to include a different unique id # As a result, any previously cached tables will not be found self . _cache_uid = ascii_uid ( 8 ) # As a result, any previously cached tables will not be found self . _intermediate_table_cache . invalidate_cache () # Also drop any existing splink tables from the database # Note, this is not actually necessary, it's just good housekeeping self . _delete_tables_created_by_splink_from_db () def register_table_input_nodes_concat_with_tf ( self , input_data , overwrite = False ): \"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that you want to re-use e.g. that you created in a previous run This method allowed you to register this table in the Splink cache so it will be used rather than Splink computing this table anew. Args: input_data: The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe. overwrite (bool): Overwrite the table in the underlying database if it exists \"\"\" table_name_physical = \"__splink__df_concat_with_tf_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_concat_with_tf\" ] = splink_dataframe return splink_dataframe def register_table_predict ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_predict_\" + self . _cache_uid splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ \"__splink__df_predict\" ] = splink_dataframe return splink_dataframe def register_term_frequency_lookup ( self , input_data , col_name , overwrite = False ): input_col = InputColumn ( col_name , settings_obj = self . _settings_obj ) table_name_templated = colname_to_tf_tablename ( input_col ) table_name_physical = f \" { table_name_templated } _ { self . _cache_uid } \" splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) self . _intermediate_table_cache [ table_name_templated ] = splink_dataframe return splink_dataframe def register_labels_table ( self , input_data , overwrite = False ): table_name_physical = \"__splink__df_labels_\" + ascii_uid ( 8 ) splink_dataframe = self . register_table ( input_data , table_name_physical , overwrite = overwrite ) return splink_dataframe","title":"Documentation for Linker object methods related to QA"},{"location":"linkerqa.html#splink.linker.Linker.cluster_studio_dashboard","text":"Generate an interactive html visualization of the predicted cluster and save to out_path . Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required df_clustered SplinkDataFrame The outputs of linker.cluster_pairwise_predictions_at_threshold() required out_path str The path (including filename) to save the html file to. required sampling_method str random or by_cluster_size . Defaults to random . 'random' sample_size int Number of clusters to show in the dahboard. Defaults to 10. 10 cluster_ids list The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the sampling_method and sample_size arguments. Defaults to None. None overwrite bool Overwrite the html file if it already exists? Defaults to False. False cluster_names list If provided, the dashboard will display these names in the selection box. Ony works in conjunction with cluster_ids . Defaults to None. None return_html_as_string If True, return the html as a string False Examples: >>> df_p = linker . predict () >>> df_c = linker . cluster_pairwise_predictions_at_threshold ( df_p , 0.5 ) >>> linker . cluster_studio_dashboard ( >>> df_p , df_c , [ 0 , 4 , 7 ], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. return_html_as_string: If True, return the html as a string Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () rendered = render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) if return_html_as_string : return rendered","title":"cluster_studio_dashboard()"},{"location":"linkerqa.html#splink.linker.Linker.comparison_viewer_dashboard","text":"Generate an interactive html visualization of the linker's predictions and save to out_path . For more information see this video Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required out_path str The path (including filename) to save the html file to. required overwrite bool Overwrite the html file if it already exists? Defaults to False. False num_example_rows int Number of example rows per comparison vector. Defaults to 2. 2 return_html_as_string If True, return the html as a string False Examples: >>> df_predictions = linker . predict () >>> linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , return_html_as_string = False , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. return_html_as_string: If True, return the html as a string Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) rendered = render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) if return_html_as_string : return rendered","title":"comparison_viewer_dashboard()"},{"location":"linkerqa.html#splink.linker.Linker.m_u_parameters_chart","text":"Display a chart of the m and u parameters of the linkage model Examples: >>> linker . m_u_parameters_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . m_u_parameters_chart ()","title":"m_u_parameters_chart()"},{"location":"linkerqa.html#splink.linker.Linker.match_weights_chart","text":"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker . match_weights_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" return self . _settings_obj . match_weights_chart ()","title":"match_weights_chart()"},{"location":"linkerqa.html#splink.linker.Linker.parameter_estimate_comparisons_chart","text":"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Parameters: Name Type Description Default include_m bool Show different estimates of m values. Defaults to True. True include_u bool Show different estimates of u values. Defaults to True. True Source code in splink/linker.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records )","title":"parameter_estimate_comparisons_chart()"},{"location":"linkerqa.html#splink.linker.Linker.precision_recall_chart_from_labels_column","text":"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . precision_recall_chart_from_labels_column ( \"ground_truth\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 def precision_recall_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.precision_recall_chart_from_labels_column(\"ground_truth\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs )","title":"precision_recall_chart_from_labels_column()"},{"location":"linkerqa.html#splink.linker.Linker.precision_recall_chart_from_labels_table","text":"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . precision_recall_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 def precision_recall_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs )","title":"precision_recall_chart_from_labels_table()"},{"location":"linkerqa.html#splink.linker.Linker.prediction_errors_from_labels_table","text":"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required include_false_positives bool Defaults to True. True include_false_negatives bool Defaults to True. True threshold float Threshold above which a score is considered to be a match. Defaults to 0.5. 0.5 Returns: Name Type Description SplinkDataFrame Table containing false positives and negatives Source code in splink/linker.py 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 def prediction_errors_from_labels_table ( self , labels_splinkdataframe_or_table_name , include_false_positives = True , include_false_negatives = True , threshold = 0.5 , ): \"\"\"Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database include_false_positives (bool, optional): Defaults to True. include_false_negatives (bool, optional): Defaults to True. threshold (float, optional): Threshold above which a score is considered to be a match. Defaults to 0.5. Returns: SplinkDataFrame: Table containing false positives and negatives \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) return prediction_errors_from_labels_table ( self , labels_tablename , include_false_positives , include_false_negatives , threshold , )","title":"prediction_errors_from_labels_table()"},{"location":"linkerqa.html#splink.linker.Linker.roc_chart_from_labels_column","text":"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called labels_column_name Parameters: Name Type Description Default labels_column_name str Column name containing labels in the input table required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . roc_chart_from_labels_column ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 def roc_chart_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called `labels_column_name` Args: labels_column_name (str): Column name containing labels in the input table threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.roc_chart_from_labels_column(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" df_truth_space = truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs )","title":"roc_chart_from_labels_column()"},{"location":"linkerqa.html#splink.linker.Linker.roc_chart_from_labels_table","text":"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_chart_from_labels_table ( \"labels\" ) Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 def roc_chart_from_labels_table ( self , labels_splinkdataframe_or_table_name : str | SplinkDataFrame , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels_table(\"labels\") Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () df_truth_space = truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs )","title":"roc_chart_from_labels_table()"},{"location":"linkerqa.html#splink.linker.Linker.truth_space_table_from_labels_column","text":"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> linker . truth_space_table_from_labels_column ( \"cluster\" ) Returns: Name Type Description SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 def truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> linker.truth_space_table_from_labels_column(\"cluster\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" return truth_space_table_from_labels_column ( self , labels_column_name , threshold_actual , match_weight_round_to_nearest )","title":"truth_space_table_from_labels_column()"},{"location":"linkerqa.html#splink.linker.Linker.truth_space_table_from_labels_table","text":"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_splinkdataframe_or_table_name str | SplinkDataFrame Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . register_table ( labels , \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . truth_space_table_from_labels_table ( \"labels\" ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame Table of truth statistics Source code in splink/linker.py 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 def truth_space_table_from_labels_table ( self , labels_splinkdataframe_or_table_name , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker.register_table(labels, \"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.truth_space_table_from_labels_table(\"labels\") Returns: SplinkDataFrame: Table of truth statistics \"\"\" labels_tablename = self . _get_labels_tablename_from_input ( labels_splinkdataframe_or_table_name ) self . _raise_error_if_necessary_accuracy_columns_not_computed () return truth_space_table_from_labels_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , )","title":"truth_space_table_from_labels_table()"},{"location":"linkerqa.html#splink.linker.Linker.unlinkables_chart","text":"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self ) return unlinkables_chart ( records , x_col , source_dataset , as_dict )","title":"unlinkables_chart()"},{"location":"linkerqa.html#splink.linker.Linker.waterfall_chart","text":"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. Examples: >>> df = linker . predict ( threshold_match_weight = 2 ) >>> records = df . as_record_dict ( limit = 10 ) >>> linker . waterfall_chart ( records ) Parameters: Name Type Description Default records List [ dict ] Usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. required filter_nulls bool Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. True Returns: Name Type Description VegaLite A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the spec attribute. Source code in splink/linker.py 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 def waterfall_chart ( self , records : list [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. Returns: VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the `spec` attribute. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls )","title":"waterfall_chart()"},{"location":"settings_dict_guide.html","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"],"text":"Guide to Splink settings \u00b6 This document enumerates all the settings and configuration options available when developing your data linkage model. You can find an interative settings editor here . Settings keys in the base setting dictionary \u00b6 link_type \u00b6 The type of data linking task. Required. When dedupe_only , splink find duplicates. User expected to provide a single input dataset. When link_and_dedupe , splink finds links within and between input datasets. User is expected to provide two or more input datasets. When link_only , splink finds links between datasets, but does not attempt to deduplicate the datasets (it does not try and find links within each input dataset.) User is expected to provide two or more input datasets. Examples : ['dedupe_only', 'link_only', 'link_and_dedupe'] probability_two_random_records_match \u00b6 The probability that two records chosen at random (with no blocking) are a match. For example, if there are a million input records and each has on average one match, then this value should be 1/1,000,000. If you estimate parameters using expectation maximisation (EM), this provides an initial value (prior) from which the EM algorithm will start iterating. EM will then estimate the true value of this parameter. Default value : 0.0001 Examples : [1e-05, 0.006] em_convergence \u00b6 Convergence tolerance for the Expectation Maximisation algorithm The algorithm will stop converging when the maximum of the change in model parameters between iterations is below this value Default value : 0.0001 Examples : [0.0001, 1e-05, 1e-06] max_iterations \u00b6 The maximum number of Expectation Maximisation iterations to run (even if convergence has not been reached) Default value : 25 Examples : [20, 150] unique_id_column_name \u00b6 Splink requires that the input dataset has a column that uniquely identifies each reecord. unique_id_column_name is the name of the column in the input dataset representing this unique id For linking tasks, ids must be unique within each dataset being linked, and do not need to be globally unique across input datasets Default value : unique_id Examples : ['unique_id', 'id', 'pk'] source_dataset_column_name \u00b6 The name of the column in the input dataset representing the source dataset Where we are linking datasets, we can't guarantee that the unique id column is globally unique across datasets, so we combine it with a source_dataset column. Usually, this is created by Splink for the user Default value : source_dataset Examples : ['source_dataset', 'dataset_name'] retain_matching_columns \u00b6 If set to true, each column used by the comparisons sql expressions will be retained in output datasets This is helpful so that the user can inspect matches, but once the comparison vector (gamma) columns are computed, this information is not actually needed by the algorithm. The algorithm will run faster and use less resources if this is set to false. Default value : True Examples : [False, True] retain_intermediate_calculation_columns \u00b6 Retain intermediate calculation columns, such as the bayes factors associated with each column in comparisons The algorithm will run faster and use less resources if this is set to false. Default value : False Examples : [False, True] comparisons \u00b6 A list specifying how records should be compared for probabalistic matching. Each element is a dictionary blocking_rules_to_generate_predictions \u00b6 A list of one or more blocking rules to apply. A cartesian join is applied if blocking_rules_to_generate_predictions is empty or not supplied. Each rule is a SQL expression representing the blocking rule, which will be used to create a join. The left table is aliased with l and the right table is aliased with r . For example, if you want to block on a first_name column, the blocking rule would be l.first_name = r.first_name . To block on first name and the first letter of surname, it would be l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1) . Note that splink deduplicates the comparisons generated by the blocking rules. If empty or not supplied, all comparisons between the input dataset(s) will be generated and blocking will not be used. For large input datasets, this will generally be computationally intractable because it will generate comparisons equal to the number of rows squared. Default value : [] Examples : [['l.first_name = r.first_name AND l.surname = r.surname', 'l.dob = r.dob']] additional_columns_to_retain \u00b6 A list of columns not being used in the probabalistic matching comparisons that you want to include in your results. By default, splink drops columns which are not used by any comparisons. This gives you the option to retain columns which are not used by the model. A common example is if the user has labelled data (training data) and wishes to retain the labels in the outputs Default value : [] Examples : [['cluster', 'col_2'], ['other_information']] bayes_factor_column_prefix \u00b6 The prefix to use for the columns that will be created to store the bayes factors Default value : bf_ Examples : ['bf_', '__bf__'] term_frequency_adjustment_column_prefix \u00b6 The prefix to use for the columns that will be created to store the term frequency adjustments Default value : tf_ Examples : ['tf_', '__tf__'] comparison_vector_value_column_prefix \u00b6 The prefix to use for the columns that will be created to store the comparison vector values Default value : gamma_ Examples : ['gamma_', '__gamma__'] sql_dialect \u00b6 The SQL dialect in which sql_conditions are written. Must be a valid sqlglot dialect Default value : None Examples : ['spark', 'duckdb', 'presto', 'sqlite'] Settings keys nested within each member of comparisons \u00b6 output_column_name \u00b6 The name used to refer to this comparison in the output dataset. By default, Splink will set this to the name(s) of any input columns used in the comparison. This key is most useful to give a clearer description to comparisons that use multiple input columns. e.g. a location column that uses postcode and town may be named location For a comparison column that uses a single input column, e.g. first_name, this will be set first_name. For comparison columns that use multiple columns, if left blank, this will be set to the concatenation of columns used. Examples : ['first_name', 'surname'] comparison_description \u00b6 An optional label to describe this comparison, to be used in charting outputs. Examples : ['First name exact match', 'Surname with middle levenshtein level'] comparison_levels \u00b6 Comparison levels specify how input values should be compared. Each level corresponds to an assessment of similarity, such as exact match, jaro winkler match, one side of the match being null, etc Each comparison level represents a branch of a SQL case expression. They are specified in order of evaluation, each with a sql_condition that represents the branch of a case expression Examples : [{'sql_condition': 'first_name_l IS NULL OR first_name_r IS NULL', 'label': 'null', 'null_level': True}, {'sql_condition': 'first_name_l = first_name_r', 'label': 'exact_match', 'tf_adjustment_column': 'first_name'}, {'sql_condition': 'ELSE', 'label': 'else'}] Settings keys nested within each member of comparison_levels \u00b6 sql_condition \u00b6 A branch of a SQL case expression without WHEN and THEN e.g. 'jaro_winkler_sim(surname_l, surname_r) > 0.88' Examples : ['forename_l = forename_r', 'jaro_winkler_sim(surname_l, surname_r) > 0.88'] label_for_charts \u00b6 A label for this comparson level, which will appear on charts as a reminder of what the level represents Examples : ['exact', 'postcode exact'] u_probability \u00b6 the u probability for this comparison level - i.e. the proportion of records that match this level amongst truly non-matching records Examples : [0.9] m_probability \u00b6 the m probability for this comparison level - i.e. the proportion of records that match this level amongst truly matching records Examples : [0.1] is_null_level \u00b6 If true, m and u values will not be estimated and instead the match weight will be zero for this column. See treatment of nulls here on page 356, quote '. Under this MAR assumption, we can simply ignore missing data.': https://imai.fas.harvard.edu/research/files/linkage.pdf Default value : False tf_adjustment_column \u00b6 Make term frequency adjustments for this comparison level using this input column Default value : None Examples : ['first_name', 'postcode'] tf_adjustment_weight \u00b6 Make term frequency adjustments using this weight. A weight of 1.0 is a full adjustment. A weight of 0.0 is no adjustment. A weight of 0.5 is a half adjustment Default value : 1.0 Examples : ['first_name', 'postcode'] tf_minimum_u_value \u00b6 Where the term frequency adjustment implies a u value below this value, use this minimum value instead This prevents excessive weight being assigned to very unusual terms, such as a collision on a typo Default value : 0.0 Examples : [0.001, 1e-09]","title":"Settings dictionary reference"},{"location":"settings_dict_guide.html#guide-to-splink-settings","text":"This document enumerates all the settings and configuration options available when developing your data linkage model. You can find an interative settings editor here .","title":"Guide to Splink settings"},{"location":"settings_dict_guide.html#settings-keys-in-the-base-setting-dictionary","text":"","title":"Settings keys in the base setting dictionary"},{"location":"settings_dict_guide.html#link_type","text":"The type of data linking task. Required. When dedupe_only , splink find duplicates. User expected to provide a single input dataset. When link_and_dedupe , splink finds links within and between input datasets. User is expected to provide two or more input datasets. When link_only , splink finds links between datasets, but does not attempt to deduplicate the datasets (it does not try and find links within each input dataset.) User is expected to provide two or more input datasets. Examples : ['dedupe_only', 'link_only', 'link_and_dedupe']","title":"link_type"},{"location":"settings_dict_guide.html#probability_two_random_records_match","text":"The probability that two records chosen at random (with no blocking) are a match. For example, if there are a million input records and each has on average one match, then this value should be 1/1,000,000. If you estimate parameters using expectation maximisation (EM), this provides an initial value (prior) from which the EM algorithm will start iterating. EM will then estimate the true value of this parameter. Default value : 0.0001 Examples : [1e-05, 0.006]","title":"probability_two_random_records_match"},{"location":"settings_dict_guide.html#em_convergence","text":"Convergence tolerance for the Expectation Maximisation algorithm The algorithm will stop converging when the maximum of the change in model parameters between iterations is below this value Default value : 0.0001 Examples : [0.0001, 1e-05, 1e-06]","title":"em_convergence"},{"location":"settings_dict_guide.html#max_iterations","text":"The maximum number of Expectation Maximisation iterations to run (even if convergence has not been reached) Default value : 25 Examples : [20, 150]","title":"max_iterations"},{"location":"settings_dict_guide.html#unique_id_column_name","text":"Splink requires that the input dataset has a column that uniquely identifies each reecord. unique_id_column_name is the name of the column in the input dataset representing this unique id For linking tasks, ids must be unique within each dataset being linked, and do not need to be globally unique across input datasets Default value : unique_id Examples : ['unique_id', 'id', 'pk']","title":"unique_id_column_name"},{"location":"settings_dict_guide.html#source_dataset_column_name","text":"The name of the column in the input dataset representing the source dataset Where we are linking datasets, we can't guarantee that the unique id column is globally unique across datasets, so we combine it with a source_dataset column. Usually, this is created by Splink for the user Default value : source_dataset Examples : ['source_dataset', 'dataset_name']","title":"source_dataset_column_name"},{"location":"settings_dict_guide.html#retain_matching_columns","text":"If set to true, each column used by the comparisons sql expressions will be retained in output datasets This is helpful so that the user can inspect matches, but once the comparison vector (gamma) columns are computed, this information is not actually needed by the algorithm. The algorithm will run faster and use less resources if this is set to false. Default value : True Examples : [False, True]","title":"retain_matching_columns"},{"location":"settings_dict_guide.html#retain_intermediate_calculation_columns","text":"Retain intermediate calculation columns, such as the bayes factors associated with each column in comparisons The algorithm will run faster and use less resources if this is set to false. Default value : False Examples : [False, True]","title":"retain_intermediate_calculation_columns"},{"location":"settings_dict_guide.html#comparisons","text":"A list specifying how records should be compared for probabalistic matching. Each element is a dictionary","title":"comparisons"},{"location":"settings_dict_guide.html#blocking_rules_to_generate_predictions","text":"A list of one or more blocking rules to apply. A cartesian join is applied if blocking_rules_to_generate_predictions is empty or not supplied. Each rule is a SQL expression representing the blocking rule, which will be used to create a join. The left table is aliased with l and the right table is aliased with r . For example, if you want to block on a first_name column, the blocking rule would be l.first_name = r.first_name . To block on first name and the first letter of surname, it would be l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1) . Note that splink deduplicates the comparisons generated by the blocking rules. If empty or not supplied, all comparisons between the input dataset(s) will be generated and blocking will not be used. For large input datasets, this will generally be computationally intractable because it will generate comparisons equal to the number of rows squared. Default value : [] Examples : [['l.first_name = r.first_name AND l.surname = r.surname', 'l.dob = r.dob']]","title":"blocking_rules_to_generate_predictions"},{"location":"settings_dict_guide.html#additional_columns_to_retain","text":"A list of columns not being used in the probabalistic matching comparisons that you want to include in your results. By default, splink drops columns which are not used by any comparisons. This gives you the option to retain columns which are not used by the model. A common example is if the user has labelled data (training data) and wishes to retain the labels in the outputs Default value : [] Examples : [['cluster', 'col_2'], ['other_information']]","title":"additional_columns_to_retain"},{"location":"settings_dict_guide.html#bayes_factor_column_prefix","text":"The prefix to use for the columns that will be created to store the bayes factors Default value : bf_ Examples : ['bf_', '__bf__']","title":"bayes_factor_column_prefix"},{"location":"settings_dict_guide.html#term_frequency_adjustment_column_prefix","text":"The prefix to use for the columns that will be created to store the term frequency adjustments Default value : tf_ Examples : ['tf_', '__tf__']","title":"term_frequency_adjustment_column_prefix"},{"location":"settings_dict_guide.html#comparison_vector_value_column_prefix","text":"The prefix to use for the columns that will be created to store the comparison vector values Default value : gamma_ Examples : ['gamma_', '__gamma__']","title":"comparison_vector_value_column_prefix"},{"location":"settings_dict_guide.html#sql_dialect","text":"The SQL dialect in which sql_conditions are written. Must be a valid sqlglot dialect Default value : None Examples : ['spark', 'duckdb', 'presto', 'sqlite']","title":"sql_dialect"},{"location":"settings_dict_guide.html#settings-keys-nested-within-each-member-of-comparisons","text":"","title":"Settings keys nested within each member of comparisons"},{"location":"settings_dict_guide.html#output_column_name","text":"The name used to refer to this comparison in the output dataset. By default, Splink will set this to the name(s) of any input columns used in the comparison. This key is most useful to give a clearer description to comparisons that use multiple input columns. e.g. a location column that uses postcode and town may be named location For a comparison column that uses a single input column, e.g. first_name, this will be set first_name. For comparison columns that use multiple columns, if left blank, this will be set to the concatenation of columns used. Examples : ['first_name', 'surname']","title":"output_column_name"},{"location":"settings_dict_guide.html#comparison_description","text":"An optional label to describe this comparison, to be used in charting outputs. Examples : ['First name exact match', 'Surname with middle levenshtein level']","title":"comparison_description"},{"location":"settings_dict_guide.html#comparison_levels","text":"Comparison levels specify how input values should be compared. Each level corresponds to an assessment of similarity, such as exact match, jaro winkler match, one side of the match being null, etc Each comparison level represents a branch of a SQL case expression. They are specified in order of evaluation, each with a sql_condition that represents the branch of a case expression Examples : [{'sql_condition': 'first_name_l IS NULL OR first_name_r IS NULL', 'label': 'null', 'null_level': True}, {'sql_condition': 'first_name_l = first_name_r', 'label': 'exact_match', 'tf_adjustment_column': 'first_name'}, {'sql_condition': 'ELSE', 'label': 'else'}]","title":"comparison_levels"},{"location":"settings_dict_guide.html#settings-keys-nested-within-each-member-of-comparison_levels","text":"","title":"Settings keys nested within each member of comparison_levels"},{"location":"settings_dict_guide.html#sql_condition","text":"A branch of a SQL case expression without WHEN and THEN e.g. 'jaro_winkler_sim(surname_l, surname_r) > 0.88' Examples : ['forename_l = forename_r', 'jaro_winkler_sim(surname_l, surname_r) > 0.88']","title":"sql_condition"},{"location":"settings_dict_guide.html#label_for_charts","text":"A label for this comparson level, which will appear on charts as a reminder of what the level represents Examples : ['exact', 'postcode exact']","title":"label_for_charts"},{"location":"settings_dict_guide.html#u_probability","text":"the u probability for this comparison level - i.e. the proportion of records that match this level amongst truly non-matching records Examples : [0.9]","title":"u_probability"},{"location":"settings_dict_guide.html#m_probability","text":"the m probability for this comparison level - i.e. the proportion of records that match this level amongst truly matching records Examples : [0.1]","title":"m_probability"},{"location":"settings_dict_guide.html#is_null_level","text":"If true, m and u values will not be estimated and instead the match weight will be zero for this column. See treatment of nulls here on page 356, quote '. Under this MAR assumption, we can simply ignore missing data.': https://imai.fas.harvard.edu/research/files/linkage.pdf Default value : False","title":"is_null_level"},{"location":"settings_dict_guide.html#tf_adjustment_column","text":"Make term frequency adjustments for this comparison level using this input column Default value : None Examples : ['first_name', 'postcode']","title":"tf_adjustment_column"},{"location":"settings_dict_guide.html#tf_adjustment_weight","text":"Make term frequency adjustments using this weight. A weight of 1.0 is a full adjustment. A weight of 0.0 is no adjustment. A weight of 0.5 is a half adjustment Default value : 1.0 Examples : ['first_name', 'postcode']","title":"tf_adjustment_weight"},{"location":"settings_dict_guide.html#tf_minimum_u_value","text":"Where the term frequency adjustment implies a u value below this value, use this minimum value instead This prevents excessive weight being assigned to very unusual terms, such as a collision on a typo Default value : 0.0 Examples : [0.001, 1e-09]","title":"tf_minimum_u_value"},{"location":"demos/index.html","text":"splink_demos \u00b6 This repo contains interactive notebooks containing demonstration and tutorial for version 3 of the Splink record linking library, the homepage for which is here . Running these notebooks interactively \u00b6 You can run these notebooks in an interactive Jupyter notebook by clicking the button below: Running these notebooks locally in VSCode \u00b6 If you don't already have it, you'll need to install java on your system in order to run pyspark , which splink currently depends on. Download java for your specific OS from here . You can check the installation went correctly by using: java -version within a terminal instance. It should return details of your java installation. If you have multiple java installations, you may need to change the version of java you're currently using. To download the example notebooks, simply clone this repository: git clone git@github.com:moj-analytical-services/splink_demos.git Create a virtual environment using: python3 -m venv venv source venv/bin/activate Install the package list (which includes pyspark ) with: pip3 install -r requirements.txt and, if you want to use jupyter, add a kernel corresopnding to your venv: python -m ipykernel install --user --name=splink_demos jupyter lab","title":"splink_demos"},{"location":"demos/index.html#splink_demos","text":"This repo contains interactive notebooks containing demonstration and tutorial for version 3 of the Splink record linking library, the homepage for which is here .","title":"splink_demos"},{"location":"demos/index.html#running-these-notebooks-interactively","text":"You can run these notebooks in an interactive Jupyter notebook by clicking the button below:","title":"Running these notebooks interactively"},{"location":"demos/index.html#running-these-notebooks-locally-in-vscode","text":"If you don't already have it, you'll need to install java on your system in order to run pyspark , which splink currently depends on. Download java for your specific OS from here . You can check the installation went correctly by using: java -version within a terminal instance. It should return details of your java installation. If you have multiple java installations, you may need to change the version of java you're currently using. To download the example notebooks, simply clone this repository: git clone git@github.com:moj-analytical-services/splink_demos.git Create a virtual environment using: python3 -m venv venv source venv/bin/activate Install the package list (which includes pyspark ) with: pip3 install -r requirements.txt and, if you want to use jupyter, add a kernel corresopnding to your venv: python -m ipykernel install --user --name=splink_demos jupyter lab","title":"Running these notebooks locally in VSCode"},{"location":"demos/00_Tutorial_Introduction.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introductory tutorial \u00b6 This is the introduction to a five part tutorial which demonstrates how to de-duplicate a small dataset using simple settings. The aim of the tutorial is to demonstrate core Splink functionality succinctly, rather that comprehensively document all configuration options. The seven parts are: 1. Data prep pre-requisites 2. Exploratory analysis 3. Choosing blocking rules to optimise runtimes 4. Estimating model parameters 5. Predicting results 6. Visualising predictions 7. Quality assurance Throughout the tutorial, we use the duckdb backend, which is the recommended option for smaller datasets of up to around 1 million records on a normal laptop. You can find these tutorial notebooks in the splink_demos repo, and you can run them live in your web browser by clicking the following link: End-to-end demos \u00b6 After following the steps of the tutorial, it might prove useful to have a look at some of the example notebooks that show various use-case scenarios of Splink from start to finish.","title":"0. Tutorial introduction"},{"location":"demos/00_Tutorial_Introduction.html#introductory-tutorial","text":"This is the introduction to a five part tutorial which demonstrates how to de-duplicate a small dataset using simple settings. The aim of the tutorial is to demonstrate core Splink functionality succinctly, rather that comprehensively document all configuration options. The seven parts are: 1. Data prep pre-requisites 2. Exploratory analysis 3. Choosing blocking rules to optimise runtimes 4. Estimating model parameters 5. Predicting results 6. Visualising predictions 7. Quality assurance Throughout the tutorial, we use the duckdb backend, which is the recommended option for smaller datasets of up to around 1 million records on a normal laptop. You can find these tutorial notebooks in the splink_demos repo, and you can run them live in your web browser by clicking the following link:","title":"Introductory tutorial"},{"location":"demos/00_Tutorial_Introduction.html#end-to-end-demos","text":"After following the steps of the tutorial, it might prove useful to have a look at some of the example notebooks that show various use-case scenarios of Splink from start to finish.","title":"End-to-end demos"},{"location":"demos/01_Prerequisites.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Data Prerequisites \u00b6 Splink requires that you clean your data and assign unique IDs to rows before linking. This section outlines the additional data cleaning steps needed before loading data into Splink. Unique IDs \u00b6 Each input dataset must have a unique ID column, which is unique within the dataset. By default, Splink assumes this column will be called unique_id , but this can be changed with the unique_id_column_name key in your Splink settings. The unique id is essential because it enables Splink to keep track each row correctly. Conformant input datasets \u00b6 Input datasets must be conformant, meaning they share the same column names and data formats. For instance, if one dataset has a \"date of birth\" column and another has a \"dob\" column, rename them to match. Ensure data type and number formatting are consistent across both columns. The order of columns in input dataframes is not important. Cleaning \u00b6 Ensure data consistency by cleaning your data. This process includes standardizing date formats, matching text case, and handling invalid data. For example, if one dataset uses \"yyyy-mm-dd\" date format and another uses \"mm/dd/yyyy,\" convert them to the same format before using Splink. Try also to identify and rectify any obvious data entry errors, such as removing values such as 'Mr' or 'Mrs' from a 'first name' column. Ensure nulls are consistently and correctly represented \u00b6 Ensure null values (or other 'not known' indicators) are represented as true nulls, not empty strings. Splink treats null values differently from empty strings, so using true nulls guarantees proper matching across datasets. Further details on data cleaning and standardisation \u00b6 Splink performs optimally with cleaned and standardized data. Here is a non-exhaustive list of suggestions for data cleaning rules to enhance matching accuracy: Trim leading and trailing whitespace from string values (e.g., \" john smith \" becomes \"john smith\"). Remove special characters from string values (e.g., \"O'Hara\" becomes \"Ohara\"). Standardise date formats as strings in \"yyyy-mm-dd\" format. Replace abbreviations with full words (e.g., standardize \"St.\" and \"Street\" to \"Street\").","title":"1. Data prep prerequisites"},{"location":"demos/01_Prerequisites.html#data-prerequisites","text":"Splink requires that you clean your data and assign unique IDs to rows before linking. This section outlines the additional data cleaning steps needed before loading data into Splink.","title":"Data Prerequisites"},{"location":"demos/01_Prerequisites.html#unique-ids","text":"Each input dataset must have a unique ID column, which is unique within the dataset. By default, Splink assumes this column will be called unique_id , but this can be changed with the unique_id_column_name key in your Splink settings. The unique id is essential because it enables Splink to keep track each row correctly.","title":"Unique IDs"},{"location":"demos/01_Prerequisites.html#conformant-input-datasets","text":"Input datasets must be conformant, meaning they share the same column names and data formats. For instance, if one dataset has a \"date of birth\" column and another has a \"dob\" column, rename them to match. Ensure data type and number formatting are consistent across both columns. The order of columns in input dataframes is not important.","title":"Conformant input datasets"},{"location":"demos/01_Prerequisites.html#cleaning","text":"Ensure data consistency by cleaning your data. This process includes standardizing date formats, matching text case, and handling invalid data. For example, if one dataset uses \"yyyy-mm-dd\" date format and another uses \"mm/dd/yyyy,\" convert them to the same format before using Splink. Try also to identify and rectify any obvious data entry errors, such as removing values such as 'Mr' or 'Mrs' from a 'first name' column.","title":"Cleaning"},{"location":"demos/01_Prerequisites.html#ensure-nulls-are-consistently-and-correctly-represented","text":"Ensure null values (or other 'not known' indicators) are represented as true nulls, not empty strings. Splink treats null values differently from empty strings, so using true nulls guarantees proper matching across datasets.","title":"Ensure nulls are consistently and correctly represented"},{"location":"demos/01_Prerequisites.html#further-details-on-data-cleaning-and-standardisation","text":"Splink performs optimally with cleaned and standardized data. Here is a non-exhaustive list of suggestions for data cleaning rules to enhance matching accuracy: Trim leading and trailing whitespace from string values (e.g., \" john smith \" becomes \"john smith\"). Remove special characters from string values (e.g., \"O'Hara\" becomes \"Ohara\"). Standardise date formats as strings in \"yyyy-mm-dd\" format. Replace abbreviations with full words (e.g., standardize \"St.\" and \"Street\" to \"Street\").","title":"Further details on data cleaning and standardisation"},{"location":"demos/02_Exploratory_analysis.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exploratory analysis \u00b6 The purpose of exploratory analysis is to understand your data and any idiosyncrasies which may be relevant to the task of data linking. Splink includes functionality to visualise and summarise your data, to identify characteristics most salient to data linking. In this notebook we perform some basic exploratory analysis, and interpret the results. Read in the data \u00b6 For the purpose of this tutorial we will use a 1,000 row synthetic dataset that contains duplicates. The first five rows of this dataset are printed below. Note that the cluster column represents the 'ground truth' - a column which tells us with which rows refer to the same person. In most real linkage scenarios, we wouldn't have this column (this is what Splink is trying to estimate.) import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 2 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 3 3 Robert Alen 1971-06-24 Lonon NaN 0 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 Instantiate the linker \u00b6 Most of Splink's core functionality can be accessed as methods on a linker object. For example, to make predictions, you would call linker.predict() . We therefore begin by instantiating the linker, passing in the data we wish to deduplicate. # Initialise the linker, passing in the input dataset(s) from splink.duckdb.duckdb_linker import DuckDBLinker linker = DuckDBLinker ( df ) Analyse missingness \u00b6 It's important to understand the level of missingness in your data, because columns with higher levels of missingness are less useful for data linking. linker . missingness_chart () The above summary chart shows that in this dataset, the email , city , surname and forename columns contain nulls, but the level of missingness is relatively low (less than 22%). Analyse the distribution of values in your data \u00b6 The distribution of values in your data is important for two main reasons: Columns with higher cardinality (number of distinct values) are usually more useful for data linking. For instance, date of birth is a much stronger linkage variable than gender. The skew of values is important. If you have a city column that has 1,000 distinct values, but 75% of them are London , this is much less useful for linkage than if the 1,000 values were equally distributed The linker.profile_columns() method creates summary charts to help you understand these aspects of your data. You may input column names (e.g. first_name ), or arbitrary sql expressions like concat(first_name, surname) . linker . profile_columns ([ \"first_name\" , \"city\" , \"surname\" , \"email\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) This chart is very information-dense, but here are some key takehomes relevant to our linkage: There is strong skew in the city field with around 20% of the values being London . We therefore will probably want to use term_frequency_adjustments in our linkage model, so that it can weight a match on London differently to a match on, say, Norwich . Looking at the \"Bottom 5 values by value count\", we can see typos in the data in most fields. This tells us this information was possibly entered by hand, or using Optical Character Recognition, giving us an insight into the type of data entry errors we may see. Email is a much more uniquely-identifying field than any others, with a maximum value count of 6. It's likely to be a strong linking variable. Next steps \u00b6 At this point, we have begin to develop a strong understanding of our data. It's time to move on to estimating a linkage model Further reading \u00b6 You can find the documentation for the exploratory analysis tools in Splink here","title":"2. Exploratory analysis"},{"location":"demos/02_Exploratory_analysis.html#exploratory-analysis","text":"The purpose of exploratory analysis is to understand your data and any idiosyncrasies which may be relevant to the task of data linking. Splink includes functionality to visualise and summarise your data, to identify characteristics most salient to data linking. In this notebook we perform some basic exploratory analysis, and interpret the results.","title":"Exploratory analysis"},{"location":"demos/02_Exploratory_analysis.html#read-in-the-data","text":"For the purpose of this tutorial we will use a 1,000 row synthetic dataset that contains duplicates. The first five rows of this dataset are printed below. Note that the cluster column represents the 'ground truth' - a column which tells us with which rows refer to the same person. In most real linkage scenarios, we wouldn't have this column (this is what Splink is trying to estimate.) import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 2 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 3 3 Robert Alen 1971-06-24 Lonon NaN 0 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1","title":"Read in the data"},{"location":"demos/02_Exploratory_analysis.html#instantiate-the-linker","text":"Most of Splink's core functionality can be accessed as methods on a linker object. For example, to make predictions, you would call linker.predict() . We therefore begin by instantiating the linker, passing in the data we wish to deduplicate. # Initialise the linker, passing in the input dataset(s) from splink.duckdb.duckdb_linker import DuckDBLinker linker = DuckDBLinker ( df )","title":"Instantiate the linker"},{"location":"demos/02_Exploratory_analysis.html#analyse-missingness","text":"It's important to understand the level of missingness in your data, because columns with higher levels of missingness are less useful for data linking. linker . missingness_chart () The above summary chart shows that in this dataset, the email , city , surname and forename columns contain nulls, but the level of missingness is relatively low (less than 22%).","title":"Analyse missingness"},{"location":"demos/02_Exploratory_analysis.html#analyse-the-distribution-of-values-in-your-data","text":"The distribution of values in your data is important for two main reasons: Columns with higher cardinality (number of distinct values) are usually more useful for data linking. For instance, date of birth is a much stronger linkage variable than gender. The skew of values is important. If you have a city column that has 1,000 distinct values, but 75% of them are London , this is much less useful for linkage than if the 1,000 values were equally distributed The linker.profile_columns() method creates summary charts to help you understand these aspects of your data. You may input column names (e.g. first_name ), or arbitrary sql expressions like concat(first_name, surname) . linker . profile_columns ([ \"first_name\" , \"city\" , \"surname\" , \"email\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) This chart is very information-dense, but here are some key takehomes relevant to our linkage: There is strong skew in the city field with around 20% of the values being London . We therefore will probably want to use term_frequency_adjustments in our linkage model, so that it can weight a match on London differently to a match on, say, Norwich . Looking at the \"Bottom 5 values by value count\", we can see typos in the data in most fields. This tells us this information was possibly entered by hand, or using Optical Character Recognition, giving us an insight into the type of data entry errors we may see. Email is a much more uniquely-identifying field than any others, with a maximum value count of 6. It's likely to be a strong linking variable.","title":"Analyse the distribution of values in your data"},{"location":"demos/02_Exploratory_analysis.html#next-steps","text":"At this point, we have begin to develop a strong understanding of our data. It's time to move on to estimating a linkage model","title":"Next steps"},{"location":"demos/02_Exploratory_analysis.html#further-reading","text":"You can find the documentation for the exploratory analysis tools in Splink here","title":"Further reading"},{"location":"demos/03_Blocking.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Choosing blocking rules to optimise runtime \u00b6 To link records, we need to compare pairs of records, and decide which pairs are matches and non matches. For most large datasets, it is computationally intractable to compare every row with every other row, since the number of comparisons rises quadratically with the number of records. Instead we rely on blocking rules, which specify which pairwise comparisons to generate. For example, we could generate the subset of pairwise comparisons where either first name or surname matches. This is part of a two step process to link data: Use blocking rules to generate candidate pairwise record comparisons Use a probabilistic linkage model to score these candidate pairs, to determine which ones should be linked Blocking rules are the most important determinant of the performance of your linkage job . When deciding on your blocking rules, you're trading off accuracy for performance: If your rules are too loose, your linkage job may fail. If they're too tight, you may miss some valid links. This tutorial clarifies what blocking rules are, and how to choose good rules. Blocking rules in Splink \u00b6 In Splink, blocking rules are specified as SQL expressions. For example, to generate the subset of record comparisons where the first name matches, we can specify the following blocking rule: l.first_name = r.first_name Since blocking rules are SQL expressions, they can be arbitrarily complex. For example, you could create record comparisons where the initial of the first name and the surname match with the following rule: substr(l.first_name, 1,1) = substr(r.first_name, 1,1) and l.surname = r.surname Devising effective blocking rules \u00b6 The aims of your blocking rules are twofold: 1. Eliminate enough non-matching comparison pairs so your record linkage job is small enough to compute 2. Eliminate as few truly matching pairs as possible (ideally none) It is usually impossible to find a single blocking rule which achieves both aims, so we recommend using multiple blocking rules. When we specify multiple blocking rules, Splink will generate all comparison pairs that meet any one of the rules. For example, consider the following blocking rule: l.first_name = r.first_name and l.dob = r.dob This rule is likely to be effective in reducing the number of comparison pairs. It will retain all truly matching pairs, except those with errors or nulls in either the first_name or dob fields. Now consider a second blocking rule: l.email and r.email . This will retain all truly matching pairs, except those with errors or nulls in the email column. Individually, these blocking rules are problematic because they exclude true matches where the records contain typos of certain types. But between them, they might do quite a good job. For a true match to be eliminated by the use of these two blocking rules, it would have to have an error in both email AND (first name or date of birth). This is not completely implausible, but it is significantly less likely than if we'd just used a single rule. More generally, we can often specify multiple blocking rules such that it becomes highly implausible that a true match would not meet at least one of these blocking critera. This is the recommended approach in Splink. Generally we would recommend between about 3 and 10, though even more is possible. The question then becomes how to choose what to put in this list. Splink tools to help choose your blocking rules \u00b6 Splink contains a number of tools to help you choose effective blocking rules. Let's try them out, using our small test dataset: import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) Counting the number of comparisons created by a single blocking rule \u00b6 On large datasets, some blocking rules imply the creation of trillions of record comparisons, which would cause a linkage job to fail. Before using a blocking rule in a linkage job, it's therefore a good idea to count the number of records it generates to ensure it is not too loose: from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"dedupe_only\" } linker = DuckDBLinker ( df , settings ) blocking_rule_1 = \"substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_1 ) print ( f \"Number of comparisons generated by ' { blocking_rule_1 } ': { count : ,.0f } \" ) blocking_rule_2 = \"l.surname = r.surname\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_2 ) print ( f \"Number of comparisons generated by ' { blocking_rule_2 } ': { count : ,.0f } \" ) blocking_rule_3 = \"l.email = r.email\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_3 ) print ( f \"Number of comparisons generated by ' { blocking_rule_3 } ': { count : ,.0f } \" ) blocking_rule_3 = \"l.city = r.city and l.first_name = r.first_name\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_3 ) print ( f \"Number of comparisons generated by ' { blocking_rule_3 } ': { count : ,.0f } \" ) Number of comparisons generated by 'substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname': 473 Number of comparisons generated by 'l.surname = r.surname': 1,638 Number of comparisons generated by 'l.email = r.email': 682 Number of comparisons generated by 'l.city = r.city and l.first_name = r.first_name': 315 The maximum number of comparisons that you can compute will be affected by your choice of SQL backend, and how powerful your computer is. For linkages in DuckDB on a standard laptop, we suggest using blocking rules that create no more than about 20 million comparisons. For Spark and Athena, try starting with fewer than a a billion comparisons, before scaling up. Counting the number of comparisons created by a list of blocking rules \u00b6 As noted above, it's usually a good idea to use multiple blocking rules. It's therefore useful to know how many record comparisons will be generated when these rules are applied. Since the same record comparison may be created by several blocking rules, and Splink automatically deduplicates these comparisons, we cannot simply total the number of comparisons generated by each rule individually. Splink provides a chart that shows the marginal (additional) comparisons generated by each blocking rule, after deduplication: blocking_rules = [ blocking_rule_1 , blocking_rule_2 , blocking_rule_3 ] linker . cumulative_num_comparisons_from_blocking_rules_chart ( blocking_rules ) Understanding why certain blocking rules create large numbers of comparisons \u00b6 Finally, we can use the profile_columns function we saw in the previous tutorial to understand a specific blocking rule in more depth. Suppose we're interested in blocking on city and first initial. Within each distinct value of (city, first initial) , all possible pairwise comparisons will be generated. So for instance, if there are 15 distinct records with London,J then these records will result in n(n-1)/2 = 105 pairwise comparisons being generated. In a larger dataset, we might observe 10,000 London,J records, which would then be responsible for 49,995,000 comparisons. These high-frequency values therefore have a disproportionate influence on the overall number of pairwise comparisons, and so it can be useful to analyse skew, as follows: linker . profile_columns ( \"city || left(first_name,1) \" )","title":"3. Blocking"},{"location":"demos/03_Blocking.html#choosing-blocking-rules-to-optimise-runtime","text":"To link records, we need to compare pairs of records, and decide which pairs are matches and non matches. For most large datasets, it is computationally intractable to compare every row with every other row, since the number of comparisons rises quadratically with the number of records. Instead we rely on blocking rules, which specify which pairwise comparisons to generate. For example, we could generate the subset of pairwise comparisons where either first name or surname matches. This is part of a two step process to link data: Use blocking rules to generate candidate pairwise record comparisons Use a probabilistic linkage model to score these candidate pairs, to determine which ones should be linked Blocking rules are the most important determinant of the performance of your linkage job . When deciding on your blocking rules, you're trading off accuracy for performance: If your rules are too loose, your linkage job may fail. If they're too tight, you may miss some valid links. This tutorial clarifies what blocking rules are, and how to choose good rules.","title":"Choosing blocking rules to optimise runtime"},{"location":"demos/03_Blocking.html#blocking-rules-in-splink","text":"In Splink, blocking rules are specified as SQL expressions. For example, to generate the subset of record comparisons where the first name matches, we can specify the following blocking rule: l.first_name = r.first_name Since blocking rules are SQL expressions, they can be arbitrarily complex. For example, you could create record comparisons where the initial of the first name and the surname match with the following rule: substr(l.first_name, 1,1) = substr(r.first_name, 1,1) and l.surname = r.surname","title":"Blocking rules in Splink"},{"location":"demos/03_Blocking.html#devising-effective-blocking-rules","text":"The aims of your blocking rules are twofold: 1. Eliminate enough non-matching comparison pairs so your record linkage job is small enough to compute 2. Eliminate as few truly matching pairs as possible (ideally none) It is usually impossible to find a single blocking rule which achieves both aims, so we recommend using multiple blocking rules. When we specify multiple blocking rules, Splink will generate all comparison pairs that meet any one of the rules. For example, consider the following blocking rule: l.first_name = r.first_name and l.dob = r.dob This rule is likely to be effective in reducing the number of comparison pairs. It will retain all truly matching pairs, except those with errors or nulls in either the first_name or dob fields. Now consider a second blocking rule: l.email and r.email . This will retain all truly matching pairs, except those with errors or nulls in the email column. Individually, these blocking rules are problematic because they exclude true matches where the records contain typos of certain types. But between them, they might do quite a good job. For a true match to be eliminated by the use of these two blocking rules, it would have to have an error in both email AND (first name or date of birth). This is not completely implausible, but it is significantly less likely than if we'd just used a single rule. More generally, we can often specify multiple blocking rules such that it becomes highly implausible that a true match would not meet at least one of these blocking critera. This is the recommended approach in Splink. Generally we would recommend between about 3 and 10, though even more is possible. The question then becomes how to choose what to put in this list.","title":"Devising effective blocking rules"},{"location":"demos/03_Blocking.html#splink-tools-to-help-choose-your-blocking-rules","text":"Splink contains a number of tools to help you choose effective blocking rules. Let's try them out, using our small test dataset: import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" )","title":"Splink tools to help choose your blocking rules"},{"location":"demos/03_Blocking.html#counting-the-number-of-comparisons-created-by-a-single-blocking-rule","text":"On large datasets, some blocking rules imply the creation of trillions of record comparisons, which would cause a linkage job to fail. Before using a blocking rule in a linkage job, it's therefore a good idea to count the number of records it generates to ensure it is not too loose: from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"dedupe_only\" } linker = DuckDBLinker ( df , settings ) blocking_rule_1 = \"substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_1 ) print ( f \"Number of comparisons generated by ' { blocking_rule_1 } ': { count : ,.0f } \" ) blocking_rule_2 = \"l.surname = r.surname\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_2 ) print ( f \"Number of comparisons generated by ' { blocking_rule_2 } ': { count : ,.0f } \" ) blocking_rule_3 = \"l.email = r.email\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_3 ) print ( f \"Number of comparisons generated by ' { blocking_rule_3 } ': { count : ,.0f } \" ) blocking_rule_3 = \"l.city = r.city and l.first_name = r.first_name\" count = linker . count_num_comparisons_from_blocking_rule ( blocking_rule_3 ) print ( f \"Number of comparisons generated by ' { blocking_rule_3 } ': { count : ,.0f } \" ) Number of comparisons generated by 'substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname': 473 Number of comparisons generated by 'l.surname = r.surname': 1,638 Number of comparisons generated by 'l.email = r.email': 682 Number of comparisons generated by 'l.city = r.city and l.first_name = r.first_name': 315 The maximum number of comparisons that you can compute will be affected by your choice of SQL backend, and how powerful your computer is. For linkages in DuckDB on a standard laptop, we suggest using blocking rules that create no more than about 20 million comparisons. For Spark and Athena, try starting with fewer than a a billion comparisons, before scaling up.","title":"Counting the number of comparisons created by a single blocking rule"},{"location":"demos/03_Blocking.html#counting-the-number-of-comparisons-created-by-a-list-of-blocking-rules","text":"As noted above, it's usually a good idea to use multiple blocking rules. It's therefore useful to know how many record comparisons will be generated when these rules are applied. Since the same record comparison may be created by several blocking rules, and Splink automatically deduplicates these comparisons, we cannot simply total the number of comparisons generated by each rule individually. Splink provides a chart that shows the marginal (additional) comparisons generated by each blocking rule, after deduplication: blocking_rules = [ blocking_rule_1 , blocking_rule_2 , blocking_rule_3 ] linker . cumulative_num_comparisons_from_blocking_rules_chart ( blocking_rules )","title":"Counting the number of comparisons created by a list of blocking rules"},{"location":"demos/03_Blocking.html#understanding-why-certain-blocking-rules-create-large-numbers-of-comparisons","text":"Finally, we can use the profile_columns function we saw in the previous tutorial to understand a specific blocking rule in more depth. Suppose we're interested in blocking on city and first initial. Within each distinct value of (city, first initial) , all possible pairwise comparisons will be generated. So for instance, if there are 15 distinct records with London,J then these records will result in n(n-1)/2 = 105 pairwise comparisons being generated. In a larger dataset, we might observe 10,000 London,J records, which would then be responsible for 49,995,000 comparisons. These high-frequency values therefore have a disproportionate influence on the overall number of pairwise comparisons, and so it can be useful to analyse skew, as follows: linker . profile_columns ( \"city || left(first_name,1) \" )","title":"Understanding why certain blocking rules create large numbers of comparisons"},{"location":"demos/04_Estimating_model_parameters.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Specifying and estimating a linkage model \u00b6 In the last tutorial we looked at how we can use blocking rules to generate pairwise record comparisons. Now it's time to estimate a probabilistic linkage model to score each of these comparisons. The resultant match score is a prediction of whether the two records represent the same entity (e.g. are the same person). The purpose of estimating the model is to learn the relative importance of different parts of your data for the purpose of data linking. For example, a match on date of birth is a much stronger indicator that two records refer to the same entity than a match on gender. A mismatch on gender may be a stronger indicate against two records referring than a mismatch on name, since names are more likely to be entered differently. The relative importance of different information is captured in the (partial) 'match weights', which can be learned from your data. These match weights are then added up to compute the overall match score. The match weights are are derived from the m and u parameters of the underlying Fellegi Sunter model. Splink uses various statistical routines to estimate these parameters. Further details of the underlying theory can be found here , which will help you understand this part of the tutorial. # Begin by reading in the tutorial data again from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd import altair as alt alt . renderers . enable ( \"mimetype\" ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) Specifying a linkage model \u00b6 To build a linkage model, the user defines the partial match weights that splink needs to estimate. This is done by defining how the information in the input records should be compared. To be concrete, here is an example comparison: first_name_l first_name_r surname_l surname_r dob_l dob_r city_l city_r email_l email_r Robert Rob Allen Allen 1971-05-24 1971-06-24 nan London roberta25@smith.net roberta25@smith.net What functions should we use to assess the similarity of Rob vs. Robert in the the first_name field? Should similarity in the dob field be computed in the same way, or a different way? Your job as the developer of a linkage model is to decide what comparisons are most appropriate for the types of data you have. Splink can then estimate how much weight to place on a fuzzy match of Rob vs. Robert , relative to an exact match on Robert , or a non-match. Defining these scenarios is done using Comparison s. Comparisons \u00b6 The concept of a Comparison has a specific definition within Splink: it defines how data from one or more input columns is compared, using SQL expressions to assess similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Another Comparison may represent the comparison of a person's name or location. A model is composed of many Comparison s, which between them assess the similarity of all of the columns being used for data linking. Each Comparison contains two or more ComparisonLevels which define n discrete gradations of similarity between the input columns within the Comparison. As such ComparisonLevels are nested within Comparisons as follows: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on surname \u2502 \u251c\u2500-- ComparisonLevel: All other \u2502 etc. Our example data would therefore result in the following comparisons, for dob and surname : dob_l dob_r comparison_level interpretation 1971-05-24 1971-05-24 Exact match great match 1971-05-24 1971-06-24 One character difference ok match 1971-05-24 2000-01-02 All other bad match surname_l surname_r comparison_level interpretation Rob Rob Exact match great match Rob Jane All other bad match Rob Robert All other bad match, this comparison has no notion of nicknames More information about comparisons can be found here . We will now use these concepts to build a data linking model. Specifying the model using comparisons \u00b6 Splink includes libraries of comparison functions to make it simple to get started: Let's start by looking at a Comparison for first_name : import splink.duckdb.duckdb_comparison_library as cl first_name_comparison = cl . levenshtein_at_thresholds ( \"first_name\" , 2 ) print ( first_name_comparison . human_readable_description ) Comparison 'Exact match vs. levenshtein at threshold 2 vs. anything else' of \"first_name\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL - 'Exact match' with SQL rule: \"first_name_l\" = \"first_name_r\" - 'levenshtein <= 2' with SQL rule: levenshtein(\"first_name_l\", \"first_name_r\") <= 2 - 'All other comparisons' with SQL rule: ELSE Specifying the full settings dictionary \u00b6 Comparisons are specified as part of the Splink settings , a Python dictionary which controls all of the configuration of a Splink model: settings = { \"link_type\" : \"dedupe_only\" , \"comparisons\" : [ cl . exact_match ( \"first_name\" ), cl . levenshtein_at_thresholds ( \"surname\" ), cl . levenshtein_at_thresholds ( \"dob\" , 1 ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings ) In words, this setting dictionary says: We are performing a dedupe_only (the other options are link_only , or link_and_dedupe , which may be used if there are multiple input datasets). When comparing records, we will use information from the first_name , surname , dob , city and email columns to compute a match score. The blocking_rules_to_generate_predictions states that we will only check for duplicates amongst records where either the first_name or surname is identical. We have enabled term frequency adjustments for the 'city' column, because some values (e.g. London ) appear much more frequently than others. We have set retain_intermediate_calculation_columns and additional_columns_to_retain to True so that Splink outputs additional information that helps the user understand the calculations. If they were False , the computations would run faster. Estimate the parameters of the model \u00b6 Now that we have specified our linkage model, we need to estimate the probability_two_random_records_match , u , and m parameters. The probability_two_random_records_match parameter is the probability that two records taken at random from your input data represent a match (typically a very small number). The u values are the proportion of records falling into each ComparisonLevel amongst truly non-matching records. The m values are the proportion of records falling into each ComparisonLevel amongst truly matching records You can read more about the theory of what these mean . We can estimate these parameters using unlabeled data. If we have labels, then we can estimate them even more accurately. Estimation of probability_two_random_records_match \u00b6 In some cases, the probability_two_random_records_match will be known. For example, if you are linking two tables of 10,000 records and expect a one-to-one match, then you should set this value to 1/10_000 in your settings instead of estimating it. More generally, this parameter is unknown and needs to be estimated. It can be estimated accurately enough for most purposes by combining a series of deterministic matching rules and a guess of the recall corresponding to those rules. For further details of the rationale behind this appraoch see here . In this example, I guess that the following deterministic matching rules have a recall of about 70%: deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) Probability two random records match is estimated to be 0.00333. This means that amongst all possible pairwise record comparisons, one in 300.13 are expected to match. With 499,500 total possible comparisons, we expect a total of around 1,664.29 matching pairs Estimation of u probabilities \u00b6 Once we have the probability_two_random_records_match parameter, we can estimate the u probabilities. We estimate u using the estimate_u_using_random_sampling method, which doesn't require any labels. It works by sampling random pairs of records, since most of these pairs are going to be non-matches. Over these non-matches we compute the distribution of ComparisonLevel s for each Comparison . For instance, for gender , we would find that the the gender matches 50% of the time, and mismatches 50% of the time. For dob on the other hand, we would find that the dob matches 1% of the time, has a \"one character difference\" 3% of the time, and everything else happens 96% of the time. The larger the random sample, the more accurate the predictions. You control this using the max_pairs parameter. For large datasets, we recommend using at least 10 million - but the higher the better and 1 billion is often appropriate for larger datasets. linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). Estimation of m probabilities \u00b6 m is the trickiest of the parameters to estimate, because we have to have some idea of what the true matches are. If we have labels, we can directly estimate it. If we do not have labelled data, the m parameters can be estimated using an iterative maximum likelihood approach called Expectation Maximisation. Estimating directly \u00b6 If we have labels, we can estimate m directly using the estimate_m_from_label_column method of the linker. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. For example (in this tutorial we don't have labels, so we're not actually going to use this): linker . estimate_m_from_label_column ( \"social_security_number\" ) Estimating with Expectation Maximisation \u00b6 This algorithm estimates the m values by generating pairwise record comparisons, and using them to maximise a likelihood function. Each estimation pass requires the user to configure an estimation blocking rule to reduce the number of record comparisons generated to a manageable level. In our first estimation pass, we block on first_name and surname , meaning we will generate all record comparisons that have first_name and surname exactly equal. Recall we are trying to estimate the m values of the model, i.e. proportion of records falling into each ComparisonLevel amongst truly matching records. This means that, in this training session, we cannot estimate parameter estimates for the first_name or surname columns, since they will be equal for all the comparisons we do. We can, however, estimate parameter estimates for all of the other columns. The output messages produced by Splink confirm this. training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_fname_sname = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was -0.514 in the m_probability of dob, level `Exact match` Iteration 2: Largest change in params was 0.0474 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0212 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0113 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00694 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00463 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00328 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00243 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00186 in probability_two_random_records_match Iteration 10: Largest change in params was 0.00146 in probability_two_random_records_match Iteration 11: Largest change in params was 0.00117 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000954 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000787 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000658 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000555 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000471 in probability_two_random_records_match Iteration 17: Largest change in params was 0.000404 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000347 in probability_two_random_records_match Iteration 19: Largest change in params was 0.000301 in probability_two_random_records_match Iteration 20: Largest change in params was 0.000261 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000228 in probability_two_random_records_match Iteration 22: Largest change in params was 0.0002 in probability_two_random_records_match Iteration 23: Largest change in params was 0.000175 in probability_two_random_records_match Iteration 24: Largest change in params was 0.000154 in probability_two_random_records_match Iteration 25: Largest change in params was 0.000136 in probability_two_random_records_match EM converged after 25 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). In a second estimation pass, we block on dob. This allows us to estimate parameters for the first_name and surname comparisons. Between the two estimation passes, we now have parameter estimates for all comparisons. from numpy import fix training_blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.392 in the m_probability of surname, level `Exact match` Iteration 2: Largest change in params was 0.137 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0416 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0171 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00853 in probability_two_random_records_match Iteration 6: Largest change in params was 0.0047 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00274 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00165 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00101 in probability_two_random_records_match Iteration 10: Largest change in params was 0.000629 in probability_two_random_records_match Iteration 11: Largest change in params was 0.000394 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000247 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000156 in probability_two_random_records_match Iteration 14: Largest change in params was 9.86e-05 in probability_two_random_records_match EM converged after 14 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values Note that Splink includes other algorithms for estimating m and u values, which are documented here . Visualising model parameters \u00b6 Splink can generate a number of charts to help you understand your model. For an introduction to these charts and how to interpret them, please see this video. The final estimated match weights can be viewed in the match weights chart: linker . match_weights_chart () linker . m_u_parameters_chart () Saving the model \u00b6 We can save the model, including our estimated parameters, to a .json file, so we can use it in the next tutorial. settings = linker . save_settings_to_json ( \"./demo_settings/saved_model_from_demo.json\" , overwrite = True ) Detecting unlinkable records \u00b6 An interesting application of our trained model that is useful to explore before making any predictions is to detect 'unlinkable' records. Unlinkable records are those which do not contain enough information to be linked. A simple example would be a record containing only 'John Smith', and null in all other fields. This record may link to other records, but we'll never know because there's not enough information to disambiguate any potential links. Unlinkable records can be found by linking records to themselves - if, even when matched to themselves, they don't meet the match threshold score, we can be sure they will never link to anything. linker . unlinkables_chart () In the above chart, we can see that about 1.3% of records in the input dataset are unlinkable at a threshold match weight of 6.11 (correponding to a match probability of around 98.6%) Next steps \u00b6 Now we have trained a model, we can move on to using it predict matching records. Further reading \u00b6 Full documentation for all of the ways of estimating model parameters can be found here .","title":"4. Estimating model parameters"},{"location":"demos/04_Estimating_model_parameters.html#specifying-and-estimating-a-linkage-model","text":"In the last tutorial we looked at how we can use blocking rules to generate pairwise record comparisons. Now it's time to estimate a probabilistic linkage model to score each of these comparisons. The resultant match score is a prediction of whether the two records represent the same entity (e.g. are the same person). The purpose of estimating the model is to learn the relative importance of different parts of your data for the purpose of data linking. For example, a match on date of birth is a much stronger indicator that two records refer to the same entity than a match on gender. A mismatch on gender may be a stronger indicate against two records referring than a mismatch on name, since names are more likely to be entered differently. The relative importance of different information is captured in the (partial) 'match weights', which can be learned from your data. These match weights are then added up to compute the overall match score. The match weights are are derived from the m and u parameters of the underlying Fellegi Sunter model. Splink uses various statistical routines to estimate these parameters. Further details of the underlying theory can be found here , which will help you understand this part of the tutorial. # Begin by reading in the tutorial data again from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd import altair as alt alt . renderers . enable ( \"mimetype\" ) df = pd . read_csv ( \"./data/fake_1000.csv\" )","title":"Specifying and estimating a linkage model"},{"location":"demos/04_Estimating_model_parameters.html#specifying-a-linkage-model","text":"To build a linkage model, the user defines the partial match weights that splink needs to estimate. This is done by defining how the information in the input records should be compared. To be concrete, here is an example comparison: first_name_l first_name_r surname_l surname_r dob_l dob_r city_l city_r email_l email_r Robert Rob Allen Allen 1971-05-24 1971-06-24 nan London roberta25@smith.net roberta25@smith.net What functions should we use to assess the similarity of Rob vs. Robert in the the first_name field? Should similarity in the dob field be computed in the same way, or a different way? Your job as the developer of a linkage model is to decide what comparisons are most appropriate for the types of data you have. Splink can then estimate how much weight to place on a fuzzy match of Rob vs. Robert , relative to an exact match on Robert , or a non-match. Defining these scenarios is done using Comparison s.","title":"Specifying a linkage model"},{"location":"demos/04_Estimating_model_parameters.html#comparisons","text":"The concept of a Comparison has a specific definition within Splink: it defines how data from one or more input columns is compared, using SQL expressions to assess similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Another Comparison may represent the comparison of a person's name or location. A model is composed of many Comparison s, which between them assess the similarity of all of the columns being used for data linking. Each Comparison contains two or more ComparisonLevels which define n discrete gradations of similarity between the input columns within the Comparison. As such ComparisonLevels are nested within Comparisons as follows: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on surname \u2502 \u251c\u2500-- ComparisonLevel: All other \u2502 etc. Our example data would therefore result in the following comparisons, for dob and surname : dob_l dob_r comparison_level interpretation 1971-05-24 1971-05-24 Exact match great match 1971-05-24 1971-06-24 One character difference ok match 1971-05-24 2000-01-02 All other bad match surname_l surname_r comparison_level interpretation Rob Rob Exact match great match Rob Jane All other bad match Rob Robert All other bad match, this comparison has no notion of nicknames More information about comparisons can be found here . We will now use these concepts to build a data linking model.","title":"Comparisons"},{"location":"demos/04_Estimating_model_parameters.html#specifying-the-model-using-comparisons","text":"Splink includes libraries of comparison functions to make it simple to get started: Let's start by looking at a Comparison for first_name : import splink.duckdb.duckdb_comparison_library as cl first_name_comparison = cl . levenshtein_at_thresholds ( \"first_name\" , 2 ) print ( first_name_comparison . human_readable_description ) Comparison 'Exact match vs. levenshtein at threshold 2 vs. anything else' of \"first_name\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL - 'Exact match' with SQL rule: \"first_name_l\" = \"first_name_r\" - 'levenshtein <= 2' with SQL rule: levenshtein(\"first_name_l\", \"first_name_r\") <= 2 - 'All other comparisons' with SQL rule: ELSE","title":"Specifying the model using comparisons"},{"location":"demos/04_Estimating_model_parameters.html#specifying-the-full-settings-dictionary","text":"Comparisons are specified as part of the Splink settings , a Python dictionary which controls all of the configuration of a Splink model: settings = { \"link_type\" : \"dedupe_only\" , \"comparisons\" : [ cl . exact_match ( \"first_name\" ), cl . levenshtein_at_thresholds ( \"surname\" ), cl . levenshtein_at_thresholds ( \"dob\" , 1 ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings ) In words, this setting dictionary says: We are performing a dedupe_only (the other options are link_only , or link_and_dedupe , which may be used if there are multiple input datasets). When comparing records, we will use information from the first_name , surname , dob , city and email columns to compute a match score. The blocking_rules_to_generate_predictions states that we will only check for duplicates amongst records where either the first_name or surname is identical. We have enabled term frequency adjustments for the 'city' column, because some values (e.g. London ) appear much more frequently than others. We have set retain_intermediate_calculation_columns and additional_columns_to_retain to True so that Splink outputs additional information that helps the user understand the calculations. If they were False , the computations would run faster.","title":"Specifying the full settings dictionary"},{"location":"demos/04_Estimating_model_parameters.html#estimate-the-parameters-of-the-model","text":"Now that we have specified our linkage model, we need to estimate the probability_two_random_records_match , u , and m parameters. The probability_two_random_records_match parameter is the probability that two records taken at random from your input data represent a match (typically a very small number). The u values are the proportion of records falling into each ComparisonLevel amongst truly non-matching records. The m values are the proportion of records falling into each ComparisonLevel amongst truly matching records You can read more about the theory of what these mean . We can estimate these parameters using unlabeled data. If we have labels, then we can estimate them even more accurately.","title":"Estimate the parameters of the model"},{"location":"demos/04_Estimating_model_parameters.html#estimation-of-probability_two_random_records_match","text":"In some cases, the probability_two_random_records_match will be known. For example, if you are linking two tables of 10,000 records and expect a one-to-one match, then you should set this value to 1/10_000 in your settings instead of estimating it. More generally, this parameter is unknown and needs to be estimated. It can be estimated accurately enough for most purposes by combining a series of deterministic matching rules and a guess of the recall corresponding to those rules. For further details of the rationale behind this appraoch see here . In this example, I guess that the following deterministic matching rules have a recall of about 70%: deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) Probability two random records match is estimated to be 0.00333. This means that amongst all possible pairwise record comparisons, one in 300.13 are expected to match. With 499,500 total possible comparisons, we expect a total of around 1,664.29 matching pairs","title":"Estimation of probability_two_random_records_match"},{"location":"demos/04_Estimating_model_parameters.html#estimation-of-u-probabilities","text":"Once we have the probability_two_random_records_match parameter, we can estimate the u probabilities. We estimate u using the estimate_u_using_random_sampling method, which doesn't require any labels. It works by sampling random pairs of records, since most of these pairs are going to be non-matches. Over these non-matches we compute the distribution of ComparisonLevel s for each Comparison . For instance, for gender , we would find that the the gender matches 50% of the time, and mismatches 50% of the time. For dob on the other hand, we would find that the dob matches 1% of the time, has a \"one character difference\" 3% of the time, and everything else happens 96% of the time. The larger the random sample, the more accurate the predictions. You control this using the max_pairs parameter. For large datasets, we recommend using at least 10 million - but the higher the better and 1 billion is often appropriate for larger datasets. linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained).","title":"Estimation of u probabilities"},{"location":"demos/04_Estimating_model_parameters.html#estimation-of-m-probabilities","text":"m is the trickiest of the parameters to estimate, because we have to have some idea of what the true matches are. If we have labels, we can directly estimate it. If we do not have labelled data, the m parameters can be estimated using an iterative maximum likelihood approach called Expectation Maximisation.","title":"Estimation of m probabilities"},{"location":"demos/04_Estimating_model_parameters.html#estimating-directly","text":"If we have labels, we can estimate m directly using the estimate_m_from_label_column method of the linker. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. For example (in this tutorial we don't have labels, so we're not actually going to use this): linker . estimate_m_from_label_column ( \"social_security_number\" )","title":"Estimating directly"},{"location":"demos/04_Estimating_model_parameters.html#estimating-with-expectation-maximisation","text":"This algorithm estimates the m values by generating pairwise record comparisons, and using them to maximise a likelihood function. Each estimation pass requires the user to configure an estimation blocking rule to reduce the number of record comparisons generated to a manageable level. In our first estimation pass, we block on first_name and surname , meaning we will generate all record comparisons that have first_name and surname exactly equal. Recall we are trying to estimate the m values of the model, i.e. proportion of records falling into each ComparisonLevel amongst truly matching records. This means that, in this training session, we cannot estimate parameter estimates for the first_name or surname columns, since they will be equal for all the comparisons we do. We can, however, estimate parameter estimates for all of the other columns. The output messages produced by Splink confirm this. training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_fname_sname = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was -0.514 in the m_probability of dob, level `Exact match` Iteration 2: Largest change in params was 0.0474 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0212 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0113 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00694 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00463 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00328 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00243 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00186 in probability_two_random_records_match Iteration 10: Largest change in params was 0.00146 in probability_two_random_records_match Iteration 11: Largest change in params was 0.00117 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000954 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000787 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000658 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000555 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000471 in probability_two_random_records_match Iteration 17: Largest change in params was 0.000404 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000347 in probability_two_random_records_match Iteration 19: Largest change in params was 0.000301 in probability_two_random_records_match Iteration 20: Largest change in params was 0.000261 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000228 in probability_two_random_records_match Iteration 22: Largest change in params was 0.0002 in probability_two_random_records_match Iteration 23: Largest change in params was 0.000175 in probability_two_random_records_match Iteration 24: Largest change in params was 0.000154 in probability_two_random_records_match Iteration 25: Largest change in params was 0.000136 in probability_two_random_records_match EM converged after 25 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). In a second estimation pass, we block on dob. This allows us to estimate parameters for the first_name and surname comparisons. Between the two estimation passes, we now have parameter estimates for all comparisons. from numpy import fix training_blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.392 in the m_probability of surname, level `Exact match` Iteration 2: Largest change in params was 0.137 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0416 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0171 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00853 in probability_two_random_records_match Iteration 6: Largest change in params was 0.0047 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00274 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00165 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00101 in probability_two_random_records_match Iteration 10: Largest change in params was 0.000629 in probability_two_random_records_match Iteration 11: Largest change in params was 0.000394 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000247 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000156 in probability_two_random_records_match Iteration 14: Largest change in params was 9.86e-05 in probability_two_random_records_match EM converged after 14 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values Note that Splink includes other algorithms for estimating m and u values, which are documented here .","title":"Estimating with Expectation Maximisation"},{"location":"demos/04_Estimating_model_parameters.html#visualising-model-parameters","text":"Splink can generate a number of charts to help you understand your model. For an introduction to these charts and how to interpret them, please see this video. The final estimated match weights can be viewed in the match weights chart: linker . match_weights_chart () linker . m_u_parameters_chart ()","title":"Visualising model parameters"},{"location":"demos/04_Estimating_model_parameters.html#saving-the-model","text":"We can save the model, including our estimated parameters, to a .json file, so we can use it in the next tutorial. settings = linker . save_settings_to_json ( \"./demo_settings/saved_model_from_demo.json\" , overwrite = True )","title":"Saving the model"},{"location":"demos/04_Estimating_model_parameters.html#detecting-unlinkable-records","text":"An interesting application of our trained model that is useful to explore before making any predictions is to detect 'unlinkable' records. Unlinkable records are those which do not contain enough information to be linked. A simple example would be a record containing only 'John Smith', and null in all other fields. This record may link to other records, but we'll never know because there's not enough information to disambiguate any potential links. Unlinkable records can be found by linking records to themselves - if, even when matched to themselves, they don't meet the match threshold score, we can be sure they will never link to anything. linker . unlinkables_chart () In the above chart, we can see that about 1.3% of records in the input dataset are unlinkable at a threshold match weight of 6.11 (correponding to a match probability of around 98.6%)","title":"Detecting unlinkable records"},{"location":"demos/04_Estimating_model_parameters.html#next-steps","text":"Now we have trained a model, we can move on to using it predict matching records.","title":"Next steps"},{"location":"demos/04_Estimating_model_parameters.html#further-reading","text":"Full documentation for all of the ways of estimating model parameters can be found here .","title":"Further reading"},{"location":"demos/05_Predicting_results.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Predicting which records match \u00b6 In the previous tutorial, we built and estimated a linkage model. In this tutorial, we will load the estimated model and use it to make predictions of which pairwise record comparisons match. from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd pd . options . display . max_columns = 1000 df = pd . read_csv ( \"./data/fake_1000.csv\" ) Load estimated model from previous tutorial \u00b6 linker = DuckDBLinker ( df ) linker . load_settings_from_json ( \"./demo_settings/saved_model_from_demo.json\" ) Predicting match weights using the trained model \u00b6 We use linker.predict() to run the model. Under the hood this will: Generate all pairwise record comparisons that match at least one of the blocking_rules_to_generate_predictions Use the rules specified in the Comparisons to evaluate the similarity of the input data Use the estimated match weights, applying term frequency adjustments where requested to produce the final match_weight and match_probability scores Optionally, a threshold_match_probability or threshold_match_weight can be provided, which will drop any row where the predicted score is below the threshold. df_predictions = linker . predict ( threshold_match_probability = 0.2 ) df_predictions . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.728972 0.999853 4 5 Grace Grace 1 85.803382 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 1 92.704873 Hull NaN -1 0.001230 NaN 1.000000 1.000000 grace.kelly52@jones.com grace.kelly52@jones.com 3 255.30162 0 1 11.216421 0.999580 26 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 Loodon NaN -1 0.001230 NaN 1.000000 1.000000 gabriel.t54@nnichls.info NaN -1 1.00000 0 2 11.216421 0.999580 28 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 London NaN -1 0.212792 NaN 1.000000 1.000000 gabriel.t54@nichols.info NaN -1 1.00000 0 3 -1.284382 0.291055 37 860 Theodore Theodore 1 85.803382 Morris Marshall 0 0.267994 1978-08-19 1972-07-25 0 0.464198 Birmingham Birmingham 1 0.049200 0.0492 10.264354 1.120874 t.m39@brooks-sawyer.com NaN -1 1.00000 0 4 -1.284382 0.291055 39 860 Theodore Theodore 1 85.803382 Morris Marshall 0 0.267994 1978-08-19 1972-07-25 0 0.464198 Birmingham Birmingham 1 0.049200 0.0492 10.264354 1.120874 t.m39@brooks-sawyer.com NaN -1 1.00000 0 Clustering \u00b6 The result of linker.predict() is a list of pairwise record comparisons and their associated scores. For instance, if we have input records A, B, C and D, it could be represented conceptually as: A -> B with score 0.9 B -> C with score 0.95 C -> D with score 0.1 D -> E with score 0.99 Often, an alternative representation of this result is more useful, where each row is an input record, and where records link, they are assigned to the same cluster. With a score threshold of 0.5, the above data could be represented conceptually as: ID, Cluster ID A, 1 B, 1 C, 1 D, 2 E, 2 The algorithm that converts between the pairwise results and the clusters is called connected components, and it is included in Splink. You can use it as follows: clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predictions , threshold_match_probability = 0.5 ) clusters . as_pandas_dataframe ( limit = 10 ) Completed iteration 1, root rows count 13 Completed iteration 2, root rows count 1 Completed iteration 3, root rows count 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cluster_id unique_id first_name surname dob city email cluster tf_city 0 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 NaN 1 0 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 NaN 2 0 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 0.212792 3 0 3 Robert Alen 1971-06-24 Lonon NaN 0 0.007380 4 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 0.001230 5 4 5 Grace Kelly 1991-04-26 NaN grace.kelly52@jones.com 1 NaN 6 6 6 Logan pMurphy 1973-08-01 NaN NaN 2 NaN 7 7 7 NaN NaN 2015-03-03 Portsmouth evied56@harris-bailey.net 3 0.017220 8 8 8 NaN Dean 2015-03-03 NaN NaN 3 NaN 9 8 9 Evie Dean 2015-03-03 Pootsmruth evihd56@earris-bailey.net 3 0.001230 sql = f \"\"\" select * from { df_predictions . physical_name } limit 2 \"\"\" linker . query_sql ( sql ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.728972 0.999853 4 5 Grace Grace 1 85.803382 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 1 92.704873 Hull NaN -1 0.00123 NaN 1.0 1.0 grace.kelly52@jones.com grace.kelly52@jones.com 3 255.30162 0 1 11.216421 0.999580 26 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 Loodon NaN -1 0.00123 NaN 1.0 1.0 gabriel.t54@nnichls.info NaN -1 1.00000 0","title":"5. Predicting results"},{"location":"demos/05_Predicting_results.html#predicting-which-records-match","text":"In the previous tutorial, we built and estimated a linkage model. In this tutorial, we will load the estimated model and use it to make predictions of which pairwise record comparisons match. from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd pd . options . display . max_columns = 1000 df = pd . read_csv ( \"./data/fake_1000.csv\" )","title":"Predicting which records match"},{"location":"demos/05_Predicting_results.html#load-estimated-model-from-previous-tutorial","text":"linker = DuckDBLinker ( df ) linker . load_settings_from_json ( \"./demo_settings/saved_model_from_demo.json\" )","title":"Load estimated model from previous tutorial"},{"location":"demos/05_Predicting_results.html#predicting-match-weights-using-the-trained-model","text":"We use linker.predict() to run the model. Under the hood this will: Generate all pairwise record comparisons that match at least one of the blocking_rules_to_generate_predictions Use the rules specified in the Comparisons to evaluate the similarity of the input data Use the estimated match weights, applying term frequency adjustments where requested to produce the final match_weight and match_probability scores Optionally, a threshold_match_probability or threshold_match_weight can be provided, which will drop any row where the predicted score is below the threshold. df_predictions = linker . predict ( threshold_match_probability = 0.2 ) df_predictions . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.728972 0.999853 4 5 Grace Grace 1 85.803382 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 1 92.704873 Hull NaN -1 0.001230 NaN 1.000000 1.000000 grace.kelly52@jones.com grace.kelly52@jones.com 3 255.30162 0 1 11.216421 0.999580 26 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 Loodon NaN -1 0.001230 NaN 1.000000 1.000000 gabriel.t54@nnichls.info NaN -1 1.00000 0 2 11.216421 0.999580 28 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 London NaN -1 0.212792 NaN 1.000000 1.000000 gabriel.t54@nichols.info NaN -1 1.00000 0 3 -1.284382 0.291055 37 860 Theodore Theodore 1 85.803382 Morris Marshall 0 0.267994 1978-08-19 1972-07-25 0 0.464198 Birmingham Birmingham 1 0.049200 0.0492 10.264354 1.120874 t.m39@brooks-sawyer.com NaN -1 1.00000 0 4 -1.284382 0.291055 39 860 Theodore Theodore 1 85.803382 Morris Marshall 0 0.267994 1978-08-19 1972-07-25 0 0.464198 Birmingham Birmingham 1 0.049200 0.0492 10.264354 1.120874 t.m39@brooks-sawyer.com NaN -1 1.00000 0","title":"Predicting match weights using the trained model"},{"location":"demos/05_Predicting_results.html#clustering","text":"The result of linker.predict() is a list of pairwise record comparisons and their associated scores. For instance, if we have input records A, B, C and D, it could be represented conceptually as: A -> B with score 0.9 B -> C with score 0.95 C -> D with score 0.1 D -> E with score 0.99 Often, an alternative representation of this result is more useful, where each row is an input record, and where records link, they are assigned to the same cluster. With a score threshold of 0.5, the above data could be represented conceptually as: ID, Cluster ID A, 1 B, 1 C, 1 D, 2 E, 2 The algorithm that converts between the pairwise results and the clusters is called connected components, and it is included in Splink. You can use it as follows: clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predictions , threshold_match_probability = 0.5 ) clusters . as_pandas_dataframe ( limit = 10 ) Completed iteration 1, root rows count 13 Completed iteration 2, root rows count 1 Completed iteration 3, root rows count 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cluster_id unique_id first_name surname dob city email cluster tf_city 0 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 NaN 1 0 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 NaN 2 0 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 0.212792 3 0 3 Robert Alen 1971-06-24 Lonon NaN 0 0.007380 4 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 0.001230 5 4 5 Grace Kelly 1991-04-26 NaN grace.kelly52@jones.com 1 NaN 6 6 6 Logan pMurphy 1973-08-01 NaN NaN 2 NaN 7 7 7 NaN NaN 2015-03-03 Portsmouth evied56@harris-bailey.net 3 0.017220 8 8 8 NaN Dean 2015-03-03 NaN NaN 3 NaN 9 8 9 Evie Dean 2015-03-03 Pootsmruth evihd56@earris-bailey.net 3 0.001230 sql = f \"\"\" select * from { df_predictions . physical_name } limit 2 \"\"\" linker . query_sql ( sql ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.728972 0.999853 4 5 Grace Grace 1 85.803382 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 1 92.704873 Hull NaN -1 0.00123 NaN 1.0 1.0 grace.kelly52@jones.com grace.kelly52@jones.com 3 255.30162 0 1 11.216421 0.999580 26 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 Loodon NaN -1 0.00123 NaN 1.0 1.0 gabriel.t54@nnichls.info NaN -1 1.00000 0","title":"Clustering"},{"location":"demos/06_Visualising_predictions.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Visualising predictions \u00b6 Splink contains a variety of tools to help you visualise your predictions. The idea is that, by developing an understanding of how your model works, you can gain confidence that the predictions it makes are sensible, or alternatively find examples of where your model isn't working, which may help you improve the model specification and fix these problems. # Rerun our predictions to we're ready to view the charts from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df ) linker . load_settings_from_json ( \"./demo_settings/saved_model_from_demo.json\" ) df_predictions = linker . predict ( threshold_match_probability = 0.2 ) Waterfall chart \u00b6 The waterfall chart provides a means of visualising individual predictions to understand how Splink computed the final matchweight for a particular pairwise record comparison. To plot a waterfall chart, the user chooses one or more records from the results of linker.predict() , and provides these records to the linker.waterfall_chart() function. For an introduction to waterfall charts and how to interpret them, please see this video. records_to_view = df_predictions . as_record_dict ( limit = 5 ) linker . waterfall_chart ( records_to_view , filter_nulls = False ) Comparison viewer dashboard \u00b6 The comparison viewer dashboard takes this one step further by producing an interactive dashboard that contains example predictions from across the spectrum of match scores. An in-depth video describing how to interpret the dashboard can be found here . linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , overwrite = True ) # You can view the scv.html file in your browser, or inline in a notbook as follows from IPython.display import IFrame IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Cluster studio dashboard \u00b6 Cluster studio is an interactive dashboards that visualises the results of clustering your predictions. It provides examples of clusters of different sizes. The shape and size of clusters can be indicative of problems with record linkage, so it provides a tool to help you find potential false positive and negative links. df_clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predictions , threshold_match_probability = 0.5 ) linker . cluster_studio_dashboard ( df_predictions , df_clusters , \"cluster_studio.html\" , sampling_method = \"by_cluster_size\" , overwrite = True ) # You can view the scv.html file in your browser, or inline in a notbook as follows from IPython.display import IFrame IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Completed iteration 1, root rows count 13 Completed iteration 2, root rows count 1 Completed iteration 3, root rows count 0","title":"6. Visualising predictions"},{"location":"demos/06_Visualising_predictions.html#visualising-predictions","text":"Splink contains a variety of tools to help you visualise your predictions. The idea is that, by developing an understanding of how your model works, you can gain confidence that the predictions it makes are sensible, or alternatively find examples of where your model isn't working, which may help you improve the model specification and fix these problems. # Rerun our predictions to we're ready to view the charts from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df ) linker . load_settings_from_json ( \"./demo_settings/saved_model_from_demo.json\" ) df_predictions = linker . predict ( threshold_match_probability = 0.2 )","title":"Visualising predictions"},{"location":"demos/06_Visualising_predictions.html#waterfall-chart","text":"The waterfall chart provides a means of visualising individual predictions to understand how Splink computed the final matchweight for a particular pairwise record comparison. To plot a waterfall chart, the user chooses one or more records from the results of linker.predict() , and provides these records to the linker.waterfall_chart() function. For an introduction to waterfall charts and how to interpret them, please see this video. records_to_view = df_predictions . as_record_dict ( limit = 5 ) linker . waterfall_chart ( records_to_view , filter_nulls = False )","title":"Waterfall chart"},{"location":"demos/06_Visualising_predictions.html#comparison-viewer-dashboard","text":"The comparison viewer dashboard takes this one step further by producing an interactive dashboard that contains example predictions from across the spectrum of match scores. An in-depth video describing how to interpret the dashboard can be found here . linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , overwrite = True ) # You can view the scv.html file in your browser, or inline in a notbook as follows from IPython.display import IFrame IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 )","title":"Comparison viewer dashboard"},{"location":"demos/06_Visualising_predictions.html#cluster-studio-dashboard","text":"Cluster studio is an interactive dashboards that visualises the results of clustering your predictions. It provides examples of clusters of different sizes. The shape and size of clusters can be indicative of problems with record linkage, so it provides a tool to help you find potential false positive and negative links. df_clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predictions , threshold_match_probability = 0.5 ) linker . cluster_studio_dashboard ( df_predictions , df_clusters , \"cluster_studio.html\" , sampling_method = \"by_cluster_size\" , overwrite = True ) # You can view the scv.html file in your browser, or inline in a notbook as follows from IPython.display import IFrame IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Completed iteration 1, root rows count 13 Completed iteration 2, root rows count 1 Completed iteration 3, root rows count 0","title":"Cluster studio dashboard"},{"location":"demos/07_Quality_assurance.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Quality assurance of prediction results \u00b6 In the previous tutorial, we looked at various ways to visualise the results of our model. These are useful for quality assurance purposes because they allow us to understand how our model works and verify that it is doing something sensible. They can also be useful to identify examples where the model is not performing as expected. In addition to these spot checks, Splink also has functions to perform more formal accuracy analysis. These functions allow you to understand the likely prevalence of false positives and false negatives in your linkage models. They rely on the existence of a sample of labelled (ground truth) matches, which may have been produced (for example) by human beings. For the accuracy analysis to be unbiased, the sample should be representative of the overall dataset. # Rerun our predictions to we're ready to view the charts from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df ) linker . load_settings_from_json ( \"./demo_settings/saved_model_from_demo.json\" ) df_predictions = linker . predict ( threshold_match_probability = 0.2 ) Load in labels \u00b6 The labels file contains a list of pairwise comparisons which represent matches and non-matches. The required format of the labels file is described here . df_labels = pd . read_csv ( \"./data/fake_1000_labels.csv\" ) df_labels . head ( 5 ) labels_table = linker . register_labels_table ( df_labels ) Receiver operating characteristic curve \u00b6 A ROC chart shows how the number of false positives and false negatives varies depending on the match threshold chosen. The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches. linker . roc_chart_from_labels_table ( labels_table ) Precision-recall chart \u00b6 An alternative representation of truth space is called a precision recall curve . This can be plotted as follows: linker . precision_recall_chart_from_labels_table ( labels_table ) Truth table \u00b6 Finally, Splink can also report the underlying table used to construct the ROC and precision recall curves. roc_table = linker . truth_space_table_from_labels_table ( labels_table ) roc_table . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } truth_threshold match_probability row_count P N TP TN FP FN P_rate N_rate TP_rate TN_rate FP_rate FN_rate precision recall F1 0 -16.180925 0.000013 1225.0 80.0 1145.0 80.0 0.0 1145.0 0.0 0.0 0.934694 1.0 0.000000 1.000000 0.0 0.065306 1.0 0.122699 1 -15.197628 0.000027 1225.0 80.0 1145.0 80.0 106.0 1039.0 0.0 0.0 0.934694 1.0 0.092576 0.907424 0.0 0.071492 1.0 0.133556 2 -15.058351 0.000029 1225.0 80.0 1145.0 80.0 194.0 951.0 0.0 0.0 0.934694 1.0 0.169432 0.830568 0.0 0.077595 1.0 0.144144 3 -14.281196 0.000050 1225.0 80.0 1145.0 80.0 373.0 772.0 0.0 0.0 0.934694 1.0 0.325764 0.674236 0.0 0.093897 1.0 0.171674 4 -14.075054 0.000058 1225.0 80.0 1145.0 80.0 416.0 729.0 0.0 0.0 0.934694 1.0 0.363319 0.636681 0.0 0.098888 1.0 0.180180","title":"7. Quality assurance"},{"location":"demos/07_Quality_assurance.html#quality-assurance-of-prediction-results","text":"In the previous tutorial, we looked at various ways to visualise the results of our model. These are useful for quality assurance purposes because they allow us to understand how our model works and verify that it is doing something sensible. They can also be useful to identify examples where the model is not performing as expected. In addition to these spot checks, Splink also has functions to perform more formal accuracy analysis. These functions allow you to understand the likely prevalence of false positives and false negatives in your linkage models. They rely on the existence of a sample of labelled (ground truth) matches, which may have been produced (for example) by human beings. For the accuracy analysis to be unbiased, the sample should be representative of the overall dataset. # Rerun our predictions to we're ready to view the charts from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df ) linker . load_settings_from_json ( \"./demo_settings/saved_model_from_demo.json\" ) df_predictions = linker . predict ( threshold_match_probability = 0.2 )","title":"Quality assurance of prediction results"},{"location":"demos/07_Quality_assurance.html#load-in-labels","text":"The labels file contains a list of pairwise comparisons which represent matches and non-matches. The required format of the labels file is described here . df_labels = pd . read_csv ( \"./data/fake_1000_labels.csv\" ) df_labels . head ( 5 ) labels_table = linker . register_labels_table ( df_labels )","title":"Load in labels"},{"location":"demos/07_Quality_assurance.html#receiver-operating-characteristic-curve","text":"A ROC chart shows how the number of false positives and false negatives varies depending on the match threshold chosen. The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches. linker . roc_chart_from_labels_table ( labels_table )","title":"Receiver operating characteristic curve"},{"location":"demos/07_Quality_assurance.html#precision-recall-chart","text":"An alternative representation of truth space is called a precision recall curve . This can be plotted as follows: linker . precision_recall_chart_from_labels_table ( labels_table )","title":"Precision-recall chart"},{"location":"demos/07_Quality_assurance.html#truth-table","text":"Finally, Splink can also report the underlying table used to construct the ROC and precision recall curves. roc_table = linker . truth_space_table_from_labels_table ( labels_table ) roc_table . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } truth_threshold match_probability row_count P N TP TN FP FN P_rate N_rate TP_rate TN_rate FP_rate FN_rate precision recall F1 0 -16.180925 0.000013 1225.0 80.0 1145.0 80.0 0.0 1145.0 0.0 0.0 0.934694 1.0 0.000000 1.000000 0.0 0.065306 1.0 0.122699 1 -15.197628 0.000027 1225.0 80.0 1145.0 80.0 106.0 1039.0 0.0 0.0 0.934694 1.0 0.092576 0.907424 0.0 0.071492 1.0 0.133556 2 -15.058351 0.000029 1225.0 80.0 1145.0 80.0 194.0 951.0 0.0 0.0 0.934694 1.0 0.169432 0.830568 0.0 0.077595 1.0 0.144144 3 -14.281196 0.000050 1225.0 80.0 1145.0 80.0 373.0 772.0 0.0 0.0 0.934694 1.0 0.325764 0.674236 0.0 0.093897 1.0 0.171674 4 -14.075054 0.000058 1225.0 80.0 1145.0 80.0 416.0 729.0 0.0 0.0 0.934694 1.0 0.363319 0.636681 0.0 0.098888 1.0 0.180180","title":"Truth table"},{"location":"demos/example_accuracy_analysis_from_labels_column.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Quality assurance when you have fully labelled data \u00b6 In this example, our data contains a fully-populated ground-truth column called cluster that enables us to perform accuracy analysis of the final model import pandas as pd import altair as alt alt . renderers . enable ( \"mimetype\" ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . levenshtein_at_thresholds ( \"surname\" , 2 ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) session_dob = linker . estimate_parameters_using_expectation_maximisation ( \"l.dob = r.dob\" ) session_email = linker . estimate_parameters_using_expectation_maximisation ( \"l.email = r.email\" ) linker . truth_space_table_from_labels_column ( \"cluster\" , match_weight_round_to_nearest = 0.1 ) . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } truth_threshold row_count P N TP TN FP FN P_rate N_rate TP_rate TN_rate FP_rate FN_rate precision recall 0 -18.4 4353.0 2031.0 2322.0 2031.0 0.0 2322.0 0.0 0.0 0.533425 1.000000 0.0 1.0 0.000000 0.466575 1.000000 1 -17.2 4353.0 2031.0 2322.0 2029.0 0.0 2322.0 2.0 0.0 0.533425 0.999015 0.0 1.0 0.000985 0.466330 0.999015 2 -16.2 4353.0 2031.0 2322.0 2026.0 0.0 2322.0 5.0 0.0 0.533425 0.997538 0.0 1.0 0.002462 0.465961 0.997538 3 -15.4 4353.0 2031.0 2322.0 2024.0 0.0 2322.0 7.0 0.0 0.533425 0.996553 0.0 1.0 0.003447 0.465716 0.996553 4 -14.2 4353.0 2031.0 2322.0 2019.0 0.0 2322.0 12.0 0.0 0.533425 0.994092 0.0 1.0 0.005908 0.465100 0.994092 linker . roc_chart_from_labels_column ( \"cluster\" ) linker . precision_recall_chart_from_labels_column ( \"cluster\" ) # Plot some false positives linker . prediction_errors_from_labels_column ( \"cluster\" , include_false_negatives = True , include_false_positives = True ) . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } clerical_match_score found_by_blocking_rules match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email cluster_l cluster_r match_key 0 0.0 True 0.205044 0.535472 486 619 Hannah NaN -1 1.000000 ... 0.04920 10.852673 1.120874 NaN od@holloway.info -1 1.000000 122 157 1 1 1.0 True -3.399559 0.086559 617 620 NaN Olivia -1 1.000000 ... 0.04920 1.000000 1.000000 NaN NaN -1 1.000000 157 157 1 2 1.0 True -3.399559 0.086559 618 620 NaN Olivia -1 1.000000 ... 0.04920 1.000000 1.000000 od@holloway.info NaN -1 1.000000 157 157 1 3 1.0 True -1.472365 0.264917 660 661 Charlie Cahlrae 0 0.218912 ... 0.00123 0.424936 1.000000 NaN charlieh@sandoval-sanders.info -1 1.000000 168 168 1 4 1.0 True -2.268212 0.171902 505 508 NaN NaN -1 1.000000 ... 0.00246 0.424936 1.000000 f.s@jharp.com f.j@shrarp.com 0 0.126095 126 126 1 5 rows \u00d7 32 columns records = linker . prediction_errors_from_labels_column ( \"cluster\" , include_false_negatives = True , include_false_positives = True ) . as_record_dict ( limit = 5 ) linker . waterfall_chart ( records )","title":"QA from ground truth column"},{"location":"demos/example_accuracy_analysis_from_labels_column.html#quality-assurance-when-you-have-fully-labelled-data","text":"In this example, our data contains a fully-populated ground-truth column called cluster that enables us to perform accuracy analysis of the final model import pandas as pd import altair as alt alt . renderers . enable ( \"mimetype\" ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . levenshtein_at_thresholds ( \"surname\" , 2 ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) session_dob = linker . estimate_parameters_using_expectation_maximisation ( \"l.dob = r.dob\" ) session_email = linker . estimate_parameters_using_expectation_maximisation ( \"l.email = r.email\" ) linker . truth_space_table_from_labels_column ( \"cluster\" , match_weight_round_to_nearest = 0.1 ) . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } truth_threshold row_count P N TP TN FP FN P_rate N_rate TP_rate TN_rate FP_rate FN_rate precision recall 0 -18.4 4353.0 2031.0 2322.0 2031.0 0.0 2322.0 0.0 0.0 0.533425 1.000000 0.0 1.0 0.000000 0.466575 1.000000 1 -17.2 4353.0 2031.0 2322.0 2029.0 0.0 2322.0 2.0 0.0 0.533425 0.999015 0.0 1.0 0.000985 0.466330 0.999015 2 -16.2 4353.0 2031.0 2322.0 2026.0 0.0 2322.0 5.0 0.0 0.533425 0.997538 0.0 1.0 0.002462 0.465961 0.997538 3 -15.4 4353.0 2031.0 2322.0 2024.0 0.0 2322.0 7.0 0.0 0.533425 0.996553 0.0 1.0 0.003447 0.465716 0.996553 4 -14.2 4353.0 2031.0 2322.0 2019.0 0.0 2322.0 12.0 0.0 0.533425 0.994092 0.0 1.0 0.005908 0.465100 0.994092 linker . roc_chart_from_labels_column ( \"cluster\" ) linker . precision_recall_chart_from_labels_column ( \"cluster\" ) # Plot some false positives linker . prediction_errors_from_labels_column ( \"cluster\" , include_false_negatives = True , include_false_positives = True ) . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } clerical_match_score found_by_blocking_rules match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email cluster_l cluster_r match_key 0 0.0 True 0.205044 0.535472 486 619 Hannah NaN -1 1.000000 ... 0.04920 10.852673 1.120874 NaN od@holloway.info -1 1.000000 122 157 1 1 1.0 True -3.399559 0.086559 617 620 NaN Olivia -1 1.000000 ... 0.04920 1.000000 1.000000 NaN NaN -1 1.000000 157 157 1 2 1.0 True -3.399559 0.086559 618 620 NaN Olivia -1 1.000000 ... 0.04920 1.000000 1.000000 od@holloway.info NaN -1 1.000000 157 157 1 3 1.0 True -1.472365 0.264917 660 661 Charlie Cahlrae 0 0.218912 ... 0.00123 0.424936 1.000000 NaN charlieh@sandoval-sanders.info -1 1.000000 168 168 1 4 1.0 True -2.268212 0.171902 505 508 NaN NaN -1 1.000000 ... 0.00246 0.424936 1.000000 f.s@jharp.com f.j@shrarp.com 0 0.126095 126 126 1 5 rows \u00d7 32 columns records = linker . prediction_errors_from_labels_column ( \"cluster\" , include_false_negatives = True , include_false_positives = True ) . as_record_dict ( limit = 5 ) linker . waterfall_chart ( records )","title":"Quality assurance when you have fully labelled data"},{"location":"demos/example_deduplicate_50k_synthetic.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linking a dataset of real historical persons \u00b6 In this example, we deduplicate a more realistic dataset. The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors introduced. from splink.duckdb.duckdb_linker import DuckDBLinker import altair as alt alt . renderers . enable ( 'mimetype' ) import pandas as pd pd . options . display . max_rows = 1000 df = pd . read_parquet ( \"./data/historical_figures_with_errors_50k.parquet\" ) # Simple settings dictionary will be used for exploratory analysis settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.surname = r.surname and l.dob = r.dob\" , \"l.first_name = r.first_name and l.dob = r.dob\" , \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\" , ], } linker = DuckDBLinker ( df , settings ) linker . profile_columns ( [ \"first_name\" , \"postcode_fake\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) linker . cumulative_num_comparisons_from_blocking_rules_chart () import splink.duckdb.duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.surname = r.surname and l.dob = r.dob\" , \"l.first_name = r.first_name and l.dob = r.dob\" , \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\" , ], \"comparisons\" : [ cl . jaro_winkler_at_thresholds ( \"first_name\" , [ 0.9 , 0.7 ], term_frequency_adjustments = True ), cl . jaro_winkler_at_thresholds ( \"surname\" , [ 0.9 , 0.7 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"postcode_fake\" , 2 , term_frequency_adjustments = True ), cl . exact_match ( \"birth_place\" , term_frequency_adjustments = True ), cl . exact_match ( \"occupation\" , term_frequency_adjustments = True ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"max_iterations\" : 10 , \"em_convergence\" : 0.01 } linker = DuckDBLinker ( df , settings , connection = \":temporary:\" ) linker . estimate_probability_two_random_records_match ( [ \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\" , \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\" , \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\" , ], recall = 0.6 , ) Probability two random records match is estimated to be 0.000136. This means that amongst all possible pairwise record comparisons, one in 7,362.31 are expected to match. With 1,279,041,753 total possible comparisons, we expect a total of around 173,728.33 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 5e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - postcode_fake (no m values are trained). - birth_place (no m values are trained). - occupation (no m values are trained). blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_names = linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - postcode_fake - birth_place - occupation Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was -0.524 in probability_two_random_records_match Iteration 2: Largest change in params was -0.0339 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0119 in the m_probability of birth_place, level `Exact match` Iteration 4: Largest change in params was 0.00374 in the m_probability of birth_place, level `Exact match` EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - postcode_fake - birth_place - occupation Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.362 in the m_probability of first_name, level `Exact match` Iteration 2: Largest change in params was -0.0302 in the m_probability of first_name, level `Exact match` Iteration 3: Largest change in params was 0.00453 in the m_probability of first_name, level `All other comparisons` EM converged after 3 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values The final match weights can be viewed in the match weights chart: linker . match_weights_chart () linker . unlinkables_chart () df_predict = linker . predict () df_e = df_predict . as_pandas_dataframe ( limit = 5 ) df_e .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 18.131419 0.999997 Q2296770-1 Q2296770-14 thomas thomas 3 0.028667 0.028667 44.380695 ... 1.000000 1.000000 politician politician 1 0.088932 0.088932 22.786409 0.443341 0 1 40.311237 1.000000 Q1443188-1 Q1443188-3 frank frank 3 0.006335 0.006335 44.380695 ... 0.154553 1.000000 liturgist liturgist 1 0.000237 0.000237 22.786409 166.178966 0 2 40.311237 1.000000 Q1443188-2 Q1443188-3 frank frank 3 0.006335 0.006335 44.380695 ... 0.154553 1.000000 liturgist liturgist 1 0.000237 0.000237 22.786409 166.178966 0 3 41.258395 1.000000 Q90404618-1 Q90404618-3 emlie emlie 3 0.000099 0.000099 44.380695 ... 178.565257 4.701184 playwright playwright 1 0.002728 0.002728 22.786409 14.450345 0 4 41.258395 1.000000 Q90404618-2 Q90404618-3 emlie emlie 3 0.000099 0.000099 44.380695 ... 178.565257 4.701184 playwright playwright 1 0.002728 0.002728 22.786409 14.450345 0 5 rows \u00d7 47 columns You can also view rows in this dataset as a waterfall chart as follows: from splink.charts import waterfall_chart records_to_plot = df_e . to_dict ( orient = \"records\" ) linker . waterfall_chart ( records_to_plot , filter_nulls = False ) clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predict , threshold_match_probability = 0.95 ) Completed iteration 1, root rows count 638 Completed iteration 2, root rows count 122 Completed iteration 3, root rows count 35 Completed iteration 4, root rows count 6 Completed iteration 5, root rows count 0 linker . cluster_studio_dashboard ( df_predict , clusters , \"50k_cluster.html\" , sampling_method = 'by_cluster_size' , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./50k_cluster.html\" , width = \"100%\" , height = 1200 ) linker . roc_chart_from_labels_column ( \"cluster\" , match_weight_round_to_nearest = 0.02 ) records = linker . prediction_errors_from_labels_column ( \"cluster\" , threshold = 0.999 , include_false_negatives = False , include_false_positives = True , ) . as_record_dict () linker . waterfall_chart ( records ) # Some of the false negatives will be because they weren't detected by the blocking rules records = linker . prediction_errors_from_labels_column ( \"cluster\" , threshold = 0.5 , include_false_negatives = True , include_false_positives = False , ) . as_record_dict ( limit = 50 ) linker . waterfall_chart ( records )","title":"Deduplicate 50k rows historical persons"},{"location":"demos/example_deduplicate_50k_synthetic.html#linking-a-dataset-of-real-historical-persons","text":"In this example, we deduplicate a more realistic dataset. The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors introduced. from splink.duckdb.duckdb_linker import DuckDBLinker import altair as alt alt . renderers . enable ( 'mimetype' ) import pandas as pd pd . options . display . max_rows = 1000 df = pd . read_parquet ( \"./data/historical_figures_with_errors_50k.parquet\" ) # Simple settings dictionary will be used for exploratory analysis settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.surname = r.surname and l.dob = r.dob\" , \"l.first_name = r.first_name and l.dob = r.dob\" , \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\" , ], } linker = DuckDBLinker ( df , settings ) linker . profile_columns ( [ \"first_name\" , \"postcode_fake\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) linker . cumulative_num_comparisons_from_blocking_rules_chart () import splink.duckdb.duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.surname = r.surname and l.dob = r.dob\" , \"l.first_name = r.first_name and l.dob = r.dob\" , \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\" , ], \"comparisons\" : [ cl . jaro_winkler_at_thresholds ( \"first_name\" , [ 0.9 , 0.7 ], term_frequency_adjustments = True ), cl . jaro_winkler_at_thresholds ( \"surname\" , [ 0.9 , 0.7 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"postcode_fake\" , 2 , term_frequency_adjustments = True ), cl . exact_match ( \"birth_place\" , term_frequency_adjustments = True ), cl . exact_match ( \"occupation\" , term_frequency_adjustments = True ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"max_iterations\" : 10 , \"em_convergence\" : 0.01 } linker = DuckDBLinker ( df , settings , connection = \":temporary:\" ) linker . estimate_probability_two_random_records_match ( [ \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\" , \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\" , \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\" , ], recall = 0.6 , ) Probability two random records match is estimated to be 0.000136. This means that amongst all possible pairwise record comparisons, one in 7,362.31 are expected to match. With 1,279,041,753 total possible comparisons, we expect a total of around 173,728.33 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 5e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - postcode_fake (no m values are trained). - birth_place (no m values are trained). - occupation (no m values are trained). blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_names = linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - postcode_fake - birth_place - occupation Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was -0.524 in probability_two_random_records_match Iteration 2: Largest change in params was -0.0339 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0119 in the m_probability of birth_place, level `Exact match` Iteration 4: Largest change in params was 0.00374 in the m_probability of birth_place, level `Exact match` EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - postcode_fake - birth_place - occupation Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.362 in the m_probability of first_name, level `Exact match` Iteration 2: Largest change in params was -0.0302 in the m_probability of first_name, level `Exact match` Iteration 3: Largest change in params was 0.00453 in the m_probability of first_name, level `All other comparisons` EM converged after 3 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values The final match weights can be viewed in the match weights chart: linker . match_weights_chart () linker . unlinkables_chart () df_predict = linker . predict () df_e = df_predict . as_pandas_dataframe ( limit = 5 ) df_e .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 18.131419 0.999997 Q2296770-1 Q2296770-14 thomas thomas 3 0.028667 0.028667 44.380695 ... 1.000000 1.000000 politician politician 1 0.088932 0.088932 22.786409 0.443341 0 1 40.311237 1.000000 Q1443188-1 Q1443188-3 frank frank 3 0.006335 0.006335 44.380695 ... 0.154553 1.000000 liturgist liturgist 1 0.000237 0.000237 22.786409 166.178966 0 2 40.311237 1.000000 Q1443188-2 Q1443188-3 frank frank 3 0.006335 0.006335 44.380695 ... 0.154553 1.000000 liturgist liturgist 1 0.000237 0.000237 22.786409 166.178966 0 3 41.258395 1.000000 Q90404618-1 Q90404618-3 emlie emlie 3 0.000099 0.000099 44.380695 ... 178.565257 4.701184 playwright playwright 1 0.002728 0.002728 22.786409 14.450345 0 4 41.258395 1.000000 Q90404618-2 Q90404618-3 emlie emlie 3 0.000099 0.000099 44.380695 ... 178.565257 4.701184 playwright playwright 1 0.002728 0.002728 22.786409 14.450345 0 5 rows \u00d7 47 columns You can also view rows in this dataset as a waterfall chart as follows: from splink.charts import waterfall_chart records_to_plot = df_e . to_dict ( orient = \"records\" ) linker . waterfall_chart ( records_to_plot , filter_nulls = False ) clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predict , threshold_match_probability = 0.95 ) Completed iteration 1, root rows count 638 Completed iteration 2, root rows count 122 Completed iteration 3, root rows count 35 Completed iteration 4, root rows count 6 Completed iteration 5, root rows count 0 linker . cluster_studio_dashboard ( df_predict , clusters , \"50k_cluster.html\" , sampling_method = 'by_cluster_size' , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./50k_cluster.html\" , width = \"100%\" , height = 1200 ) linker . roc_chart_from_labels_column ( \"cluster\" , match_weight_round_to_nearest = 0.02 ) records = linker . prediction_errors_from_labels_column ( \"cluster\" , threshold = 0.999 , include_false_negatives = False , include_false_positives = True , ) . as_record_dict () linker . waterfall_chart ( records ) # Some of the false negatives will be because they weren't detected by the blocking rules records = linker . prediction_errors_from_labels_column ( \"cluster\" , threshold = 0.5 , include_false_negatives = True , include_false_positives = False , ) . as_record_dict ( limit = 50 ) linker . waterfall_chart ( records )","title":"Linking a dataset of real historical persons"},{"location":"demos/example_febrl3.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Deduplicating the febrl3 dataset \u00b6 See A.2 here and here for the source of this data import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/febrl/dataset3.csv\" , delimiter = \", \" , dtype = { \"date_of_birth\" : str }, engine = \"python\" ) df [ \"cluster\" ] = df [ \"rec_id\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) df . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1496-org mitchell green 7.0 wallaby place delmar cleveland 2119 sa 19560409 1804974 rec-1496 1 rec-552-dup-3 harley mccarthy 177.0 pridhamstreet milton marsden 3165 nsw 19080419 6089216 rec-552 from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"unique_id_column_name\" : \"rec_id\" , \"link_type\" : \"dedupe_only\" , } linker = DuckDBLinker ( df , settings ) linker . missingness_chart () linker . profile_columns ( list ( df . columns )) potential_blocking_rules = [ \"l.soc_sec_id = r.soc_sec_id\" , \"l.given_name = r.given_name\" , \"l.surname = r.surname\" , \"l.date_of_birth = r.date_of_birth\" , \"l.postcode = r.postcode\" ] linker . cumulative_num_comparisons_from_blocking_rules_chart ( potential_blocking_rules ) from splink.duckdb.duckdb_linker import DuckDBLinker import splink.duckdb.duckdb_comparison_level_library as cll import splink.duckdb.duckdb_comparison_library as cl settings = { \"unique_id_column_name\" : \"rec_id\" , \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : potential_blocking_rules , \"comparisons\" : [ { \"output_column_name\" : \"Given name\" , \"comparison_levels\" : [ cll . null_level ( \"given_name\" ), cll . exact_match_level ( \"given_name\" , term_frequency_adjustments = True ), { \"sql_condition\" : '\"given_name_l\" = \"surname_r\"' , \"label_for_charts\" : \"Exact match on reversed cols, l to r\" , }, cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.7 ), { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, surname_r) > 0.7\" , \"label_for_charts\" : \"Jar on reversed cols, l to r\" , }, cll . else_level (), ], }, { \"output_column_name\" : \"Surname\" , \"comparison_levels\" : [ cll . null_level ( \"surname\" ), cll . exact_match_level ( \"surname\" , term_frequency_adjustments = True ), { \"sql_condition\" : '\"given_name_r\" = \"surname_l\"' , \"label_for_charts\" : \"Exact match on reversed cols, r to l\" , }, cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.7 ), { \"sql_condition\" : \"jaro_winkler_similarity(given_name_r, surname_l) > 0.7\" , \"label_for_charts\" : \"Jaro on reversed cols, l to r\" , }, cll . else_level (), ], }, cl . levenshtein_at_thresholds ( \"date_of_birth\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"soc_sec_id\" , [ 2 ]), cl . exact_match ( \"street_number\" , term_frequency_adjustments = True ), cl . exact_match ( \"postcode\" , term_frequency_adjustments = True ), ], \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings ) deterministic_rules = [ \"l.soc_sec_id = r.soc_sec_id\" , \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\" , \"l.given_name = r.surname and l.surname = r.given_name and l.date_of_birth = r.date_of_birth\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.9 ) Probability two random records match is estimated to be 0.000527. This means that amongst all possible pairwise record comparisons, one in 1,899.32 are expected to match. With 12,497,500 total possible comparisons, we expect a total of around 6,580.00 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - Given name (no m values are trained). - Surname (no m values are trained). - date_of_birth (no m values are trained). - soc_sec_id (no m values are trained). - street_number (no m values are trained). - postcode (no m values are trained). comparison = linker . _settings_obj . comparisons [ 2 ] . as_dict () session_dob = linker . estimate_parameters_using_expectation_maximisation ( \"substr(l.date_of_birth,1,3) = substr(r.date_of_birth,1,3)\" ) session_postcode = linker . estimate_parameters_using_expectation_maximisation ( \"substr(l.postcode,1,2) = substr(r.postcode,1,2)\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: substr(l.date_of_birth,1,3) = substr(r.date_of_birth,1,3) Parameter estimates will be made for the following comparison(s): - Given name - Surname - soc_sec_id - street_number - postcode Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - date_of_birth Iteration 1: Largest change in params was -0.493 in probability_two_random_records_match Iteration 2: Largest change in params was -0.00611 in the m_probability of soc_sec_id, level `All other comparisons` Iteration 3: Largest change in params was -0.000544 in the m_probability of soc_sec_id, level `All other comparisons` Iteration 4: Largest change in params was -6.13e-05 in the m_probability of soc_sec_id, level `All other comparisons` EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - date_of_birth (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: substr(l.postcode,1,2) = substr(r.postcode,1,2) Parameter estimates will be made for the following comparison(s): - Given name - Surname - date_of_birth - soc_sec_id - street_number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - postcode Iteration 1: Largest change in params was -0.234 in probability_two_random_records_match Iteration 2: Largest change in params was -0.0152 in the m_probability of soc_sec_id, level `All other comparisons` Iteration 3: Largest change in params was -0.000974 in the m_probability of soc_sec_id, level `All other comparisons` linker . match_weights_chart () results = linker . predict ( threshold_match_probability = 0.2 ) linker . roc_chart_from_labels_column ( \"cluster\" ) pred_errors_df = linker . prediction_errors_from_labels_column ( \"cluster\" ) . as_pandas_dataframe () len ( pred_errors_df ) pred_errors_df . head () records = linker . prediction_errors_from_labels_column ( \"cluster\" ) . as_record_dict ( limit = 10 ) linker . waterfall_chart ( records )","title":"Febrl3 Dedupe"},{"location":"demos/example_febrl3.html#deduplicating-the-febrl3-dataset","text":"See A.2 here and here for the source of this data import pandas as pd import altair as alt alt . renderers . enable ( 'mimetype' ) df = pd . read_csv ( \"./data/febrl/dataset3.csv\" , delimiter = \", \" , dtype = { \"date_of_birth\" : str }, engine = \"python\" ) df [ \"cluster\" ] = df [ \"rec_id\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) df . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1496-org mitchell green 7.0 wallaby place delmar cleveland 2119 sa 19560409 1804974 rec-1496 1 rec-552-dup-3 harley mccarthy 177.0 pridhamstreet milton marsden 3165 nsw 19080419 6089216 rec-552 from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"unique_id_column_name\" : \"rec_id\" , \"link_type\" : \"dedupe_only\" , } linker = DuckDBLinker ( df , settings ) linker . missingness_chart () linker . profile_columns ( list ( df . columns )) potential_blocking_rules = [ \"l.soc_sec_id = r.soc_sec_id\" , \"l.given_name = r.given_name\" , \"l.surname = r.surname\" , \"l.date_of_birth = r.date_of_birth\" , \"l.postcode = r.postcode\" ] linker . cumulative_num_comparisons_from_blocking_rules_chart ( potential_blocking_rules ) from splink.duckdb.duckdb_linker import DuckDBLinker import splink.duckdb.duckdb_comparison_level_library as cll import splink.duckdb.duckdb_comparison_library as cl settings = { \"unique_id_column_name\" : \"rec_id\" , \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : potential_blocking_rules , \"comparisons\" : [ { \"output_column_name\" : \"Given name\" , \"comparison_levels\" : [ cll . null_level ( \"given_name\" ), cll . exact_match_level ( \"given_name\" , term_frequency_adjustments = True ), { \"sql_condition\" : '\"given_name_l\" = \"surname_r\"' , \"label_for_charts\" : \"Exact match on reversed cols, l to r\" , }, cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.7 ), { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, surname_r) > 0.7\" , \"label_for_charts\" : \"Jar on reversed cols, l to r\" , }, cll . else_level (), ], }, { \"output_column_name\" : \"Surname\" , \"comparison_levels\" : [ cll . null_level ( \"surname\" ), cll . exact_match_level ( \"surname\" , term_frequency_adjustments = True ), { \"sql_condition\" : '\"given_name_r\" = \"surname_l\"' , \"label_for_charts\" : \"Exact match on reversed cols, r to l\" , }, cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.7 ), { \"sql_condition\" : \"jaro_winkler_similarity(given_name_r, surname_l) > 0.7\" , \"label_for_charts\" : \"Jaro on reversed cols, l to r\" , }, cll . else_level (), ], }, cl . levenshtein_at_thresholds ( \"date_of_birth\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"soc_sec_id\" , [ 2 ]), cl . exact_match ( \"street_number\" , term_frequency_adjustments = True ), cl . exact_match ( \"postcode\" , term_frequency_adjustments = True ), ], \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings ) deterministic_rules = [ \"l.soc_sec_id = r.soc_sec_id\" , \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\" , \"l.given_name = r.surname and l.surname = r.given_name and l.date_of_birth = r.date_of_birth\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.9 ) Probability two random records match is estimated to be 0.000527. This means that amongst all possible pairwise record comparisons, one in 1,899.32 are expected to match. With 12,497,500 total possible comparisons, we expect a total of around 6,580.00 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - Given name (no m values are trained). - Surname (no m values are trained). - date_of_birth (no m values are trained). - soc_sec_id (no m values are trained). - street_number (no m values are trained). - postcode (no m values are trained). comparison = linker . _settings_obj . comparisons [ 2 ] . as_dict () session_dob = linker . estimate_parameters_using_expectation_maximisation ( \"substr(l.date_of_birth,1,3) = substr(r.date_of_birth,1,3)\" ) session_postcode = linker . estimate_parameters_using_expectation_maximisation ( \"substr(l.postcode,1,2) = substr(r.postcode,1,2)\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: substr(l.date_of_birth,1,3) = substr(r.date_of_birth,1,3) Parameter estimates will be made for the following comparison(s): - Given name - Surname - soc_sec_id - street_number - postcode Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - date_of_birth Iteration 1: Largest change in params was -0.493 in probability_two_random_records_match Iteration 2: Largest change in params was -0.00611 in the m_probability of soc_sec_id, level `All other comparisons` Iteration 3: Largest change in params was -0.000544 in the m_probability of soc_sec_id, level `All other comparisons` Iteration 4: Largest change in params was -6.13e-05 in the m_probability of soc_sec_id, level `All other comparisons` EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - date_of_birth (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: substr(l.postcode,1,2) = substr(r.postcode,1,2) Parameter estimates will be made for the following comparison(s): - Given name - Surname - date_of_birth - soc_sec_id - street_number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - postcode Iteration 1: Largest change in params was -0.234 in probability_two_random_records_match Iteration 2: Largest change in params was -0.0152 in the m_probability of soc_sec_id, level `All other comparisons` Iteration 3: Largest change in params was -0.000974 in the m_probability of soc_sec_id, level `All other comparisons` linker . match_weights_chart () results = linker . predict ( threshold_match_probability = 0.2 ) linker . roc_chart_from_labels_column ( \"cluster\" ) pred_errors_df = linker . prediction_errors_from_labels_column ( \"cluster\" ) . as_pandas_dataframe () len ( pred_errors_df ) pred_errors_df . head () records = linker . prediction_errors_from_labels_column ( \"cluster\" ) . as_record_dict ( limit = 10 ) linker . waterfall_chart ( records )","title":"Deduplicating the febrl3 dataset"},{"location":"demos/example_febrl4.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linking the febrl4 datasets \u00b6 See A.2 here and here for the source of this data. It consists of two datasets, A and B, of 5000 records each, with each record in dataset A having a corresponding record in dataset B. The aim will be to capture as many of those 5000 true links as possible, with minimal false linkages. It is worth noting that we should not necessarily expect to capture all links. There are some links that although we know they do correspond to the same person, the data is so mismatched between them that we would not reasonably expect a model to link them, and indeed should a model do so may indicate that we have overengineered things using our knowledge of true links, which will not be a helpful reference in situations where we attempt to link unlabelled data, as will usually be the case. Exploring data and defining model \u00b6 Firstly let's read in the data and have a little look at it import pandas as pd import altair as alt from IPython.display import IFrame alt . renderers . enable ( 'mimetype' ) def read_and_prepare ( fn ): df = pd . read_csv ( fn , delimiter = \", \" , dtype = { \"date_of_birth\" : str }, engine = \"python\" ) df [ \"cluster\" ] = df [ \"rec_id\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) return df dfs = [ read_and_prepare ( f \"./data/febrl/dataset4 { dataset } .csv\" ) for dataset in [ \"a\" , \"b\" ] ] display ( dfs [ 0 ] . head ( 2 )) display ( dfs [ 1 ] . head ( 2 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1070-org michaela neumann 8.0 stanley street miami winston hills 4223 nsw 19151111 5304218 rec-1070 1 rec-1016-org courtney painter 12.0 pinkerton circuit bega flats richlands 4560 vic 19161214 4066625 rec-1016 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-561-dup-0 elton NaN 3.0 light setreet pinehill windermere 3212 vic 19651013 1551941 rec-561 1 rec-2642-dup-0 mitchell maxon 47.0 edkins street lochaoair north ryde 3355 nsw 19390212 8859999 rec-2642 Next, to better understand which variables will prove useful in linking, we have a look at how populated each column is, as well as the distribution of unique values within each from splink.duckdb.duckdb_linker import DuckDBLinker basic_settings = { \"unique_id_column_name\" : \"rec_id\" , \"link_type\" : \"link_only\" , # NB as we are linking one-one, we know the probability that a random pair will be a match # hence we could set: # \"probability_two_random_records_match\": 1/5000, # however we will not specify this here, as we will use this as a check that # our estimation procedure returns something sensible } linker = DuckDBLinker ( dfs , basic_settings ) linker . missingness_chart () cols_to_profile = list ( dfs [ 0 ] . columns ) cols_to_profile = [ col for col in cols_to_profile if col not in ( \"rec_id\" , \"cluster\" )] linker . profile_columns ( cols_to_profile ) Next let's come up with some candidate blocking rules, which define which record comparisons are generated, and have a look at how many comparisons each will generate. For blocking rules that we use in prediction, our aim is to have the union of all rules cover all true matches, whilst avoiding generating so many comparisons that it becomes computationally intractable - i.e. each true match should have at least one of the following conditions holding. blocking_rules = [ \"l.given_name = r.given_name AND l.surname = r.surname\" , \"l.date_of_birth = r.date_of_birth\" , \"l.soc_sec_id = r.soc_sec_id\" , \"l.state = r.state AND l.address_1 = r.address_1\" , \"l.street_number = r.street_number AND l.address_1 = r.address_1\" , \"l.postcode = r.postcode\" , ] linker . cumulative_num_comparisons_from_blocking_rules_chart ( blocking_rules ) The broadest rule, having a matching postcode, unsurpisingly gives the largest number of comparisons. For this small dataset we still have a very manageable number, but if it was larger we might have needed to include a further AND condition with it to break the number of comparisons further. Now we get the full settings by including the blocking rules, as well as deciding the actual comparisons we will be including in our model. We will define two models, each with a separate linker with different settings, so that we can compare performance. One will be a very basic model, whilst the other will include a lot more detail. import splink.duckdb.duckdb_comparison_level_library as cll import splink.duckdb.duckdb_comparison_library as cl # the simple model only considers a few columns, and only two comparison levels for each simple_model_settings = { ** basic_settings , \"blocking_rules_to_generate_predictions\" : blocking_rules , \"comparisons\" : [ cl . exact_match ( \"given_name\" , term_frequency_adjustments = True ), cl . exact_match ( \"surname\" , term_frequency_adjustments = True ), cl . exact_match ( \"street_number\" , term_frequency_adjustments = True ), ], \"retain_intermediate_calculation_columns\" : True , } # the detailed model considers more columns, using the information we saw in the exploratory phase # we also include further comparison levels to account for typos and other differences detailed_model_settings = { ** basic_settings , \"blocking_rules_to_generate_predictions\" : blocking_rules , \"comparisons\" : [ { \"output_column_name\" : \"Given name\" , \"comparison_levels\" : [ cll . null_level ( \"given_name\" ), cll . exact_match_level ( \"given_name\" , term_frequency_adjustments = True ), cll . columns_reversed_level ( \"given_name\" , \"surname\" ), cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.7 ), cll . else_level (), ], }, { \"output_column_name\" : \"Surname\" , \"comparison_levels\" : [ cll . null_level ( \"surname\" ), cll . exact_match_level ( \"surname\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.7 ), cll . else_level (), ], }, cl . levenshtein_at_thresholds ( \"date_of_birth\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"soc_sec_id\" , [ 1 , 2 ]), cl . exact_match ( \"street_number\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"postcode\" , [ 1 , 2 ], term_frequency_adjustments = True ), # we don't consider further location columns as they will be strongly correlated with postcode ], \"retain_intermediate_calculation_columns\" : True , } linker_simple = DuckDBLinker ( dfs , simple_model_settings ) linker_detailed = DuckDBLinker ( dfs , detailed_model_settings ) Estimating model parameters \u00b6 We need to furnish our models with parameter estimates so that we can generate results. We will focus on the detailed model, generating the values for the simple model at the end We can instead estimate the probability two random records match, and compare with the known value of 1/5000 = 0.0002, to see how well our estimation procedure works. To do this we come up with some deterministic rules - the aim here is that we generate very few false positives (i.e. we expect that the majority of records with at least one of these conditions holding are true matches), whilst also capturing the majority of matches - our guess here is that these two rules should capture 80% of all matches. deterministic_rules = [ \"l.soc_sec_id = r.soc_sec_id\" , \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\" , ] linker_detailed . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.8 ) Probability two random records match is estimated to be 0.000238. This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match. With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs Even playing around with changing these deterministic rules, or the nominal recall leaves us with an answer which is pretty close to our known value Next we estimate u and m values for each comparison, so that we can move to generating predictions linker_detailed . estimate_u_using_random_sampling ( max_pairs = 1e7 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - Given name (no m values are trained). - Surname (no m values are trained). - date_of_birth (no m values are trained). - soc_sec_id (no m values are trained). - street_number (no m values are trained). - postcode (no m values are trained). When training the m values using expectation maximisation, we need somre more blocking rules to reduce the total number of comparisons. For each rule, we want to ensure that we have neither proportionally too many matches, or too few. We must run this multiple times using different rules so that we can obtain estimates for all comparisons - if we block on e.g. date_of_birth , then we cannot compute the m values for the date_of_birth comparison, as we have only looked at records where these match. session_dob = linker_detailed . estimate_parameters_using_expectation_maximisation ( \"l.date_of_birth = r.date_of_birth\" ) session_pc = linker_detailed . estimate_parameters_using_expectation_maximisation ( \"l.postcode = r.postcode\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.date_of_birth = r.date_of_birth Parameter estimates will be made for the following comparison(s): - Given name - Surname - soc_sec_id - street_number - postcode Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - date_of_birth Iteration 1: Largest change in params was 0.529 in probability_two_random_records_match Iteration 2: Largest change in params was 0.00321 in probability_two_random_records_match Iteration 3: Largest change in params was 2.92e-05 in the m_probability of soc_sec_id, level `All other comparisons` EM converged after 3 iterations Your model is not yet fully trained. Missing estimates for: - date_of_birth (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.postcode = r.postcode Parameter estimates will be made for the following comparison(s): - Given name - Surname - date_of_birth - soc_sec_id - street_number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - postcode Iteration 1: Largest change in params was 0.0296 in the m_probability of date_of_birth, level `All other comparisons` Iteration 2: Largest change in params was 0.000291 in the m_probability of date_of_birth, level `All other comparisons` Iteration 3: Largest change in params was 4.43e-06 in the m_probability of soc_sec_id, level `All other comparisons` EM converged after 3 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values If we wish we can have a look at how our parameter estimates changes over these training sessions # session_dob.m_u_values_interactive_history_chart() For variables that aren't used in the m -training blocking rules, we have two estimates --- one from each of the training sessions (see for example street_number ). We can have a look at how the values compare between them, to ensure that we don't have drastically different values, which may be indicative of an issue. linker_detailed . parameter_estimate_comparisons_chart () We repeat our parameter estimations for the simple model in much the same fashion linker_simple . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.8 ) linker_simple . estimate_u_using_random_sampling ( max_pairs = 1e7 ) session_ssid = linker_simple . estimate_parameters_using_expectation_maximisation ( \"l.given_name = r.given_name\" ) session_pc = linker_simple . estimate_parameters_using_expectation_maximisation ( \"l.street_number = r.street_number\" ) # linker_simple.parameter_estimate_comparisons_chart() Probability two random records match is estimated to be 0.000238. This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match. With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - given_name (no m values are trained). - surname (no m values are trained). - street_number (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.given_name = r.given_name Parameter estimates will be made for the following comparison(s): - surname - street_number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - given_name Iteration 1: Largest change in params was -0.0987 in the m_probability of surname, level `Exact match` Iteration 2: Largest change in params was -0.0471 in the m_probability of surname, level `Exact match` Iteration 3: Largest change in params was 0.0331 in the m_probability of surname, level `All other comparisons` Iteration 4: Largest change in params was 0.0219 in the m_probability of surname, level `All other comparisons` Iteration 5: Largest change in params was -0.0138 in the m_probability of surname, level `Exact match` Iteration 6: Largest change in params was -0.00855 in the m_probability of surname, level `Exact match` Iteration 7: Largest change in params was -0.00527 in the m_probability of surname, level `Exact match` Iteration 8: Largest change in params was 0.00325 in the m_probability of surname, level `All other comparisons` Iteration 9: Largest change in params was -0.00202 in the m_probability of surname, level `Exact match` Iteration 10: Largest change in params was 0.00126 in the m_probability of surname, level `All other comparisons` Iteration 11: Largest change in params was -0.000793 in the m_probability of surname, level `Exact match` Iteration 12: Largest change in params was -0.0005 in the m_probability of surname, level `Exact match` Iteration 13: Largest change in params was -0.000316 in the m_probability of surname, level `Exact match` Iteration 14: Largest change in params was -0.000201 in the m_probability of surname, level `Exact match` Iteration 15: Largest change in params was 0.000128 in the m_probability of surname, level `All other comparisons` Iteration 16: Largest change in params was -8.12e-05 in the m_probability of surname, level `Exact match` EM converged after 16 iterations Your model is not yet fully trained. Missing estimates for: - given_name (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.street_number = r.street_number Parameter estimates will be made for the following comparison(s): - given_name - surname Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - street_number Iteration 1: Largest change in params was 0.0656 in the m_probability of given_name, level `All other comparisons` Iteration 2: Largest change in params was -0.0433 in the m_probability of given_name, level `Exact match` Iteration 3: Largest change in params was 0.0278 in the m_probability of given_name, level `All other comparisons` Iteration 4: Largest change in params was -0.0161 in the m_probability of given_name, level `Exact match` Iteration 5: Largest change in params was 0.00883 in the m_probability of given_name, level `All other comparisons` Iteration 6: Largest change in params was -0.00481 in the m_probability of given_name, level `Exact match` Iteration 7: Largest change in params was 0.00265 in the m_probability of given_name, level `All other comparisons` Iteration 8: Largest change in params was 0.00149 in the m_probability of given_name, level `All other comparisons` Iteration 9: Largest change in params was -0.000869 in the m_probability of given_name, level `Exact match` Iteration 10: Largest change in params was -0.000523 in the m_probability of given_name, level `Exact match` Iteration 11: Largest change in params was -0.000325 in the m_probability of given_name, level `Exact match` Iteration 12: Largest change in params was -0.000226 in the m_probability of surname, level `Exact match` Iteration 13: Largest change in params was -0.000163 in the m_probability of surname, level `Exact match` Iteration 14: Largest change in params was 0.000116 in the m_probability of surname, level `All other comparisons` Iteration 15: Largest change in params was 8.19e-05 in the m_probability of surname, level `All other comparisons` EM converged after 15 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values # import json # we can have a look at the full settings if we wish, including the values of our estimated parameters: # print(json.dumps(linker_detailed._settings_obj.as_dict(), indent=2)) # we can also get a handy summary of of the model in an easily readable format if we wish: # print(linker_detailed._settings_obj.human_readable_description) # (we suppress output here for brevity) We can now visualise some of the details of our models. We can look at the match weights, which tell us the relative importance for/against a match for each of our comparsion levels. Comparing the two models will show the added benefit we get in the more detailed model --- what in the simple model is classed as 'all other comparisons' is instead broken down further, and we can see that the detail of how this is broken down in fact gives us quite a bit of useful information about the likelihood of a match. linker_simple . match_weights_chart () linker_detailed . match_weights_chart () As well as the match weights, which give us an idea of the overall effect of each comparison level, we can also look at the individual u and m parameter estimates, which tells us about the prevalence of coincidences and mistakes (for further details/explanation about this see this article ). We might want to revise aspects of our model based on the information we ascertain here. Note however that some of these values are very small, which is why the match weight chart is often more useful for getting a decent picture of things. # linker_simple.m_u_parameters_chart() linker_detailed . m_u_parameters_chart () It is also useful to have a look at unlinkable records - these are records which do not contain enough information to be linked at some match probability threshold. We can figure this out be seeing whether records are able to be matched with themselves. This is of course relative to the information we have put into the model - we see that in our simple model, at a 99% match threshold nearly 10% of records are unlinkable, as we have not included enough information in the model for distinct records to be adequately distinguished; this is not an issue in our more detailed model. linker_simple . unlinkables_chart () linker_detailed . unlinkables_chart () Our simple model doesn't do terribly , but suffers if we want to have a high match probability --- to be 99% certain of matches we have ~10% of records that we will be unable to link. Our detailed model, however, has enough nuance that we can at least self-link records. Predictions \u00b6 Now that we have had a look into the details of the models, we will focus on only our more detailed model, which should be able to capture more of the genuine links in our data predictions = linker_detailed . predict () df_predictions = predictions . as_pandas_dataframe () df_predictions . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability source_dataset_l rec_id_l source_dataset_r rec_id_r given_name_l given_name_r surname_l surname_r ... bf_street_number bf_tf_adj_street_number postcode_l postcode_r gamma_postcode tf_postcode_l tf_postcode_r bf_postcode bf_tf_adj_postcode match_key 0 42.077751 1.0 _a rec-1016-org _b rec-1016-dup-0 courtney courtney painter painter ... 59.013018 0.647003 4560 4560 3 0.0030 0.0030 609.446252 0.459895 0 1 50.653290 1.0 _a rec-1288-org _b rec-1288-dup-0 vanessa vanessa parr parr ... 59.013018 70.523279 2135 2135 3 0.0015 0.0015 609.446252 0.919790 0 2 32.748771 1.0 _a rec-298-org _b rec-298-dup-0 blake blake howie howie ... 59.013018 0.418536 6017 6071 1 0.0005 0.0005 0.914304 1.000000 0 3 49.012108 1.0 _a rec-2404-org _b rec-2404-dup-0 blakeston blakeston broadby broadby ... 59.013018 4.029902 3083 3083 3 0.0011 0.0011 609.446252 1.254259 0 4 30.096869 1.0 _a rec-453-org _b rec-453-dup-0 edward edward denholm denholm ... 59.013018 0.548819 4221 4221 3 0.0017 0.0017 609.446252 0.811580 0 5 rows \u00d7 46 columns We can see how our model performs at different probability thresholds, with a couple of options depending on the space we wish to view things # linker_detailed.roc_chart_from_labels_column(\"cluster\") linker_detailed . precision_recall_chart_from_labels_column ( \"cluster\" ) and we can easily see how many individuals we identify and link by looking at clusters generated at some threshold match probability of interest - in this example 99% clusters = linker_detailed . cluster_pairwise_predictions_at_threshold ( predictions , threshold_match_probability = 0.99 ) df_clusters = clusters . as_pandas_dataframe () . sort_values ( \"cluster_id\" ) df_clusters . groupby ( \"cluster_id\" ) . size () . value_counts () Completed iteration 1, root rows count 0 2 4966 1 68 dtype: int64 In this case, we happen to know what the true links are, so we can manually inspect the ones that are doing worst to see what our model is not capturing - i.e. where we have false negatives. Similarly, we can look at the non-links which are performing the best, to see whether we have an issue with false positives. Ordinarily we would not have this luxury, and so would need to dig a bit deeper for clues as to how to improve our model, such as manually inspecting records across threshold probabilities, df_predictions [ \"cluster_l\" ] = df_predictions [ \"rec_id_l\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) df_predictions [ \"cluster_r\" ] = df_predictions [ \"rec_id_r\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) df_true_links = df_predictions [ df_predictions [ \"cluster_l\" ] == df_predictions [ \"cluster_r\" ]] . sort_values ( \"match_probability\" ) records_to_view = 3 linker_detailed . waterfall_chart ( df_true_links . head ( records_to_view ) . to_dict ( orient = \"records\" )) df_non_links = df_predictions [ df_predictions [ \"cluster_l\" ] != df_predictions [ \"cluster_r\" ]] . sort_values ( \"match_probability\" , ascending = False ) linker_detailed . waterfall_chart ( df_non_links . head ( records_to_view ) . to_dict ( orient = \"records\" )) Further refinements \u00b6 Looking at the non-links we have done well in having no false positives at any substantial match probability --- however looking at some of the true links we can see that there are a few that we are not capturing with sufficient match probability. We can see that there are a few features that we are not capturing/weighting appropriately * single-character transpostions, particularly in postcode (which is being lumped in with more 'severe typos'/probable non-matches) * given/sur-names being swapped with typos * given/sur-names being cross-matches on one only, with no match on the other cross We will quickly see if we can incorporate these features into a new model. As we are now going into more detail with the inter-relationship between given name and surname, it is probably no longer sensible to model them as independent comparisons, and so we will need to switch to a combined comparison on full name. def single_transposition_level ( column , label_for_charts = None ): if label_for_charts is None : label_for_charts = \"Single transposition\" return { # we have the same set of letters, and two levenshtein edits between them - i.e. a single transposition \"sql_condition\" : f \"jaccard( { column } _l, { column } _r) = 1 AND levenshtein( { column } _l, { column } _r) = 2\" , \"label_for_charts\" : label_for_charts } # we need to append a full name column to our source data frames # so that we can use it for term frequency adjustments dfs [ 0 ][ \"full_name\" ] = dfs [ 0 ][ \"given_name\" ] + \"_\" + dfs [ 0 ][ \"surname\" ] dfs [ 1 ][ \"full_name\" ] = dfs [ 1 ][ \"given_name\" ] + \"_\" + dfs [ 1 ][ \"surname\" ] extended_model_settings = { ** basic_settings , \"blocking_rules_to_generate_predictions\" : blocking_rules , \"comparisons\" : [ { \"output_column_name\" : \"Full name\" , \"comparison_levels\" : [ { \"sql_condition\" : \"(given_name_l IS NULL OR given_name_r IS NULL) and (surname_l IS NULL OR surname_r IS NULL)\" , \"label_for_charts\" : \"Null\" , \"is_null_level\" : True , }, # full name match { \"sql_condition\" : \"full_name_l = full_name_r\" , \"label_for_charts\" : \"Both names match\" , \"tf_adjustment_column\" : \"full_name\" , \"tf_adjustment_weight\" : 1.0 , }, # typos - keep levels across full name rather than scoring separately { \"sql_condition\" : \"jaro_winkler_similarity(full_name_l, full_name_r) > 0.9\" , \"label_for_charts\" : \"JW (full name) > 0.9\" , }, { \"sql_condition\" : \"jaro_winkler_similarity(full_name_l, full_name_r) > 0.7\" , \"label_for_charts\" : \"JW (full name) > 0.7\" , }, # name switched { \"sql_condition\" : \"given_name_l = surname_r AND surname_l = given_name_r\" , \"label_for_charts\" : \"Names switched\" , }, # name switched + typo { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) >= 1.8\" , \"label_for_charts\" : \"switched + jaro_winkler_similarity >= 1.8\" }, { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) >= 1.4\" , \"label_for_charts\" : \"switched + jaro_winkler_similarity >= 1.4\" }, # single name match { \"sql_condition\" : \"given_name_l = given_name_r\" , \"label_for_charts\" : \"given name matches\" , \"tf_adjustment_column\" : \"given_name\" , \"tf_adjustment_weight\" : 1.0 , }, { \"sql_condition\" : \"surname_l = surname_r\" , \"label_for_charts\" : \"surname matches\" , \"tf_adjustment_column\" : \"surname\" , \"tf_adjustment_weight\" : 1.0 , }, # single name cross-match { \"sql_condition\" : \"given_name_l = surname_r OR surname_l = given_name_r\" , \"label_for_charts\" : \"single name cross-matches\" }, # single name typos { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, given_name_r) > 0.9\" , \"label_for_charts\" : \"JW (given name) > 0.9\" , }, { \"sql_condition\" : \"jaro_winkler_similarity(surname_l, surname_r) > 0.9\" , \"label_for_charts\" : \"JW (surname) > 0.9\" , }, # the rest cll . else_level () ] }, { \"output_column_name\" : \"Date of birth\" , \"comparison_levels\" : [ cll . null_level ( \"date_of_birth\" ), cll . exact_match_level ( \"date_of_birth\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"date_of_birth\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), # this is the extra level single_transposition_level ( \"date_of_birth\" ), cll . distance_function_level ( \"date_of_birth\" , \"levenshtein\" , 2 , higher_is_more_similar = False ), cll . else_level () ] }, { \"output_column_name\" : \"Social security ID\" , \"comparison_levels\" : [ cll . null_level ( \"soc_sec_id\" ), cll . exact_match_level ( \"soc_sec_id\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"soc_sec_id\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), single_transposition_level ( \"soc_sec_id\" ), cll . distance_function_level ( \"soc_sec_id\" , \"levenshtein\" , 2 , higher_is_more_similar = False ), cll . else_level () ] }, { \"output_column_name\" : \"Street number\" , \"comparison_levels\" : [ cll . null_level ( \"street_number\" ), cll . exact_match_level ( \"street_number\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"street_number\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), single_transposition_level ( \"street_number\" ), cll . else_level () ] }, { \"output_column_name\" : \"Postcode\" , \"comparison_levels\" : [ cll . null_level ( \"postcode\" ), cll . exact_match_level ( \"postcode\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"postcode\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), single_transposition_level ( \"postcode\" ), cll . distance_function_level ( \"postcode\" , \"levenshtein\" , 2 , higher_is_more_similar = False ), cll . else_level () ] }, # we don't consider further location columns as they will be strongly correlated with postcode ], \"retain_intermediate_calculation_columns\" : True , } # train linker_advanced = DuckDBLinker ( dfs , extended_model_settings ) linker_advanced . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.8 ) # we increase target rows to improve accuracy for u values in full name comparison, as we have subdivided the data more finely linker_advanced . estimate_u_using_random_sampling ( max_pairs = 1e8 ) session_dob = linker_advanced . estimate_parameters_using_expectation_maximisation ( \"l.date_of_birth = r.date_of_birth\" ) session_pc = linker_advanced . estimate_parameters_using_expectation_maximisation ( \"l.postcode = r.postcode\" ) Probability two random records match is estimated to be 0.000238. This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match. With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - Full name (no m values are trained). - Date of birth (no m values are trained). - Social security ID (no m values are trained). - Street number (no m values are trained). - Postcode (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.date_of_birth = r.date_of_birth Parameter estimates will be made for the following comparison(s): - Full name - Social security ID - Street number - Postcode Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - Date of birth Iteration 1: Largest change in params was -0.484 in the m_probability of Full name, level `Both names match` Iteration 2: Largest change in params was 0.00198 in probability_two_random_records_match Iteration 3: Largest change in params was 8.13e-06 in probability_two_random_records_match EM converged after 3 iterations Your model is not yet fully trained. Missing estimates for: - Date of birth (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.postcode = r.postcode Parameter estimates will be made for the following comparison(s): - Full name - Date of birth - Social security ID - Street number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - Postcode Iteration 1: Largest change in params was 0.0333 in the m_probability of Date of birth, level `All other comparisons` Iteration 2: Largest change in params was 0.000465 in the m_probability of Date of birth, level `All other comparisons` Iteration 3: Largest change in params was 1.18e-05 in the m_probability of Social security ID, level `All other comparisons` EM converged after 3 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values # linker_advanced.parameter_estimate_comparisons_chart() linker_advanced . match_weights_chart () predictions_adv = linker_advanced . predict () df_predictions_adv = predictions_adv . as_pandas_dataframe () clusters_adv = linker_advanced . cluster_pairwise_predictions_at_threshold ( predictions_adv , threshold_match_probability = 0.99 ) df_clusters_adv = clusters_adv . as_pandas_dataframe () . sort_values ( \"cluster_id\" ) df_clusters_adv . groupby ( \"cluster_id\" ) . size () . value_counts () Completed iteration 1, root rows count 0 2 4970 1 60 dtype: int64 This is a pretty modest improvement on our previous model - however it is worth re-iterating that we should not necessarily expect to recover all matches, as in several cases it may be unreasonable for a model to have reasonable confidence that two records refer to the same entity. If we wished to improve matters we could iterate on this process - investigating where our model is not performing as we would hope, and seeing how we can adjust these areas to address these shortcomings.","title":"Febrl4 link-only"},{"location":"demos/example_febrl4.html#linking-the-febrl4-datasets","text":"See A.2 here and here for the source of this data. It consists of two datasets, A and B, of 5000 records each, with each record in dataset A having a corresponding record in dataset B. The aim will be to capture as many of those 5000 true links as possible, with minimal false linkages. It is worth noting that we should not necessarily expect to capture all links. There are some links that although we know they do correspond to the same person, the data is so mismatched between them that we would not reasonably expect a model to link them, and indeed should a model do so may indicate that we have overengineered things using our knowledge of true links, which will not be a helpful reference in situations where we attempt to link unlabelled data, as will usually be the case.","title":"Linking the febrl4 datasets"},{"location":"demos/example_febrl4.html#exploring-data-and-defining-model","text":"Firstly let's read in the data and have a little look at it import pandas as pd import altair as alt from IPython.display import IFrame alt . renderers . enable ( 'mimetype' ) def read_and_prepare ( fn ): df = pd . read_csv ( fn , delimiter = \", \" , dtype = { \"date_of_birth\" : str }, engine = \"python\" ) df [ \"cluster\" ] = df [ \"rec_id\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) return df dfs = [ read_and_prepare ( f \"./data/febrl/dataset4 { dataset } .csv\" ) for dataset in [ \"a\" , \"b\" ] ] display ( dfs [ 0 ] . head ( 2 )) display ( dfs [ 1 ] . head ( 2 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1070-org michaela neumann 8.0 stanley street miami winston hills 4223 nsw 19151111 5304218 rec-1070 1 rec-1016-org courtney painter 12.0 pinkerton circuit bega flats richlands 4560 vic 19161214 4066625 rec-1016 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-561-dup-0 elton NaN 3.0 light setreet pinehill windermere 3212 vic 19651013 1551941 rec-561 1 rec-2642-dup-0 mitchell maxon 47.0 edkins street lochaoair north ryde 3355 nsw 19390212 8859999 rec-2642 Next, to better understand which variables will prove useful in linking, we have a look at how populated each column is, as well as the distribution of unique values within each from splink.duckdb.duckdb_linker import DuckDBLinker basic_settings = { \"unique_id_column_name\" : \"rec_id\" , \"link_type\" : \"link_only\" , # NB as we are linking one-one, we know the probability that a random pair will be a match # hence we could set: # \"probability_two_random_records_match\": 1/5000, # however we will not specify this here, as we will use this as a check that # our estimation procedure returns something sensible } linker = DuckDBLinker ( dfs , basic_settings ) linker . missingness_chart () cols_to_profile = list ( dfs [ 0 ] . columns ) cols_to_profile = [ col for col in cols_to_profile if col not in ( \"rec_id\" , \"cluster\" )] linker . profile_columns ( cols_to_profile ) Next let's come up with some candidate blocking rules, which define which record comparisons are generated, and have a look at how many comparisons each will generate. For blocking rules that we use in prediction, our aim is to have the union of all rules cover all true matches, whilst avoiding generating so many comparisons that it becomes computationally intractable - i.e. each true match should have at least one of the following conditions holding. blocking_rules = [ \"l.given_name = r.given_name AND l.surname = r.surname\" , \"l.date_of_birth = r.date_of_birth\" , \"l.soc_sec_id = r.soc_sec_id\" , \"l.state = r.state AND l.address_1 = r.address_1\" , \"l.street_number = r.street_number AND l.address_1 = r.address_1\" , \"l.postcode = r.postcode\" , ] linker . cumulative_num_comparisons_from_blocking_rules_chart ( blocking_rules ) The broadest rule, having a matching postcode, unsurpisingly gives the largest number of comparisons. For this small dataset we still have a very manageable number, but if it was larger we might have needed to include a further AND condition with it to break the number of comparisons further. Now we get the full settings by including the blocking rules, as well as deciding the actual comparisons we will be including in our model. We will define two models, each with a separate linker with different settings, so that we can compare performance. One will be a very basic model, whilst the other will include a lot more detail. import splink.duckdb.duckdb_comparison_level_library as cll import splink.duckdb.duckdb_comparison_library as cl # the simple model only considers a few columns, and only two comparison levels for each simple_model_settings = { ** basic_settings , \"blocking_rules_to_generate_predictions\" : blocking_rules , \"comparisons\" : [ cl . exact_match ( \"given_name\" , term_frequency_adjustments = True ), cl . exact_match ( \"surname\" , term_frequency_adjustments = True ), cl . exact_match ( \"street_number\" , term_frequency_adjustments = True ), ], \"retain_intermediate_calculation_columns\" : True , } # the detailed model considers more columns, using the information we saw in the exploratory phase # we also include further comparison levels to account for typos and other differences detailed_model_settings = { ** basic_settings , \"blocking_rules_to_generate_predictions\" : blocking_rules , \"comparisons\" : [ { \"output_column_name\" : \"Given name\" , \"comparison_levels\" : [ cll . null_level ( \"given_name\" ), cll . exact_match_level ( \"given_name\" , term_frequency_adjustments = True ), cll . columns_reversed_level ( \"given_name\" , \"surname\" ), cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"given_name\" , \"jaro_winkler_similarity\" , 0.7 ), cll . else_level (), ], }, { \"output_column_name\" : \"Surname\" , \"comparison_levels\" : [ cll . null_level ( \"surname\" ), cll . exact_match_level ( \"surname\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.9 ), cll . distance_function_level ( \"surname\" , \"jaro_winkler_similarity\" , 0.7 ), cll . else_level (), ], }, cl . levenshtein_at_thresholds ( \"date_of_birth\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"soc_sec_id\" , [ 1 , 2 ]), cl . exact_match ( \"street_number\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"postcode\" , [ 1 , 2 ], term_frequency_adjustments = True ), # we don't consider further location columns as they will be strongly correlated with postcode ], \"retain_intermediate_calculation_columns\" : True , } linker_simple = DuckDBLinker ( dfs , simple_model_settings ) linker_detailed = DuckDBLinker ( dfs , detailed_model_settings )","title":"Exploring data and defining model"},{"location":"demos/example_febrl4.html#estimating-model-parameters","text":"We need to furnish our models with parameter estimates so that we can generate results. We will focus on the detailed model, generating the values for the simple model at the end We can instead estimate the probability two random records match, and compare with the known value of 1/5000 = 0.0002, to see how well our estimation procedure works. To do this we come up with some deterministic rules - the aim here is that we generate very few false positives (i.e. we expect that the majority of records with at least one of these conditions holding are true matches), whilst also capturing the majority of matches - our guess here is that these two rules should capture 80% of all matches. deterministic_rules = [ \"l.soc_sec_id = r.soc_sec_id\" , \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\" , ] linker_detailed . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.8 ) Probability two random records match is estimated to be 0.000238. This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match. With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs Even playing around with changing these deterministic rules, or the nominal recall leaves us with an answer which is pretty close to our known value Next we estimate u and m values for each comparison, so that we can move to generating predictions linker_detailed . estimate_u_using_random_sampling ( max_pairs = 1e7 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - Given name (no m values are trained). - Surname (no m values are trained). - date_of_birth (no m values are trained). - soc_sec_id (no m values are trained). - street_number (no m values are trained). - postcode (no m values are trained). When training the m values using expectation maximisation, we need somre more blocking rules to reduce the total number of comparisons. For each rule, we want to ensure that we have neither proportionally too many matches, or too few. We must run this multiple times using different rules so that we can obtain estimates for all comparisons - if we block on e.g. date_of_birth , then we cannot compute the m values for the date_of_birth comparison, as we have only looked at records where these match. session_dob = linker_detailed . estimate_parameters_using_expectation_maximisation ( \"l.date_of_birth = r.date_of_birth\" ) session_pc = linker_detailed . estimate_parameters_using_expectation_maximisation ( \"l.postcode = r.postcode\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.date_of_birth = r.date_of_birth Parameter estimates will be made for the following comparison(s): - Given name - Surname - soc_sec_id - street_number - postcode Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - date_of_birth Iteration 1: Largest change in params was 0.529 in probability_two_random_records_match Iteration 2: Largest change in params was 0.00321 in probability_two_random_records_match Iteration 3: Largest change in params was 2.92e-05 in the m_probability of soc_sec_id, level `All other comparisons` EM converged after 3 iterations Your model is not yet fully trained. Missing estimates for: - date_of_birth (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.postcode = r.postcode Parameter estimates will be made for the following comparison(s): - Given name - Surname - date_of_birth - soc_sec_id - street_number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - postcode Iteration 1: Largest change in params was 0.0296 in the m_probability of date_of_birth, level `All other comparisons` Iteration 2: Largest change in params was 0.000291 in the m_probability of date_of_birth, level `All other comparisons` Iteration 3: Largest change in params was 4.43e-06 in the m_probability of soc_sec_id, level `All other comparisons` EM converged after 3 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values If we wish we can have a look at how our parameter estimates changes over these training sessions # session_dob.m_u_values_interactive_history_chart() For variables that aren't used in the m -training blocking rules, we have two estimates --- one from each of the training sessions (see for example street_number ). We can have a look at how the values compare between them, to ensure that we don't have drastically different values, which may be indicative of an issue. linker_detailed . parameter_estimate_comparisons_chart () We repeat our parameter estimations for the simple model in much the same fashion linker_simple . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.8 ) linker_simple . estimate_u_using_random_sampling ( max_pairs = 1e7 ) session_ssid = linker_simple . estimate_parameters_using_expectation_maximisation ( \"l.given_name = r.given_name\" ) session_pc = linker_simple . estimate_parameters_using_expectation_maximisation ( \"l.street_number = r.street_number\" ) # linker_simple.parameter_estimate_comparisons_chart() Probability two random records match is estimated to be 0.000238. This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match. With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - given_name (no m values are trained). - surname (no m values are trained). - street_number (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.given_name = r.given_name Parameter estimates will be made for the following comparison(s): - surname - street_number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - given_name Iteration 1: Largest change in params was -0.0987 in the m_probability of surname, level `Exact match` Iteration 2: Largest change in params was -0.0471 in the m_probability of surname, level `Exact match` Iteration 3: Largest change in params was 0.0331 in the m_probability of surname, level `All other comparisons` Iteration 4: Largest change in params was 0.0219 in the m_probability of surname, level `All other comparisons` Iteration 5: Largest change in params was -0.0138 in the m_probability of surname, level `Exact match` Iteration 6: Largest change in params was -0.00855 in the m_probability of surname, level `Exact match` Iteration 7: Largest change in params was -0.00527 in the m_probability of surname, level `Exact match` Iteration 8: Largest change in params was 0.00325 in the m_probability of surname, level `All other comparisons` Iteration 9: Largest change in params was -0.00202 in the m_probability of surname, level `Exact match` Iteration 10: Largest change in params was 0.00126 in the m_probability of surname, level `All other comparisons` Iteration 11: Largest change in params was -0.000793 in the m_probability of surname, level `Exact match` Iteration 12: Largest change in params was -0.0005 in the m_probability of surname, level `Exact match` Iteration 13: Largest change in params was -0.000316 in the m_probability of surname, level `Exact match` Iteration 14: Largest change in params was -0.000201 in the m_probability of surname, level `Exact match` Iteration 15: Largest change in params was 0.000128 in the m_probability of surname, level `All other comparisons` Iteration 16: Largest change in params was -8.12e-05 in the m_probability of surname, level `Exact match` EM converged after 16 iterations Your model is not yet fully trained. Missing estimates for: - given_name (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.street_number = r.street_number Parameter estimates will be made for the following comparison(s): - given_name - surname Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - street_number Iteration 1: Largest change in params was 0.0656 in the m_probability of given_name, level `All other comparisons` Iteration 2: Largest change in params was -0.0433 in the m_probability of given_name, level `Exact match` Iteration 3: Largest change in params was 0.0278 in the m_probability of given_name, level `All other comparisons` Iteration 4: Largest change in params was -0.0161 in the m_probability of given_name, level `Exact match` Iteration 5: Largest change in params was 0.00883 in the m_probability of given_name, level `All other comparisons` Iteration 6: Largest change in params was -0.00481 in the m_probability of given_name, level `Exact match` Iteration 7: Largest change in params was 0.00265 in the m_probability of given_name, level `All other comparisons` Iteration 8: Largest change in params was 0.00149 in the m_probability of given_name, level `All other comparisons` Iteration 9: Largest change in params was -0.000869 in the m_probability of given_name, level `Exact match` Iteration 10: Largest change in params was -0.000523 in the m_probability of given_name, level `Exact match` Iteration 11: Largest change in params was -0.000325 in the m_probability of given_name, level `Exact match` Iteration 12: Largest change in params was -0.000226 in the m_probability of surname, level `Exact match` Iteration 13: Largest change in params was -0.000163 in the m_probability of surname, level `Exact match` Iteration 14: Largest change in params was 0.000116 in the m_probability of surname, level `All other comparisons` Iteration 15: Largest change in params was 8.19e-05 in the m_probability of surname, level `All other comparisons` EM converged after 15 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values # import json # we can have a look at the full settings if we wish, including the values of our estimated parameters: # print(json.dumps(linker_detailed._settings_obj.as_dict(), indent=2)) # we can also get a handy summary of of the model in an easily readable format if we wish: # print(linker_detailed._settings_obj.human_readable_description) # (we suppress output here for brevity) We can now visualise some of the details of our models. We can look at the match weights, which tell us the relative importance for/against a match for each of our comparsion levels. Comparing the two models will show the added benefit we get in the more detailed model --- what in the simple model is classed as 'all other comparisons' is instead broken down further, and we can see that the detail of how this is broken down in fact gives us quite a bit of useful information about the likelihood of a match. linker_simple . match_weights_chart () linker_detailed . match_weights_chart () As well as the match weights, which give us an idea of the overall effect of each comparison level, we can also look at the individual u and m parameter estimates, which tells us about the prevalence of coincidences and mistakes (for further details/explanation about this see this article ). We might want to revise aspects of our model based on the information we ascertain here. Note however that some of these values are very small, which is why the match weight chart is often more useful for getting a decent picture of things. # linker_simple.m_u_parameters_chart() linker_detailed . m_u_parameters_chart () It is also useful to have a look at unlinkable records - these are records which do not contain enough information to be linked at some match probability threshold. We can figure this out be seeing whether records are able to be matched with themselves. This is of course relative to the information we have put into the model - we see that in our simple model, at a 99% match threshold nearly 10% of records are unlinkable, as we have not included enough information in the model for distinct records to be adequately distinguished; this is not an issue in our more detailed model. linker_simple . unlinkables_chart () linker_detailed . unlinkables_chart () Our simple model doesn't do terribly , but suffers if we want to have a high match probability --- to be 99% certain of matches we have ~10% of records that we will be unable to link. Our detailed model, however, has enough nuance that we can at least self-link records.","title":"Estimating model parameters"},{"location":"demos/example_febrl4.html#predictions","text":"Now that we have had a look into the details of the models, we will focus on only our more detailed model, which should be able to capture more of the genuine links in our data predictions = linker_detailed . predict () df_predictions = predictions . as_pandas_dataframe () df_predictions . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability source_dataset_l rec_id_l source_dataset_r rec_id_r given_name_l given_name_r surname_l surname_r ... bf_street_number bf_tf_adj_street_number postcode_l postcode_r gamma_postcode tf_postcode_l tf_postcode_r bf_postcode bf_tf_adj_postcode match_key 0 42.077751 1.0 _a rec-1016-org _b rec-1016-dup-0 courtney courtney painter painter ... 59.013018 0.647003 4560 4560 3 0.0030 0.0030 609.446252 0.459895 0 1 50.653290 1.0 _a rec-1288-org _b rec-1288-dup-0 vanessa vanessa parr parr ... 59.013018 70.523279 2135 2135 3 0.0015 0.0015 609.446252 0.919790 0 2 32.748771 1.0 _a rec-298-org _b rec-298-dup-0 blake blake howie howie ... 59.013018 0.418536 6017 6071 1 0.0005 0.0005 0.914304 1.000000 0 3 49.012108 1.0 _a rec-2404-org _b rec-2404-dup-0 blakeston blakeston broadby broadby ... 59.013018 4.029902 3083 3083 3 0.0011 0.0011 609.446252 1.254259 0 4 30.096869 1.0 _a rec-453-org _b rec-453-dup-0 edward edward denholm denholm ... 59.013018 0.548819 4221 4221 3 0.0017 0.0017 609.446252 0.811580 0 5 rows \u00d7 46 columns We can see how our model performs at different probability thresholds, with a couple of options depending on the space we wish to view things # linker_detailed.roc_chart_from_labels_column(\"cluster\") linker_detailed . precision_recall_chart_from_labels_column ( \"cluster\" ) and we can easily see how many individuals we identify and link by looking at clusters generated at some threshold match probability of interest - in this example 99% clusters = linker_detailed . cluster_pairwise_predictions_at_threshold ( predictions , threshold_match_probability = 0.99 ) df_clusters = clusters . as_pandas_dataframe () . sort_values ( \"cluster_id\" ) df_clusters . groupby ( \"cluster_id\" ) . size () . value_counts () Completed iteration 1, root rows count 0 2 4966 1 68 dtype: int64 In this case, we happen to know what the true links are, so we can manually inspect the ones that are doing worst to see what our model is not capturing - i.e. where we have false negatives. Similarly, we can look at the non-links which are performing the best, to see whether we have an issue with false positives. Ordinarily we would not have this luxury, and so would need to dig a bit deeper for clues as to how to improve our model, such as manually inspecting records across threshold probabilities, df_predictions [ \"cluster_l\" ] = df_predictions [ \"rec_id_l\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) df_predictions [ \"cluster_r\" ] = df_predictions [ \"rec_id_r\" ] . apply ( lambda x : \"-\" . join ( x . split ( '-' )[: 2 ])) df_true_links = df_predictions [ df_predictions [ \"cluster_l\" ] == df_predictions [ \"cluster_r\" ]] . sort_values ( \"match_probability\" ) records_to_view = 3 linker_detailed . waterfall_chart ( df_true_links . head ( records_to_view ) . to_dict ( orient = \"records\" )) df_non_links = df_predictions [ df_predictions [ \"cluster_l\" ] != df_predictions [ \"cluster_r\" ]] . sort_values ( \"match_probability\" , ascending = False ) linker_detailed . waterfall_chart ( df_non_links . head ( records_to_view ) . to_dict ( orient = \"records\" ))","title":"Predictions"},{"location":"demos/example_febrl4.html#further-refinements","text":"Looking at the non-links we have done well in having no false positives at any substantial match probability --- however looking at some of the true links we can see that there are a few that we are not capturing with sufficient match probability. We can see that there are a few features that we are not capturing/weighting appropriately * single-character transpostions, particularly in postcode (which is being lumped in with more 'severe typos'/probable non-matches) * given/sur-names being swapped with typos * given/sur-names being cross-matches on one only, with no match on the other cross We will quickly see if we can incorporate these features into a new model. As we are now going into more detail with the inter-relationship between given name and surname, it is probably no longer sensible to model them as independent comparisons, and so we will need to switch to a combined comparison on full name. def single_transposition_level ( column , label_for_charts = None ): if label_for_charts is None : label_for_charts = \"Single transposition\" return { # we have the same set of letters, and two levenshtein edits between them - i.e. a single transposition \"sql_condition\" : f \"jaccard( { column } _l, { column } _r) = 1 AND levenshtein( { column } _l, { column } _r) = 2\" , \"label_for_charts\" : label_for_charts } # we need to append a full name column to our source data frames # so that we can use it for term frequency adjustments dfs [ 0 ][ \"full_name\" ] = dfs [ 0 ][ \"given_name\" ] + \"_\" + dfs [ 0 ][ \"surname\" ] dfs [ 1 ][ \"full_name\" ] = dfs [ 1 ][ \"given_name\" ] + \"_\" + dfs [ 1 ][ \"surname\" ] extended_model_settings = { ** basic_settings , \"blocking_rules_to_generate_predictions\" : blocking_rules , \"comparisons\" : [ { \"output_column_name\" : \"Full name\" , \"comparison_levels\" : [ { \"sql_condition\" : \"(given_name_l IS NULL OR given_name_r IS NULL) and (surname_l IS NULL OR surname_r IS NULL)\" , \"label_for_charts\" : \"Null\" , \"is_null_level\" : True , }, # full name match { \"sql_condition\" : \"full_name_l = full_name_r\" , \"label_for_charts\" : \"Both names match\" , \"tf_adjustment_column\" : \"full_name\" , \"tf_adjustment_weight\" : 1.0 , }, # typos - keep levels across full name rather than scoring separately { \"sql_condition\" : \"jaro_winkler_similarity(full_name_l, full_name_r) > 0.9\" , \"label_for_charts\" : \"JW (full name) > 0.9\" , }, { \"sql_condition\" : \"jaro_winkler_similarity(full_name_l, full_name_r) > 0.7\" , \"label_for_charts\" : \"JW (full name) > 0.7\" , }, # name switched { \"sql_condition\" : \"given_name_l = surname_r AND surname_l = given_name_r\" , \"label_for_charts\" : \"Names switched\" , }, # name switched + typo { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) >= 1.8\" , \"label_for_charts\" : \"switched + jaro_winkler_similarity >= 1.8\" }, { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) >= 1.4\" , \"label_for_charts\" : \"switched + jaro_winkler_similarity >= 1.4\" }, # single name match { \"sql_condition\" : \"given_name_l = given_name_r\" , \"label_for_charts\" : \"given name matches\" , \"tf_adjustment_column\" : \"given_name\" , \"tf_adjustment_weight\" : 1.0 , }, { \"sql_condition\" : \"surname_l = surname_r\" , \"label_for_charts\" : \"surname matches\" , \"tf_adjustment_column\" : \"surname\" , \"tf_adjustment_weight\" : 1.0 , }, # single name cross-match { \"sql_condition\" : \"given_name_l = surname_r OR surname_l = given_name_r\" , \"label_for_charts\" : \"single name cross-matches\" }, # single name typos { \"sql_condition\" : \"jaro_winkler_similarity(given_name_l, given_name_r) > 0.9\" , \"label_for_charts\" : \"JW (given name) > 0.9\" , }, { \"sql_condition\" : \"jaro_winkler_similarity(surname_l, surname_r) > 0.9\" , \"label_for_charts\" : \"JW (surname) > 0.9\" , }, # the rest cll . else_level () ] }, { \"output_column_name\" : \"Date of birth\" , \"comparison_levels\" : [ cll . null_level ( \"date_of_birth\" ), cll . exact_match_level ( \"date_of_birth\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"date_of_birth\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), # this is the extra level single_transposition_level ( \"date_of_birth\" ), cll . distance_function_level ( \"date_of_birth\" , \"levenshtein\" , 2 , higher_is_more_similar = False ), cll . else_level () ] }, { \"output_column_name\" : \"Social security ID\" , \"comparison_levels\" : [ cll . null_level ( \"soc_sec_id\" ), cll . exact_match_level ( \"soc_sec_id\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"soc_sec_id\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), single_transposition_level ( \"soc_sec_id\" ), cll . distance_function_level ( \"soc_sec_id\" , \"levenshtein\" , 2 , higher_is_more_similar = False ), cll . else_level () ] }, { \"output_column_name\" : \"Street number\" , \"comparison_levels\" : [ cll . null_level ( \"street_number\" ), cll . exact_match_level ( \"street_number\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"street_number\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), single_transposition_level ( \"street_number\" ), cll . else_level () ] }, { \"output_column_name\" : \"Postcode\" , \"comparison_levels\" : [ cll . null_level ( \"postcode\" ), cll . exact_match_level ( \"postcode\" , term_frequency_adjustments = True ), cll . distance_function_level ( \"postcode\" , \"levenshtein\" , 1 , higher_is_more_similar = False ), single_transposition_level ( \"postcode\" ), cll . distance_function_level ( \"postcode\" , \"levenshtein\" , 2 , higher_is_more_similar = False ), cll . else_level () ] }, # we don't consider further location columns as they will be strongly correlated with postcode ], \"retain_intermediate_calculation_columns\" : True , } # train linker_advanced = DuckDBLinker ( dfs , extended_model_settings ) linker_advanced . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.8 ) # we increase target rows to improve accuracy for u values in full name comparison, as we have subdivided the data more finely linker_advanced . estimate_u_using_random_sampling ( max_pairs = 1e8 ) session_dob = linker_advanced . estimate_parameters_using_expectation_maximisation ( \"l.date_of_birth = r.date_of_birth\" ) session_pc = linker_advanced . estimate_parameters_using_expectation_maximisation ( \"l.postcode = r.postcode\" ) Probability two random records match is estimated to be 0.000238. This means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match. With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - Full name (no m values are trained). - Date of birth (no m values are trained). - Social security ID (no m values are trained). - Street number (no m values are trained). - Postcode (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.date_of_birth = r.date_of_birth Parameter estimates will be made for the following comparison(s): - Full name - Social security ID - Street number - Postcode Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - Date of birth Iteration 1: Largest change in params was -0.484 in the m_probability of Full name, level `Both names match` Iteration 2: Largest change in params was 0.00198 in probability_two_random_records_match Iteration 3: Largest change in params was 8.13e-06 in probability_two_random_records_match EM converged after 3 iterations Your model is not yet fully trained. Missing estimates for: - Date of birth (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.postcode = r.postcode Parameter estimates will be made for the following comparison(s): - Full name - Date of birth - Social security ID - Street number Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - Postcode Iteration 1: Largest change in params was 0.0333 in the m_probability of Date of birth, level `All other comparisons` Iteration 2: Largest change in params was 0.000465 in the m_probability of Date of birth, level `All other comparisons` Iteration 3: Largest change in params was 1.18e-05 in the m_probability of Social security ID, level `All other comparisons` EM converged after 3 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values # linker_advanced.parameter_estimate_comparisons_chart() linker_advanced . match_weights_chart () predictions_adv = linker_advanced . predict () df_predictions_adv = predictions_adv . as_pandas_dataframe () clusters_adv = linker_advanced . cluster_pairwise_predictions_at_threshold ( predictions_adv , threshold_match_probability = 0.99 ) df_clusters_adv = clusters_adv . as_pandas_dataframe () . sort_values ( \"cluster_id\" ) df_clusters_adv . groupby ( \"cluster_id\" ) . size () . value_counts () Completed iteration 1, root rows count 0 2 4970 1 60 dtype: int64 This is a pretty modest improvement on our previous model - however it is worth re-iterating that we should not necessarily expect to recover all matches, as in several cases it may be unreasonable for a model to have reasonable confidence that two records refer to the same entity. If we wished to improve matters we could iterate on this process - investigating where our model is not performing as we would hope, and seeing how we can adjust these areas to address these shortcomings.","title":"Further refinements"},{"location":"demos/example_link_only.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linking without deduplication \u00b6 A simple record linkage model using the link_only link type . import pandas as pd df_l = pd . read_parquet ( \"./data/fake_df_l.parquet\" ) df_r = pd . read_parquet ( \"./data/fake_df_r.parquet\" ) df_l . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email group 0 0 Julia None 2015-10-29 London hannah88@powers.com 0 1 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"link_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . levenshtein_at_thresholds ( \"surname\" ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], } linker = DuckDBLinker ([ df_l , df_r ], settings , input_table_aliases = [ \"df_left\" , \"df_right\" ]) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) Probability two random records match is estimated to be 0.00582. This means that amongst all possible pairwise record comparisons, one in 171.80 are expected to match. With 148,239 total possible comparisons, we expect a total of around 862.86 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). session_dob = linker . estimate_parameters_using_expectation_maximisation ( \"l.dob = r.dob\" ) session_email = linker . estimate_parameters_using_expectation_maximisation ( \"l.email = r.email\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.388 in the m_probability of first_name, level `Exact match` Iteration 2: Largest change in params was 0.21 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0695 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0254 in probability_two_random_records_match Iteration 5: Largest change in params was 0.0121 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00674 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00414 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00271 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00185 in probability_two_random_records_match Iteration 10: Largest change in params was 0.0013 in probability_two_random_records_match Iteration 11: Largest change in params was 0.000935 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000682 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000503 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000375 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000281 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000211 in probability_two_random_records_match Iteration 17: Largest change in params was 0.00016 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000121 in probability_two_random_records_match Iteration 19: Largest change in params was 9.17e-05 in probability_two_random_records_match EM converged after 19 iterations Your model is not yet fully trained. Missing estimates for: - dob (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.email = r.email Parameter estimates will be made for the following comparison(s): - first_name - surname - dob - city Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - email Iteration 1: Largest change in params was 0.417 in probability_two_random_records_match Iteration 2: Largest change in params was 0.101 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0254 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0102 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00526 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00312 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00204 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00142 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00104 in probability_two_random_records_match Iteration 10: Largest change in params was 0.000784 in probability_two_random_records_match Iteration 11: Largest change in params was 0.000609 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000485 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000392 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000322 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000268 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000225 in probability_two_random_records_match Iteration 17: Largest change in params was 0.000191 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000163 in probability_two_random_records_match Iteration 19: Largest change in params was 0.00014 in probability_two_random_records_match Iteration 20: Largest change in params was 0.00012 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000105 in probability_two_random_records_match Iteration 22: Largest change in params was 9.1e-05 in probability_two_random_records_match EM converged after 22 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values results = linker . predict ( threshold_match_probability = 0.9 ) results . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability source_dataset_l unique_id_l source_dataset_r unique_id_r first_name_l first_name_r gamma_first_name surname_l ... dob_l dob_r gamma_dob city_l city_r gamma_city email_l email_r gamma_email match_key 0 4.875607 0.967058 df_left 0 df_right 1 Julia Julia 2 NaN ... 2015-10-29 2015-07-31 0 London London 1 hannah88@powers.com hannah88@powers.com 3 0 1 4.875607 0.967058 df_left 0 df_right 2 Julia Julia 2 NaN ... 2015-10-29 2016-01-27 0 London London 1 hannah88@powers.com hannah88@powers.com 3 0 2 4.840904 0.966283 df_left 27 df_right 28 Matilda Matilda 2 Hsrir ... 1983-04-30 1983-04-30 3 London London 1 patrcio47@davis.cam patricia47@davis.com 0 0 3 15.216464 0.999974 df_left 32 df_right 34 Baxter Baxter 2 Aria ... 1992-09-07 1992-09-30 1 London London 1 christineshepherd@allen.com christineshepherd@allen.com 3 0 4 18.083396 0.999996 df_left 38 df_right 39 Olivia Olivia 2 Andrews ... 2009-01-23 2009-01-23 3 NaN London -1 hesterkurt@taylor-fitzgerald.com hesterkurt@taylor-fitzgerald.com 3 0 5 rows \u00d7 22 columns","title":"Linking two tables of persons"},{"location":"demos/example_link_only.html#linking-without-deduplication","text":"A simple record linkage model using the link_only link type . import pandas as pd df_l = pd . read_parquet ( \"./data/fake_df_l.parquet\" ) df_r = pd . read_parquet ( \"./data/fake_df_r.parquet\" ) df_l . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email group 0 0 Julia None 2015-10-29 London hannah88@powers.com 0 1 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"link_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . levenshtein_at_thresholds ( \"surname\" ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], } linker = DuckDBLinker ([ df_l , df_r ], settings , input_table_aliases = [ \"df_left\" , \"df_right\" ]) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) Probability two random records match is estimated to be 0.00582. This means that amongst all possible pairwise record comparisons, one in 171.80 are expected to match. With 148,239 total possible comparisons, we expect a total of around 862.86 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). session_dob = linker . estimate_parameters_using_expectation_maximisation ( \"l.dob = r.dob\" ) session_email = linker . estimate_parameters_using_expectation_maximisation ( \"l.email = r.email\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.388 in the m_probability of first_name, level `Exact match` Iteration 2: Largest change in params was 0.21 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0695 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0254 in probability_two_random_records_match Iteration 5: Largest change in params was 0.0121 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00674 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00414 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00271 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00185 in probability_two_random_records_match Iteration 10: Largest change in params was 0.0013 in probability_two_random_records_match Iteration 11: Largest change in params was 0.000935 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000682 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000503 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000375 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000281 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000211 in probability_two_random_records_match Iteration 17: Largest change in params was 0.00016 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000121 in probability_two_random_records_match Iteration 19: Largest change in params was 9.17e-05 in probability_two_random_records_match EM converged after 19 iterations Your model is not yet fully trained. Missing estimates for: - dob (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.email = r.email Parameter estimates will be made for the following comparison(s): - first_name - surname - dob - city Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - email Iteration 1: Largest change in params was 0.417 in probability_two_random_records_match Iteration 2: Largest change in params was 0.101 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0254 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0102 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00526 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00312 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00204 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00142 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00104 in probability_two_random_records_match Iteration 10: Largest change in params was 0.000784 in probability_two_random_records_match Iteration 11: Largest change in params was 0.000609 in probability_two_random_records_match Iteration 12: Largest change in params was 0.000485 in probability_two_random_records_match Iteration 13: Largest change in params was 0.000392 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000322 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000268 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000225 in probability_two_random_records_match Iteration 17: Largest change in params was 0.000191 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000163 in probability_two_random_records_match Iteration 19: Largest change in params was 0.00014 in probability_two_random_records_match Iteration 20: Largest change in params was 0.00012 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000105 in probability_two_random_records_match Iteration 22: Largest change in params was 9.1e-05 in probability_two_random_records_match EM converged after 22 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values results = linker . predict ( threshold_match_probability = 0.9 ) results . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability source_dataset_l unique_id_l source_dataset_r unique_id_r first_name_l first_name_r gamma_first_name surname_l ... dob_l dob_r gamma_dob city_l city_r gamma_city email_l email_r gamma_email match_key 0 4.875607 0.967058 df_left 0 df_right 1 Julia Julia 2 NaN ... 2015-10-29 2015-07-31 0 London London 1 hannah88@powers.com hannah88@powers.com 3 0 1 4.875607 0.967058 df_left 0 df_right 2 Julia Julia 2 NaN ... 2015-10-29 2016-01-27 0 London London 1 hannah88@powers.com hannah88@powers.com 3 0 2 4.840904 0.966283 df_left 27 df_right 28 Matilda Matilda 2 Hsrir ... 1983-04-30 1983-04-30 3 London London 1 patrcio47@davis.cam patricia47@davis.com 0 0 3 15.216464 0.999974 df_left 32 df_right 34 Baxter Baxter 2 Aria ... 1992-09-07 1992-09-30 1 London London 1 christineshepherd@allen.com christineshepherd@allen.com 3 0 4 18.083396 0.999996 df_left 38 df_right 39 Olivia Olivia 2 Andrews ... 2009-01-23 2009-01-23 3 NaN London -1 hesterkurt@taylor-fitzgerald.com hesterkurt@taylor-fitzgerald.com 3 0 5 rows \u00d7 22 columns","title":"Linking without deduplication"},{"location":"demos/example_pairwise_labels.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Estimating m from a sample of pairwise labels \u00b6 In this example, we estimate the m probabilities of the model from a table containing pairwise record comparisons which we know are 'true' matches. For example, these may be the result of work by a clerical team who have manually labelled a sample of matches. The table must be in the following format: source_dataset_l unique_id_l source_dataset_r unique_id_r df_1 1 df_2 2 df_1 1 df_2 3 It is assumed that every record in the table represents a certain match. Note that the column names above are the defaults. They should correspond to the values you've set for unique_id_column_name and source_dataset_column_name , if you've chosen custom values. import pandas as pd import altair as alt alt . renderers . enable ( \"mimetype\" ) RendererRegistry.enable('mimetype') pairwise_labels = pd . read_csv ( \"./data/pairwise_labels_to_estimate_m.csv\" ) pairwise_labels .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id_l source_dataset_l unique_id_r source_dataset_r 0 0 fake_1000 3 fake_1000 1 1 fake_1000 3 fake_1000 2 2 fake_1000 3 fake_1000 3 4 fake_1000 5 fake_1000 4 7 fake_1000 10 fake_1000 ... ... ... ... ... 2026 978 fake_1000 979 fake_1000 2027 985 fake_1000 986 fake_1000 2028 624 fake_1000 626 fake_1000 2029 625 fake_1000 626 fake_1000 2030 624 fake_1000 625 fake_1000 2031 rows \u00d7 4 columns We now proceed to estimate the Fellegi Sunter model: df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . levenshtein_at_thresholds ( \"surname\" , 2 ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) # Register the pairwise labels table with the database, and then use it to estimate the m values labels_df = linker . register_labels_table ( pairwise_labels , overwrite = True ) linker . estimate_m_from_pairwise_labels ( labels_df ) # Not if the labels table already existing in the dataset you could run # linker.estimate_m_from_pairwise_labels(\"labels_tablename_here\") training_blocking_rule = \"l.first_name = r.first_name\" linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) <EMTrainingSession, blocking on l.first_name = r.first_name, deactivating comparisons first_name> linker . parameter_estimate_comparisons_chart () linker . match_weights_chart ()","title":"Estimating m probabilities from labels"},{"location":"demos/example_pairwise_labels.html#estimating-m-from-a-sample-of-pairwise-labels","text":"In this example, we estimate the m probabilities of the model from a table containing pairwise record comparisons which we know are 'true' matches. For example, these may be the result of work by a clerical team who have manually labelled a sample of matches. The table must be in the following format: source_dataset_l unique_id_l source_dataset_r unique_id_r df_1 1 df_2 2 df_1 1 df_2 3 It is assumed that every record in the table represents a certain match. Note that the column names above are the defaults. They should correspond to the values you've set for unique_id_column_name and source_dataset_column_name , if you've chosen custom values. import pandas as pd import altair as alt alt . renderers . enable ( \"mimetype\" ) RendererRegistry.enable('mimetype') pairwise_labels = pd . read_csv ( \"./data/pairwise_labels_to_estimate_m.csv\" ) pairwise_labels .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id_l source_dataset_l unique_id_r source_dataset_r 0 0 fake_1000 3 fake_1000 1 1 fake_1000 3 fake_1000 2 2 fake_1000 3 fake_1000 3 4 fake_1000 5 fake_1000 4 7 fake_1000 10 fake_1000 ... ... ... ... ... 2026 978 fake_1000 979 fake_1000 2027 985 fake_1000 986 fake_1000 2028 624 fake_1000 626 fake_1000 2029 625 fake_1000 626 fake_1000 2030 624 fake_1000 625 fake_1000 2031 rows \u00d7 4 columns We now proceed to estimate the Fellegi Sunter model: df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . levenshtein_at_thresholds ( \"surname\" , 2 ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , } linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.7 ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) # Register the pairwise labels table with the database, and then use it to estimate the m values labels_df = linker . register_labels_table ( pairwise_labels , overwrite = True ) linker . estimate_m_from_pairwise_labels ( labels_df ) # Not if the labels table already existing in the dataset you could run # linker.estimate_m_from_pairwise_labels(\"labels_tablename_here\") training_blocking_rule = \"l.first_name = r.first_name\" linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) <EMTrainingSession, blocking on l.first_name = r.first_name, deactivating comparisons first_name> linker . parameter_estimate_comparisons_chart () linker . match_weights_chart ()","title":"Estimating m from a sample of pairwise labels"},{"location":"demos/example_quick_and_dirty_persons.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Historical people: Quick and dirty \u00b6 This example shows how to get some initial record linkage results as quickly as possible. There are many ways to improve the accuracy of this model. But this may be a good place to start if you just want to give Splink a try and see what it's capable of. import pandas as pd df = pd . read_parquet ( \"./data/historical_figures_with_errors_50k.parquet\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uncorrupted_record cluster full_name dob birth_place postcode_fake lat lng gender occupation unique_id 0 True Q2296770 thomas clifford, 1st baron clifford of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-1 1 False Q2296770 thomas of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-2 2 False Q2296770 tom 1st baron clifford of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-3 3 False Q2296770 thomas 1st chudleigh 1630-08-01 Devon TQ13 8HU 50.687638 -3.895877 None politician Q2296770-4 4 False Q2296770 thomas clifford, 1st baron chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 None politician Q2296770-5 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.full_name = r.full_name\" , \"substr(l.full_name,1,6) = substr(r.full_name,1,6) and l.dob = r.dob and l.birth_place = r.birth_place\" , \"l.dob = r.dob and l.birth_place = r.birth_place\" , \"l.postcode_fake = r.postcode_fake\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"full_name\" , [ 1 , 3 , 5 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"postcode_fake\" , 2 ), cl . exact_match ( \"birth_place\" , term_frequency_adjustments = True ), cl . exact_match ( \"occupation\" , term_frequency_adjustments = True ), ], } linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) deterministic_rules = [ \"l.full_name = r.full_name\" , \"l.postcode_fake = r.postcode_fake and l.dob = r.dob\" , ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.6 ) linker . estimate_u_using_random_sampling ( max_pairs = 2e6 ) results = linker . predict ( threshold_match_probability = 0.9 ) -- WARNING -- You have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary. To produce predictions the following untrained trained parameters will use default values. Comparison: 'full_name': m values not fully trained Comparison: 'dob': m values not fully trained Comparison: 'postcode_fake': m values not fully trained Comparison: 'birth_place': m values not fully trained Comparison: 'occupation': m values not fully trained results . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r full_name_l full_name_r gamma_full_name dob_l dob_r gamma_dob postcode_fake_l postcode_fake_r gamma_postcode_fake birth_place_l birth_place_r gamma_birth_place occupation_l occupation_r gamma_occupation match_key 0 31.481528 1.000000 Q90404618-1 Q90404618-3 emlie clifford emlie clifford 4 1861-01-01 1861-01-01 3 WR11 7QP WR11 7QW 1 Wychavon Wychavon 1 playwright playwright 1 0 1 31.481528 1.000000 Q90404618-2 Q90404618-3 emlie clifford emlie clifford 4 1861-01-01 1861-01-01 3 WR11 7QP WR11 7QW 1 Wychavon Wychavon 1 playwright playwright 1 0 2 14.090741 0.999943 Q2516590-3 Q2516590-9 william watts william watts 4 1860-06-07 NaN -1 SY5 7NT SY5 7NT 2 Shropshire NaN -1 geologist NaN -1 0 3 54.751297 1.000000 Q631006-1 Q631006-2 moses gaster moses gaster 4 1856-09-17 1856-09-17 3 EX20 3PZ EX20 3PZ 2 Bucharest Bucharest 1 rabbi rabbi 1 0 4 21.428205 1.000000 Q7795446-2 Q7795446-3 thomas barry thomas barry 4 1560-01-01 1560-01-01 3 CF14 5GH CF14 6TQ 0 Cardiff Cardiff 1 judge judge 1 0","title":"Quick and dirty persons model"},{"location":"demos/example_quick_and_dirty_persons.html#historical-people-quick-and-dirty","text":"This example shows how to get some initial record linkage results as quickly as possible. There are many ways to improve the accuracy of this model. But this may be a good place to start if you just want to give Splink a try and see what it's capable of. import pandas as pd df = pd . read_parquet ( \"./data/historical_figures_with_errors_50k.parquet\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uncorrupted_record cluster full_name dob birth_place postcode_fake lat lng gender occupation unique_id 0 True Q2296770 thomas clifford, 1st baron clifford of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-1 1 False Q2296770 thomas of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-2 2 False Q2296770 tom 1st baron clifford of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-3 3 False Q2296770 thomas 1st chudleigh 1630-08-01 Devon TQ13 8HU 50.687638 -3.895877 None politician Q2296770-4 4 False Q2296770 thomas clifford, 1st baron chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 None politician Q2296770-5 from splink.duckdb.duckdb_linker import DuckDBLinker from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.full_name = r.full_name\" , \"substr(l.full_name,1,6) = substr(r.full_name,1,6) and l.dob = r.dob and l.birth_place = r.birth_place\" , \"l.dob = r.dob and l.birth_place = r.birth_place\" , \"l.postcode_fake = r.postcode_fake\" , ], \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"full_name\" , [ 1 , 3 , 5 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"postcode_fake\" , 2 ), cl . exact_match ( \"birth_place\" , term_frequency_adjustments = True ), cl . exact_match ( \"occupation\" , term_frequency_adjustments = True ), ], } linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) deterministic_rules = [ \"l.full_name = r.full_name\" , \"l.postcode_fake = r.postcode_fake and l.dob = r.dob\" , ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.6 ) linker . estimate_u_using_random_sampling ( max_pairs = 2e6 ) results = linker . predict ( threshold_match_probability = 0.9 ) -- WARNING -- You have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary. To produce predictions the following untrained trained parameters will use default values. Comparison: 'full_name': m values not fully trained Comparison: 'dob': m values not fully trained Comparison: 'postcode_fake': m values not fully trained Comparison: 'birth_place': m values not fully trained Comparison: 'occupation': m values not fully trained results . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r full_name_l full_name_r gamma_full_name dob_l dob_r gamma_dob postcode_fake_l postcode_fake_r gamma_postcode_fake birth_place_l birth_place_r gamma_birth_place occupation_l occupation_r gamma_occupation match_key 0 31.481528 1.000000 Q90404618-1 Q90404618-3 emlie clifford emlie clifford 4 1861-01-01 1861-01-01 3 WR11 7QP WR11 7QW 1 Wychavon Wychavon 1 playwright playwright 1 0 1 31.481528 1.000000 Q90404618-2 Q90404618-3 emlie clifford emlie clifford 4 1861-01-01 1861-01-01 3 WR11 7QP WR11 7QW 1 Wychavon Wychavon 1 playwright playwright 1 0 2 14.090741 0.999943 Q2516590-3 Q2516590-9 william watts william watts 4 1860-06-07 NaN -1 SY5 7NT SY5 7NT 2 Shropshire NaN -1 geologist NaN -1 0 3 54.751297 1.000000 Q631006-1 Q631006-2 moses gaster moses gaster 4 1856-09-17 1856-09-17 3 EX20 3PZ EX20 3PZ 2 Bucharest Bucharest 1 rabbi rabbi 1 0 4 21.428205 1.000000 Q7795446-2 Q7795446-3 thomas barry thomas barry 4 1560-01-01 1560-01-01 3 CF14 5GH CF14 6TQ 0 Cardiff Cardiff 1 judge judge 1 0","title":"Historical people: Quick and dirty"},{"location":"demos/example_real_time_record_linkage.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Real time linkage \u00b6 In this notebook, we demonstrate splink's incremental and real time linkage capabilities - specifically: - the linker.compare_two_records function, that allows you to interactively explore the results of a linkage model; and - the linker.find_matches_to_new_records that allows you to incrementally find matches to a small number of new records Step 1: Load a pre-trained linkage model \u00b6 import pandas as pd import json from splink.duckdb.duckdb_linker import DuckDBLinker import altair as alt alt . renderers . enable ( 'mimetype' ) with open ( \"demo_settings/real_time_settings.json\" ) as f : trained_settings = json . load ( f ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df , trained_settings ) linker . waterfall_chart ( linker . predict () . as_record_dict ( limit = 2 )) Step Comparing two records \u00b6 It's now possible to compute a match weight for any two records using linker.compare_two_records() from splink.term_frequencies import compute_term_frequencies_from_concat_with_tf record_1 = { 'unique_id' : 1 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1984-01-02\" , 'city' : \"London\" , 'email' : \"lucas.smith@hotmail.com\" } record_2 = { 'unique_id' : 2 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1983-02-12\" , 'city' : \"Machester\" , 'email' : \"lucas.smith@hotmail.com\" } linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True linker . compute_tf_table ( \"first_name\" ) linker . compute_tf_table ( \"surname\" ) linker . compute_tf_table ( \"dob\" ) linker . compute_tf_table ( \"city\" ) linker . compute_tf_table ( \"email\" ) df_two = linker . compare_two_records ( record_1 , record_2 ) df_two . as_pandas_dataframe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 0 13.161672 0.999891 1 2 Lucas Lucas 2 0.001203 0.001203 87.571229 ... NaN 0.446404 1.0 lucas.smith@hotmail.com lucas.smith@hotmail.com 1 NaN NaN 263.229168 1.0 1 rows \u00d7 39 columns Step 3: Interactive comparisons \u00b6 One interesting applicatin of compare_two_records is to create a simple interface that allows the user to input two records interactively, and get real time feedback. In the following cell we use ipywidets for this purpose. \u2728\u2728 Change the values in the text boxes to see the waterfall chart update in real time. \u2728\u2728 import ipywidgets as widgets fields = [ \"unique_id\" , \"first_name\" , \"surname\" , \"dob\" , \"email\" , \"city\" ] left_text_boxes = [] right_text_boxes = [] inputs_to_interactive_output = {} for f in fields : wl = widgets . Text ( description = f , value = str ( record_1 [ f ])) left_text_boxes . append ( wl ) inputs_to_interactive_output [ f \" { f } _l\" ] = wl wr = widgets . Text ( description = f , value = str ( record_2 [ f ])) right_text_boxes . append ( wr ) inputs_to_interactive_output [ f \" { f } _r\" ] = wr b1 = widgets . VBox ( left_text_boxes ) b2 = widgets . VBox ( right_text_boxes ) ui = widgets . HBox ([ b1 , b2 ]) def myfn ( ** kwargs ): my_args = dict ( kwargs ) record_left = {} record_right = {} for key , value in my_args . items (): if value == '' : value = None if key . endswith ( \"_l\" ): record_left [ key [: - 2 ]] = value if key . endswith ( \"_r\" ): record_right [ key [: - 2 ]] = value linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True df_two = linker . compare_two_records ( record_left , record_right ) recs = df_two . as_pandas_dataframe () . to_dict ( orient = \"records\" ) from splink.charts import waterfall_chart display ( linker . waterfall_chart ( recs , filter_nulls = False )) out = widgets . interactive_output ( myfn , inputs_to_interactive_output ) display ( ui , out ) HBox(children=(VBox(children=(Text(value='1', description='unique_id'), Text(value='Lucas', description='first\u2026 Output() Finding matching records interactively \u00b6 It is also possible to search the records in the input dataset rapidly using the linker.find_matches_to_new_records() function record = { 'unique_id' : 123987 , 'first_name' : \"Robert\" , 'surname' : \"Alan\" , 'dob' : \"1971-05-24\" , 'city' : \"London\" , 'email' : \"robert255@smith.net\" } df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = []) . as_pandas_dataframe () df_inc . sort_values ( \"match_weight\" , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 3 23.531793 1.000000 0 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 robert255@smith.net robert255@smith.net 1 0.001267 0.001267 263.229168 1.730964 4 14.550320 0.999958 1 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 2 10.388623 0.999255 3 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 0.446404 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 1 2.427256 0.843228 2 123987 Rob Robert 0 0.001203 0.00361 0.218767 ... 0.212792 10.484859 0.259162 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 5 -2.123090 0.186697 8 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 6 -2.205894 0.178139 754 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 j.c@whige.wort robert255@smith.net 0 0.001267 0.001267 0.423438 1.000000 0 -2.802309 0.125383 750 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 10.484859 0.259162 j.c@white.org robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 7 rows \u00d7 39 columns Interactive interface for finding records \u00b6 Again, we can use ipywidgets to build an interactive interface for the linker.find_matches_to_new_records function from splink.charts import waterfall_chart @widgets . interact ( first_name = 'Robert' , surname = \"Alan\" , dob = \"1971-05-24\" , city = \"London\" , email = \"robert255@smith.net\" ) def interactive_link ( first_name , surname , dob , city , email ): record = { 'unique_id' : 123987 , 'first_name' : first_name , 'surname' : surname , 'dob' : dob , 'city' : city , 'email' : email , 'group' : 0 } for key in record . keys (): if type ( record [ key ]) == str : if record [ key ] . strip () == \"\" : record [ key ] = None df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = [ f \"(true)\" ]) . as_pandas_dataframe () df_inc = df_inc . sort_values ( \"match_weight\" , ascending = False ) recs = df_inc . to_dict ( orient = \"records\" ) display ( linker . waterfall_chart ( recs , filter_nulls = False )) interactive(children=(Text(value='Robert', description='first_name'), Text(value='Alan', description='surname'\u2026 linker . match_weights_chart ()","title":"Real time record linkage"},{"location":"demos/example_real_time_record_linkage.html#real-time-linkage","text":"In this notebook, we demonstrate splink's incremental and real time linkage capabilities - specifically: - the linker.compare_two_records function, that allows you to interactively explore the results of a linkage model; and - the linker.find_matches_to_new_records that allows you to incrementally find matches to a small number of new records","title":"Real time linkage"},{"location":"demos/example_real_time_record_linkage.html#step-1-load-a-pre-trained-linkage-model","text":"import pandas as pd import json from splink.duckdb.duckdb_linker import DuckDBLinker import altair as alt alt . renderers . enable ( 'mimetype' ) with open ( \"demo_settings/real_time_settings.json\" ) as f : trained_settings = json . load ( f ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df , trained_settings ) linker . waterfall_chart ( linker . predict () . as_record_dict ( limit = 2 ))","title":"Step 1: Load a pre-trained linkage model"},{"location":"demos/example_real_time_record_linkage.html#step-comparing-two-records","text":"It's now possible to compute a match weight for any two records using linker.compare_two_records() from splink.term_frequencies import compute_term_frequencies_from_concat_with_tf record_1 = { 'unique_id' : 1 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1984-01-02\" , 'city' : \"London\" , 'email' : \"lucas.smith@hotmail.com\" } record_2 = { 'unique_id' : 2 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1983-02-12\" , 'city' : \"Machester\" , 'email' : \"lucas.smith@hotmail.com\" } linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True linker . compute_tf_table ( \"first_name\" ) linker . compute_tf_table ( \"surname\" ) linker . compute_tf_table ( \"dob\" ) linker . compute_tf_table ( \"city\" ) linker . compute_tf_table ( \"email\" ) df_two = linker . compare_two_records ( record_1 , record_2 ) df_two . as_pandas_dataframe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 0 13.161672 0.999891 1 2 Lucas Lucas 2 0.001203 0.001203 87.571229 ... NaN 0.446404 1.0 lucas.smith@hotmail.com lucas.smith@hotmail.com 1 NaN NaN 263.229168 1.0 1 rows \u00d7 39 columns","title":"Step  Comparing two records"},{"location":"demos/example_real_time_record_linkage.html#step-3-interactive-comparisons","text":"One interesting applicatin of compare_two_records is to create a simple interface that allows the user to input two records interactively, and get real time feedback. In the following cell we use ipywidets for this purpose. \u2728\u2728 Change the values in the text boxes to see the waterfall chart update in real time. \u2728\u2728 import ipywidgets as widgets fields = [ \"unique_id\" , \"first_name\" , \"surname\" , \"dob\" , \"email\" , \"city\" ] left_text_boxes = [] right_text_boxes = [] inputs_to_interactive_output = {} for f in fields : wl = widgets . Text ( description = f , value = str ( record_1 [ f ])) left_text_boxes . append ( wl ) inputs_to_interactive_output [ f \" { f } _l\" ] = wl wr = widgets . Text ( description = f , value = str ( record_2 [ f ])) right_text_boxes . append ( wr ) inputs_to_interactive_output [ f \" { f } _r\" ] = wr b1 = widgets . VBox ( left_text_boxes ) b2 = widgets . VBox ( right_text_boxes ) ui = widgets . HBox ([ b1 , b2 ]) def myfn ( ** kwargs ): my_args = dict ( kwargs ) record_left = {} record_right = {} for key , value in my_args . items (): if value == '' : value = None if key . endswith ( \"_l\" ): record_left [ key [: - 2 ]] = value if key . endswith ( \"_r\" ): record_right [ key [: - 2 ]] = value linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True df_two = linker . compare_two_records ( record_left , record_right ) recs = df_two . as_pandas_dataframe () . to_dict ( orient = \"records\" ) from splink.charts import waterfall_chart display ( linker . waterfall_chart ( recs , filter_nulls = False )) out = widgets . interactive_output ( myfn , inputs_to_interactive_output ) display ( ui , out ) HBox(children=(VBox(children=(Text(value='1', description='unique_id'), Text(value='Lucas', description='first\u2026 Output()","title":"Step 3: Interactive comparisons"},{"location":"demos/example_real_time_record_linkage.html#finding-matching-records-interactively","text":"It is also possible to search the records in the input dataset rapidly using the linker.find_matches_to_new_records() function record = { 'unique_id' : 123987 , 'first_name' : \"Robert\" , 'surname' : \"Alan\" , 'dob' : \"1971-05-24\" , 'city' : \"London\" , 'email' : \"robert255@smith.net\" } df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = []) . as_pandas_dataframe () df_inc . sort_values ( \"match_weight\" , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 3 23.531793 1.000000 0 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 robert255@smith.net robert255@smith.net 1 0.001267 0.001267 263.229168 1.730964 4 14.550320 0.999958 1 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 2 10.388623 0.999255 3 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 0.446404 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 1 2.427256 0.843228 2 123987 Rob Robert 0 0.001203 0.00361 0.218767 ... 0.212792 10.484859 0.259162 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 5 -2.123090 0.186697 8 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 6 -2.205894 0.178139 754 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 j.c@whige.wort robert255@smith.net 0 0.001267 0.001267 0.423438 1.000000 0 -2.802309 0.125383 750 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 10.484859 0.259162 j.c@white.org robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 7 rows \u00d7 39 columns","title":"Finding matching records interactively"},{"location":"demos/example_real_time_record_linkage.html#interactive-interface-for-finding-records","text":"Again, we can use ipywidgets to build an interactive interface for the linker.find_matches_to_new_records function from splink.charts import waterfall_chart @widgets . interact ( first_name = 'Robert' , surname = \"Alan\" , dob = \"1971-05-24\" , city = \"London\" , email = \"robert255@smith.net\" ) def interactive_link ( first_name , surname , dob , city , email ): record = { 'unique_id' : 123987 , 'first_name' : first_name , 'surname' : surname , 'dob' : dob , 'city' : city , 'email' : email , 'group' : 0 } for key in record . keys (): if type ( record [ key ]) == str : if record [ key ] . strip () == \"\" : record [ key ] = None df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = [ f \"(true)\" ]) . as_pandas_dataframe () df_inc = df_inc . sort_values ( \"match_weight\" , ascending = False ) recs = df_inc . to_dict ( orient = \"records\" ) display ( linker . waterfall_chart ( recs , filter_nulls = False )) interactive(children=(Text(value='Robert', description='first_name'), Text(value='Alan', description='surname'\u2026 linker . match_weights_chart ()","title":"Interactive interface for finding records"},{"location":"demos/example_simple_pyspark.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linking in Spark \u00b6 from splink.spark.jar_location import similarity_jar_location from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession from pyspark.sql import types conf = SparkConf () # This parallelism setting is only suitable for a small toy example conf . set ( \"spark.driver.memory\" , \"12g\" ) conf . set ( \"spark.default.parallelism\" , \"16\" ) # Add custom similarity functions, which are bundled with Splink # documented here: https://github.com/moj-analytical-services/splink_scalaudfs path = similarity_jar_location () conf . set ( \"spark.jars\" , path ) sc = SparkContext . getOrCreate ( conf = conf ) spark = SparkSession ( sc ) spark . sparkContext . setCheckpointDir ( \"./tmp_checkpoints\" ) 23/02/04 19:37:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). import pandas as pd df = spark . read . csv ( \"./data/fake_1000.csv\" , header = True ) import splink.spark.spark_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"comparisons\" : [ cl . jaro_winkler_at_thresholds ( \"first_name\" , 0.8 ), cl . jaro_winkler_at_thresholds ( \"surname\" , 0.8 ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"em_convergence\" : 0.01 } from splink.spark.spark_linker import SparkLinker linker = SparkLinker ( df , settings ) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.6 ) Probability two random records match is estimated to be 0.00389. This means that amongst all possible pairwise record comparisons, one in 257.25 are expected to match. With 499,500 total possible comparisons, we expect a total of around 1,941.67 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 5e5 ) ----- Estimating u probabilities using random sampling ----- 23/02/04 19:37:25 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_concat_with_tf_781a41b44 Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_fname_sname = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) training_blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname 23/02/04 19:37:31 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_comparison_vectors_b3b0f8be8 Iteration 1: Largest change in params was -0.53 in the m_probability of dob, level `Exact match` Iteration 2: Largest change in params was 0.0335 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0129 in probability_two_random_records_match Iteration 4: Largest change in params was 0.00639 in probability_two_random_records_match EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob 23/02/04 19:37:37 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_comparison_vectors_5d2301fd1 Iteration 1: Largest change in params was -0.413 in the m_probability of surname, level `Exact match` Iteration 2: Largest change in params was 0.108 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0348 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0133 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00593 in probability_two_random_records_match EM converged after 5 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values results = linker . predict ( threshold_match_probability = 0.9 ) 23/02/04 19:37:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'. 23/02/04 19:37:46 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_predict_0978bb8e9 results . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 3.371739 0.911904 220 223 Logan Logan 2 86.748396 serguFon Ferguson ... 1 0.212792 0.212792 10.316002 0.259162 l.feruson46@sahh.com None -1 1.000000 0 1 14.743406 0.999964 879 880 Leo Leo 2 86.748396 Webster Webster ... 1 0.008610 0.008610 10.316002 6.404996 leo.webster54@moore.biez None -1 1.000000 0 2 13.140266 0.999889 446 450 Aisha Aisha 2 86.748396 Bryant None ... 0 0.011070 0.001230 0.456259 1.000000 aishab64@obrien-flores.com aishab64@obrien-flores.com 3 257.458944 0 3 8.829126 0.997806 446 448 Aisha Aisha 2 86.748396 Bryant BryBant ... 0 0.011070 0.001230 0.456259 1.000000 aishab64@obrien-flores.com aishab64@obrien-flores.com 3 257.458944 0 4 6.584844 0.989690 790 791 Jackson Jackson 2 86.748396 Fisreh Fishier ... 0 0.009840 0.001230 0.456259 1.000000 j.fisher4@sullivan.com None -1 1.000000 0 5 rows \u00d7 28 columns","title":"Deduplication using Pyspark"},{"location":"demos/example_simple_pyspark.html#linking-in-spark","text":"from splink.spark.jar_location import similarity_jar_location from pyspark import SparkContext , SparkConf from pyspark.sql import SparkSession from pyspark.sql import types conf = SparkConf () # This parallelism setting is only suitable for a small toy example conf . set ( \"spark.driver.memory\" , \"12g\" ) conf . set ( \"spark.default.parallelism\" , \"16\" ) # Add custom similarity functions, which are bundled with Splink # documented here: https://github.com/moj-analytical-services/splink_scalaudfs path = similarity_jar_location () conf . set ( \"spark.jars\" , path ) sc = SparkContext . getOrCreate ( conf = conf ) spark = SparkSession ( sc ) spark . sparkContext . setCheckpointDir ( \"./tmp_checkpoints\" ) 23/02/04 19:37:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). import pandas as pd df = spark . read . csv ( \"./data/fake_1000.csv\" , header = True ) import splink.spark.spark_comparison_library as cl settings = { \"link_type\" : \"dedupe_only\" , \"comparisons\" : [ cl . jaro_winkler_at_thresholds ( \"first_name\" , 0.8 ), cl . jaro_winkler_at_thresholds ( \"surname\" , 0.8 ), cl . levenshtein_at_thresholds ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . levenshtein_at_thresholds ( \"email\" ), ], \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"em_convergence\" : 0.01 } from splink.spark.spark_linker import SparkLinker linker = SparkLinker ( df , settings ) deterministic_rules = [ \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\" , \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\" , \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\" , \"l.email = r.email\" ] linker . estimate_probability_two_random_records_match ( deterministic_rules , recall = 0.6 ) Probability two random records match is estimated to be 0.00389. This means that amongst all possible pairwise record comparisons, one in 257.25 are expected to match. With 499,500 total possible comparisons, we expect a total of around 1,941.67 matching pairs linker . estimate_u_using_random_sampling ( max_pairs = 5e5 ) ----- Estimating u probabilities using random sampling ----- 23/02/04 19:37:25 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_concat_with_tf_781a41b44 Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_fname_sname = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) training_blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname 23/02/04 19:37:31 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_comparison_vectors_b3b0f8be8 Iteration 1: Largest change in params was -0.53 in the m_probability of dob, level `Exact match` Iteration 2: Largest change in params was 0.0335 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0129 in probability_two_random_records_match Iteration 4: Largest change in params was 0.00639 in probability_two_random_records_match EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob 23/02/04 19:37:37 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_comparison_vectors_5d2301fd1 Iteration 1: Largest change in params was -0.413 in the m_probability of surname, level `Exact match` Iteration 2: Largest change in params was 0.108 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0348 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0133 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00593 in probability_two_random_records_match EM converged after 5 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values results = linker . predict ( threshold_match_probability = 0.9 ) 23/02/04 19:37:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'. 23/02/04 19:37:46 WARN DataSource: All paths were ignored: file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/b1c28f87-c125-4ce0-9230-8dfb8872c5fb/__splink__df_predict_0978bb8e9 results . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 3.371739 0.911904 220 223 Logan Logan 2 86.748396 serguFon Ferguson ... 1 0.212792 0.212792 10.316002 0.259162 l.feruson46@sahh.com None -1 1.000000 0 1 14.743406 0.999964 879 880 Leo Leo 2 86.748396 Webster Webster ... 1 0.008610 0.008610 10.316002 6.404996 leo.webster54@moore.biez None -1 1.000000 0 2 13.140266 0.999889 446 450 Aisha Aisha 2 86.748396 Bryant None ... 0 0.011070 0.001230 0.456259 1.000000 aishab64@obrien-flores.com aishab64@obrien-flores.com 3 257.458944 0 3 8.829126 0.997806 446 448 Aisha Aisha 2 86.748396 Bryant BryBant ... 0 0.011070 0.001230 0.456259 1.000000 aishab64@obrien-flores.com aishab64@obrien-flores.com 3 257.458944 0 4 6.584844 0.989690 790 791 Jackson Jackson 2 86.748396 Fisreh Fishier ... 0 0.009840 0.001230 0.456259 1.000000 j.fisher4@sullivan.com None -1 1.000000 0 5 rows \u00d7 28 columns","title":"Linking in Spark"},{"location":"demos/example_transactions.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linking banking transactions \u00b6 This example shows how to perform a one-to-one link on banking transactions. The data is fake data, and was generated has the following features: Money shows up in the destination account with some time delay The amount sent and the amount received are not always the same - there are hidden fees and foreign exchange effects The memo is sometimes truncated and content is sometimes missing Since each origin payment should end up in the destination account, the probability_two_random_records_match of the model is known. # Use arrow to read in data to ensure date types are correct from pyarrow import parquet as pq from splink.duckdb.duckdb_linker import DuckDBLinker import altair as alt alt . renderers . enable ( 'mimetype' ) df_origin = pq . read_table ( \"./data/transactions_left.parquet\" ) df_origin = df_origin . slice ( length = 1_000 ) df_destination = pq . read_table ( \"./data/transactions_right.parquet\" ) df_destination = df_destination . slice ( length = 1_000 ) f \"There are { df_origin . num_rows : ,.0f } records to match\" 'There are 1,000 records to match' # Some sample records df_origin . to_pandas () . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ground_truth memo transaction_date amount unique_id 0 0 MATTHIAS C paym 2022-03-28 36.36 0 1 1 M CORVINUS dona 2022-02-14 221.91 1 In the following chart, we can see this is a challenging dataset to link: - There are only 151 distinct transaction dates, with strong skew - Some 'memos' are used multiple times (up to 48 times) - There is strong skew in the 'amount' column, with 1,400 transactions of around 60.00 # Simple settings just for exploratory analysis settings = { \"link_type\" : \"link_only\" } linker = DuckDBLinker ([ df_origin , df_destination ], settings , input_table_aliases = [ \"__ori\" , \"_dest\" ]) linker . profile_columns ([ \"transaction_date\" , \"memo\" , \"round(amount/5, 0)*5\" ]) # Design blocking rules that allow for differences in transaction date and amounts blocking_rule_date_1 = \"\"\" strftime(l.transaction_date, '%Y%m') = strftime(r.transaction_date, '%Y%m') and substr(l.memo, 1,3) = substr(r.memo,1,3) and l.amount/r.amount > 0.7 and l.amount/r.amount < 1.3 \"\"\" # Offset by half a month to ensure we capture case when the dates are e.g. 31st Jan and 1st Feb blocking_rule_date_2 = \"\"\" strftime(l.transaction_date+15, '%Y%m') = strftime(r.transaction_date, '%Y%m') and substr(l.memo, 1,3) = substr(r.memo,1,3) and l.amount/r.amount > 0.7 and l.amount/r.amount < 1.3 \"\"\" blocking_rule_memo = \"\"\" substr(l.memo,1,9) = substr(r.memo,1,9) \"\"\" blocking_rule_amount_1 = \"\"\" round(l.amount/2,0)*2 = round(r.amount/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date) \"\"\" blocking_rule_amount_2 = \"\"\" round(l.amount/2,0)*2 = round((r.amount+1)/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date + 4) \"\"\" blocking_rule_cheat = \"\"\" l.unique_id = r.unique_id \"\"\" linker . cumulative_num_comparisons_from_blocking_rules_chart ( [ blocking_rule_date_1 , blocking_rule_date_2 , blocking_rule_memo , blocking_rule_amount_1 , blocking_rule_amount_2 , blocking_rule_cheat , ] ) # Full settings for linking model import splink.duckdb.duckdb_comparison_level_library as cl comparison_amount = { \"output_column_name\" : \"amount\" , \"comparison_levels\" : [ cl . null_level ( \"amount\" ), cl . exact_match_level ( \"amount\" ), cl . percentage_difference_level ( \"amount\" , 0.01 ), cl . percentage_difference_level ( \"amount\" , 0.03 ), cl . percentage_difference_level ( \"amount\" , 0.1 ), cl . percentage_difference_level ( \"amount\" , 0.3 ), cl . else_level () ], \"comparison_description\" : \"Amount percentage difference\" , } within_n_days_template = \"transaction_date_r - transaction_date_l <= {n} and transaction_date_r >= transaction_date_l\" comparison_date = { \"output_column_name\" : \"transaction_date\" , \"comparison_levels\" : [ cl . null_level ( \"transaction_date\" ), { \"sql_condition\" : within_n_days_template . format ( n = 1 ), \"label_for_charts\" : \"1 day\" }, { \"sql_condition\" : within_n_days_template . format ( n = 4 ), \"label_for_charts\" : \"<=4 days\" }, { \"sql_condition\" : within_n_days_template . format ( n = 10 ), \"label_for_charts\" : \"<=10 days\" }, { \"sql_condition\" : within_n_days_template . format ( n = 30 ), \"label_for_charts\" : \"<=30 days\" }, cl . else_level () ], \"comparison_description\" : \"Transaction date days apart\" , } from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"link_only\" , \"probability_two_random_records_match\" : 1 / len ( df_origin ), \"blocking_rules_to_generate_predictions\" : [ blocking_rule_date_1 , blocking_rule_date_2 , blocking_rule_memo , blocking_rule_amount_1 , blocking_rule_amount_2 , blocking_rule_cheat ], \"comparisons\" : [ comparison_amount , cl . jaccard_at_thresholds ( \"memo\" , [ 0.9 , 0.7 ] ), comparison_date ], \"retain_intermediate_calculation_columns\" : True , \"retain_matching_columns\" : True , } linker = DuckDBLinker ([ df_origin , df_destination ], settings , input_table_aliases = [ \"__ori\" , \"_dest\" ]) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - amount (no m values are trained). - memo (no m values are trained). - transaction_date (no m values are trained). linker . estimate_parameters_using_expectation_maximisation ( \"l.memo = r.memo\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.memo = r.memo Parameter estimates will be made for the following comparison(s): - amount - transaction_date Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - memo Iteration 1: Largest change in params was -0.555 in the m_probability of amount, level `Exact match` Iteration 2: Largest change in params was -0.213 in the m_probability of transaction_date, level `1 day` Iteration 3: Largest change in params was 0.00794 in probability_two_random_records_match Iteration 4: Largest change in params was 0.000315 in the m_probability of transaction_date, level `<=30 days` Iteration 5: Largest change in params was -0.000177 in the m_probability of amount, level `All other comparisons` Iteration 6: Largest change in params was -0.000159 in the m_probability of amount, level `All other comparisons` Iteration 7: Largest change in params was -0.000144 in the m_probability of amount, level `All other comparisons` Iteration 8: Largest change in params was -0.000131 in the m_probability of amount, level `All other comparisons` Iteration 9: Largest change in params was -0.000119 in the m_probability of amount, level `All other comparisons` Iteration 10: Largest change in params was -0.000109 in the m_probability of amount, level `All other comparisons` Iteration 11: Largest change in params was -9.97e-05 in the m_probability of amount, level `All other comparisons` EM converged after 11 iterations Your model is not yet fully trained. Missing estimates for: - memo (no m values are trained). <EMTrainingSession, blocking on l.memo = r.memo, deactivating comparisons memo> session = linker . estimate_parameters_using_expectation_maximisation ( \"l.amount = r.amount\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.amount = r.amount Parameter estimates will be made for the following comparison(s): - memo - transaction_date Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - amount Iteration 1: Largest change in params was 0.495 in probability_two_random_records_match Iteration 2: Largest change in params was 0.134 in probability_two_random_records_match Iteration 3: Largest change in params was 0.139 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0293 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00393 in probability_two_random_records_match Iteration 6: Largest change in params was 0.000522 in probability_two_random_records_match Iteration 7: Largest change in params was 6.93e-05 in probability_two_random_records_match EM converged after 7 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values linker . match_weights_chart () df_predict = linker . predict ( threshold_match_probability = 0.001 ) linker . comparison_viewer_dashboard ( df_predict , \"comparison_viewer_transactions.html\" , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./comparison_viewer_transactions.html\" , width = \"100%\" , height = 1200 ) pred_errors = linker . prediction_errors_from_labels_column ( \"ground_truth\" , include_false_positives = True , include_false_negatives = False ) linker . waterfall_chart ( pred_errors . as_record_dict ( limit = 5 )) pred_errors = linker . prediction_errors_from_labels_column ( \"ground_truth\" , include_false_positives = False , include_false_negatives = True ) linker . waterfall_chart ( pred_errors . as_record_dict ( limit = 5 ))","title":"Linking financial transactions"},{"location":"demos/example_transactions.html#linking-banking-transactions","text":"This example shows how to perform a one-to-one link on banking transactions. The data is fake data, and was generated has the following features: Money shows up in the destination account with some time delay The amount sent and the amount received are not always the same - there are hidden fees and foreign exchange effects The memo is sometimes truncated and content is sometimes missing Since each origin payment should end up in the destination account, the probability_two_random_records_match of the model is known. # Use arrow to read in data to ensure date types are correct from pyarrow import parquet as pq from splink.duckdb.duckdb_linker import DuckDBLinker import altair as alt alt . renderers . enable ( 'mimetype' ) df_origin = pq . read_table ( \"./data/transactions_left.parquet\" ) df_origin = df_origin . slice ( length = 1_000 ) df_destination = pq . read_table ( \"./data/transactions_right.parquet\" ) df_destination = df_destination . slice ( length = 1_000 ) f \"There are { df_origin . num_rows : ,.0f } records to match\" 'There are 1,000 records to match' # Some sample records df_origin . to_pandas () . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ground_truth memo transaction_date amount unique_id 0 0 MATTHIAS C paym 2022-03-28 36.36 0 1 1 M CORVINUS dona 2022-02-14 221.91 1 In the following chart, we can see this is a challenging dataset to link: - There are only 151 distinct transaction dates, with strong skew - Some 'memos' are used multiple times (up to 48 times) - There is strong skew in the 'amount' column, with 1,400 transactions of around 60.00 # Simple settings just for exploratory analysis settings = { \"link_type\" : \"link_only\" } linker = DuckDBLinker ([ df_origin , df_destination ], settings , input_table_aliases = [ \"__ori\" , \"_dest\" ]) linker . profile_columns ([ \"transaction_date\" , \"memo\" , \"round(amount/5, 0)*5\" ]) # Design blocking rules that allow for differences in transaction date and amounts blocking_rule_date_1 = \"\"\" strftime(l.transaction_date, '%Y%m') = strftime(r.transaction_date, '%Y%m') and substr(l.memo, 1,3) = substr(r.memo,1,3) and l.amount/r.amount > 0.7 and l.amount/r.amount < 1.3 \"\"\" # Offset by half a month to ensure we capture case when the dates are e.g. 31st Jan and 1st Feb blocking_rule_date_2 = \"\"\" strftime(l.transaction_date+15, '%Y%m') = strftime(r.transaction_date, '%Y%m') and substr(l.memo, 1,3) = substr(r.memo,1,3) and l.amount/r.amount > 0.7 and l.amount/r.amount < 1.3 \"\"\" blocking_rule_memo = \"\"\" substr(l.memo,1,9) = substr(r.memo,1,9) \"\"\" blocking_rule_amount_1 = \"\"\" round(l.amount/2,0)*2 = round(r.amount/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date) \"\"\" blocking_rule_amount_2 = \"\"\" round(l.amount/2,0)*2 = round((r.amount+1)/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date + 4) \"\"\" blocking_rule_cheat = \"\"\" l.unique_id = r.unique_id \"\"\" linker . cumulative_num_comparisons_from_blocking_rules_chart ( [ blocking_rule_date_1 , blocking_rule_date_2 , blocking_rule_memo , blocking_rule_amount_1 , blocking_rule_amount_2 , blocking_rule_cheat , ] ) # Full settings for linking model import splink.duckdb.duckdb_comparison_level_library as cl comparison_amount = { \"output_column_name\" : \"amount\" , \"comparison_levels\" : [ cl . null_level ( \"amount\" ), cl . exact_match_level ( \"amount\" ), cl . percentage_difference_level ( \"amount\" , 0.01 ), cl . percentage_difference_level ( \"amount\" , 0.03 ), cl . percentage_difference_level ( \"amount\" , 0.1 ), cl . percentage_difference_level ( \"amount\" , 0.3 ), cl . else_level () ], \"comparison_description\" : \"Amount percentage difference\" , } within_n_days_template = \"transaction_date_r - transaction_date_l <= {n} and transaction_date_r >= transaction_date_l\" comparison_date = { \"output_column_name\" : \"transaction_date\" , \"comparison_levels\" : [ cl . null_level ( \"transaction_date\" ), { \"sql_condition\" : within_n_days_template . format ( n = 1 ), \"label_for_charts\" : \"1 day\" }, { \"sql_condition\" : within_n_days_template . format ( n = 4 ), \"label_for_charts\" : \"<=4 days\" }, { \"sql_condition\" : within_n_days_template . format ( n = 10 ), \"label_for_charts\" : \"<=10 days\" }, { \"sql_condition\" : within_n_days_template . format ( n = 30 ), \"label_for_charts\" : \"<=30 days\" }, cl . else_level () ], \"comparison_description\" : \"Transaction date days apart\" , } from splink.duckdb import duckdb_comparison_library as cl settings = { \"link_type\" : \"link_only\" , \"probability_two_random_records_match\" : 1 / len ( df_origin ), \"blocking_rules_to_generate_predictions\" : [ blocking_rule_date_1 , blocking_rule_date_2 , blocking_rule_memo , blocking_rule_amount_1 , blocking_rule_amount_2 , blocking_rule_cheat ], \"comparisons\" : [ comparison_amount , cl . jaccard_at_thresholds ( \"memo\" , [ 0.9 , 0.7 ] ), comparison_date ], \"retain_intermediate_calculation_columns\" : True , \"retain_matching_columns\" : True , } linker = DuckDBLinker ([ df_origin , df_destination ], settings , input_table_aliases = [ \"__ori\" , \"_dest\" ]) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - amount (no m values are trained). - memo (no m values are trained). - transaction_date (no m values are trained). linker . estimate_parameters_using_expectation_maximisation ( \"l.memo = r.memo\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.memo = r.memo Parameter estimates will be made for the following comparison(s): - amount - transaction_date Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - memo Iteration 1: Largest change in params was -0.555 in the m_probability of amount, level `Exact match` Iteration 2: Largest change in params was -0.213 in the m_probability of transaction_date, level `1 day` Iteration 3: Largest change in params was 0.00794 in probability_two_random_records_match Iteration 4: Largest change in params was 0.000315 in the m_probability of transaction_date, level `<=30 days` Iteration 5: Largest change in params was -0.000177 in the m_probability of amount, level `All other comparisons` Iteration 6: Largest change in params was -0.000159 in the m_probability of amount, level `All other comparisons` Iteration 7: Largest change in params was -0.000144 in the m_probability of amount, level `All other comparisons` Iteration 8: Largest change in params was -0.000131 in the m_probability of amount, level `All other comparisons` Iteration 9: Largest change in params was -0.000119 in the m_probability of amount, level `All other comparisons` Iteration 10: Largest change in params was -0.000109 in the m_probability of amount, level `All other comparisons` Iteration 11: Largest change in params was -9.97e-05 in the m_probability of amount, level `All other comparisons` EM converged after 11 iterations Your model is not yet fully trained. Missing estimates for: - memo (no m values are trained). <EMTrainingSession, blocking on l.memo = r.memo, deactivating comparisons memo> session = linker . estimate_parameters_using_expectation_maximisation ( \"l.amount = r.amount\" ) ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.amount = r.amount Parameter estimates will be made for the following comparison(s): - memo - transaction_date Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - amount Iteration 1: Largest change in params was 0.495 in probability_two_random_records_match Iteration 2: Largest change in params was 0.134 in probability_two_random_records_match Iteration 3: Largest change in params was 0.139 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0293 in probability_two_random_records_match Iteration 5: Largest change in params was 0.00393 in probability_two_random_records_match Iteration 6: Largest change in params was 0.000522 in probability_two_random_records_match Iteration 7: Largest change in params was 6.93e-05 in probability_two_random_records_match EM converged after 7 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values linker . match_weights_chart () df_predict = linker . predict ( threshold_match_probability = 0.001 ) linker . comparison_viewer_dashboard ( df_predict , \"comparison_viewer_transactions.html\" , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./comparison_viewer_transactions.html\" , width = \"100%\" , height = 1200 ) pred_errors = linker . prediction_errors_from_labels_column ( \"ground_truth\" , include_false_positives = True , include_false_negatives = False ) linker . waterfall_chart ( pred_errors . as_record_dict ( limit = 5 )) pred_errors = linker . prediction_errors_from_labels_column ( \"ground_truth\" , include_false_positives = False , include_false_negatives = True ) linker . waterfall_chart ( pred_errors . as_record_dict ( limit = 5 ))","title":"Linking banking transactions"},{"location":"dev_guides/caching.html","text":"Caching and pipelining \u00b6 Splink is able to run against multiple SQL backends because all of the core data linking calculations are implemented in SQL. This SQL can therefore be submitted to a chosen SQL backend for execution. Computations in Splink often take the form of a number of select statements run in sequence. For example, the predict() step: Inputs __splink__df_concat_with_tf and outputs __splink__df_blocked Inputs __splink__df_blocked and outputs __splink__df_comparison_vectors Inputs __splink__df_comparison_vectors and outputs __splink__df_match_weight_parts Inputs __splink__df_match_weight_parts and outputs __splink__df_predict To make this run faster, two key optimisations are implmented: Pipelining - combining multiple select statements into a single statemenet using WITH ( CTE ) queries Caching: saving the results of calculations so they don't need recalculating. This is especially useful because some intermediate calculations are reused multiple times during a typical Splink session This article discusses the general implementation of caching and pipelining. The implementation needs some alterations for certain backends like Spark, which lazily evaluate SQL by default. Implementation: Pipelining \u00b6 A SQLPipeline class manages SQL pipelining. A SQLPipeline is composed of a number of SQLTask objects , each of which represents a select statement. The code is fairly straightforward: Given a sequence of select statements, [a,b,c] they are combined into a single query as follows: with a as (a_sql), b as (b_sql), c_sql To make this work, each statement (a,b,c) in the pipeline must refer to the previous step by name. For example, b_sql probably selects from the a_sql table, which has been aliased a . So b_sql must use the table name a to refer to the result of a_sql . To make this tractable, each SQLTask has an output_table_name . For example, the output_table_name for a_sql in the above example is a . For instance, in the predict() pipeline above, the first output_table_name is __splink__df_blocked . By giving each task a meaningful output_table_name , subsequent tasks can reference previous outputs in a way which is semantically clear. Implementation: Caching \u00b6 When a SQL pipeline is executed, it has two output names: A physical_name , which is the name of the materialised table in the output database e.g. __splink__df_predict_cbc9833 A templated_name , which is a descriptive name of what the table represents e.g. __splink__df_predict Each time Splink runs a SQL pipeline, the SQL string is hashed . This creates a unique identifier for that particular SQL string, which serves to identify the output. When Splink is asked to execute a SQL string, before execution, it checks whether the resultant table already exists. If it does, it returns the table rather than recomputing it. For example, when we run linker.predict() , Splink: Generates the SQL tasks Pipelines them into a single SQL statement Hashes the statement to create a physical name for the outputs __splink__df_predict_cbc9833 Checks whether a table with physical name __splink__df_predict_cbc9833 alredy exists in the database If not, executes the SQL statement, creating table __splink__df_predict_cbc9833 in the database. In terms of implementation, the following happens: SQL statements are generated an put in the queue - see here Once all the tasks have been added to the queue, we call _execute_sql_pipeline() see here The SQL is combined into a single pipelined statement here We call _sql_to_splink_dataframe() which returns the table (from the cache if it already exists, or it executes the sql) The table is returned as a SplinkDataframe , an abstraction over a table in a database. See here . Some cached tables do not need a hash \u00b6 A hash is required to uniquely identify some outputs. For example, blocking is used in several places in Splink, with different results . For example, the __splink__df_blocked needed to estimate parameters is different to the __splink__df_blocked needed in the predict() step. As a result, we cannot materialise a single table called __splink__df_blocked in the database and reues it multiple times. This is why we append the hash of the SQL, so that we can uniquely identify the different versions of __splink__df_blocked which are needed in different contexts. There are, however, some tables which are globally unique. They only take a single form, and if they exist in the cache they never need recomputing. An example of this is __splink__df_concat_with_tf , which represents the concatenation of the input dataframes. To create this table, we can execute _sql_to_splink_dataframe with materialise_as_hash set to False . The resultant materialised table will not have a hash appended, and will simply be called __splink__df_concat_with_tf . This is useful, because when performing calculations Splink can now check the cache for __splink__df_concat_with_tf each time it is needed. In fact, many Splink pipelines begin with the assumption that this table exists in the database, because the first SQLTask in the pipeline refers to a table named __splink__df_concat_with_tf . To ensure this is the case, a function is used to create this table if it doesn't exist. Using pipelining to optimise Splink workloads \u00b6 At what point should a pipeline of SQLTask s be executed (materialised into a physical table)? For any individual output, it will usually be fastest to pipeline the full linage of tasks, right from raw data through to the end result. However, there are many intermediate outputs which are used by many different Splink operations. Performance can therefore be improved by computing and saving these intermediate outputs to a cache, to ensure they don't need to be computed repeatedly. This is achieved by enqueueing SQL to a pipline and strategically calling execute_sql_pipeline to materialise results that need to cached.","title":"Caching and pipelining"},{"location":"dev_guides/caching.html#caching-and-pipelining","text":"Splink is able to run against multiple SQL backends because all of the core data linking calculations are implemented in SQL. This SQL can therefore be submitted to a chosen SQL backend for execution. Computations in Splink often take the form of a number of select statements run in sequence. For example, the predict() step: Inputs __splink__df_concat_with_tf and outputs __splink__df_blocked Inputs __splink__df_blocked and outputs __splink__df_comparison_vectors Inputs __splink__df_comparison_vectors and outputs __splink__df_match_weight_parts Inputs __splink__df_match_weight_parts and outputs __splink__df_predict To make this run faster, two key optimisations are implmented: Pipelining - combining multiple select statements into a single statemenet using WITH ( CTE ) queries Caching: saving the results of calculations so they don't need recalculating. This is especially useful because some intermediate calculations are reused multiple times during a typical Splink session This article discusses the general implementation of caching and pipelining. The implementation needs some alterations for certain backends like Spark, which lazily evaluate SQL by default.","title":"Caching and pipelining"},{"location":"dev_guides/caching.html#implementation-pipelining","text":"A SQLPipeline class manages SQL pipelining. A SQLPipeline is composed of a number of SQLTask objects , each of which represents a select statement. The code is fairly straightforward: Given a sequence of select statements, [a,b,c] they are combined into a single query as follows: with a as (a_sql), b as (b_sql), c_sql To make this work, each statement (a,b,c) in the pipeline must refer to the previous step by name. For example, b_sql probably selects from the a_sql table, which has been aliased a . So b_sql must use the table name a to refer to the result of a_sql . To make this tractable, each SQLTask has an output_table_name . For example, the output_table_name for a_sql in the above example is a . For instance, in the predict() pipeline above, the first output_table_name is __splink__df_blocked . By giving each task a meaningful output_table_name , subsequent tasks can reference previous outputs in a way which is semantically clear.","title":"Implementation: Pipelining"},{"location":"dev_guides/caching.html#implementation-caching","text":"When a SQL pipeline is executed, it has two output names: A physical_name , which is the name of the materialised table in the output database e.g. __splink__df_predict_cbc9833 A templated_name , which is a descriptive name of what the table represents e.g. __splink__df_predict Each time Splink runs a SQL pipeline, the SQL string is hashed . This creates a unique identifier for that particular SQL string, which serves to identify the output. When Splink is asked to execute a SQL string, before execution, it checks whether the resultant table already exists. If it does, it returns the table rather than recomputing it. For example, when we run linker.predict() , Splink: Generates the SQL tasks Pipelines them into a single SQL statement Hashes the statement to create a physical name for the outputs __splink__df_predict_cbc9833 Checks whether a table with physical name __splink__df_predict_cbc9833 alredy exists in the database If not, executes the SQL statement, creating table __splink__df_predict_cbc9833 in the database. In terms of implementation, the following happens: SQL statements are generated an put in the queue - see here Once all the tasks have been added to the queue, we call _execute_sql_pipeline() see here The SQL is combined into a single pipelined statement here We call _sql_to_splink_dataframe() which returns the table (from the cache if it already exists, or it executes the sql) The table is returned as a SplinkDataframe , an abstraction over a table in a database. See here .","title":"Implementation: Caching"},{"location":"dev_guides/caching.html#some-cached-tables-do-not-need-a-hash","text":"A hash is required to uniquely identify some outputs. For example, blocking is used in several places in Splink, with different results . For example, the __splink__df_blocked needed to estimate parameters is different to the __splink__df_blocked needed in the predict() step. As a result, we cannot materialise a single table called __splink__df_blocked in the database and reues it multiple times. This is why we append the hash of the SQL, so that we can uniquely identify the different versions of __splink__df_blocked which are needed in different contexts. There are, however, some tables which are globally unique. They only take a single form, and if they exist in the cache they never need recomputing. An example of this is __splink__df_concat_with_tf , which represents the concatenation of the input dataframes. To create this table, we can execute _sql_to_splink_dataframe with materialise_as_hash set to False . The resultant materialised table will not have a hash appended, and will simply be called __splink__df_concat_with_tf . This is useful, because when performing calculations Splink can now check the cache for __splink__df_concat_with_tf each time it is needed. In fact, many Splink pipelines begin with the assumption that this table exists in the database, because the first SQLTask in the pipeline refers to a table named __splink__df_concat_with_tf . To ensure this is the case, a function is used to create this table if it doesn't exist.","title":"Some cached tables do not need a hash"},{"location":"dev_guides/caching.html#using-pipelining-to-optimise-splink-workloads","text":"At what point should a pipeline of SQLTask s be executed (materialised into a physical table)? For any individual output, it will usually be fastest to pipeline the full linage of tasks, right from raw data through to the end result. However, there are many intermediate outputs which are used by many different Splink operations. Performance can therefore be improved by computing and saving these intermediate outputs to a cache, to ensure they don't need to be computed repeatedly. This is achieved by enqueueing SQL to a pipline and strategically calling execute_sql_pipeline to materialise results that need to cached.","title":"Using pipelining to optimise Splink workloads"},{"location":"dev_guides/debug_modes.html","text":"Understanding and debugging Splink's computations \u00b6 Splink contains tooling to help developers understand the underlying computations, how caching and pipelining is working, and debug problems. There are two main mechanisms: debug_mode , and setting different logging levels Debug mode \u00b6 You can turn on debug mode by setting linker.debug_mode = True . This has the following effects: Each step of Splink's calculations are executed in turn. That is, pipelining is switched off. The SQL statements being executed by Splink are displayed The results of the SQL statements are displayed in tabular format This is probably the best way to understand each step of the calculations being performed by Splink - because a lot of the implementation gets 'hidden' within pipelines for performance reasons. Note that enabling debug mode will dramatically reduce Splink's performance! Logging \u00b6 Splink has a range of logging modes that output information about what Splink is doing at different levels of verbosity. Unlike debug mode, logging doesn't affect the performance of Splink. Logging levels \u00b6 You can set the logging level with code like logging.getLogger(\"splink\").setLevel(desired_level) although see notes below about gotyas . The logging levels in Splink are: logging.INFO ( 20 ): This outputs user facing messages about the training status of Splink models 15 : Outputs additional information about time taken and parameter estimation logging.DEBUG ( 10 ): Outputs information about the names of the SQL statements executed logging.DEBUG ( 7 ): Outputs information about the names of the components of the SQL pipelines logging.DEBUG ( 5 ): Outputs the SQL statements themselves How to control logging \u00b6 Note that by default Splink sets the logging level to INFO on initialisation With basic logging \u00b6 import logging linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) # This must come AFTER the linker is intialised, because the logging level # will be set to INFO logging . getLogger ( \"splink\" ) . setLevel ( logging . DEBUG ) Without basic logging \u00b6 # This code can be anywhere since set_up_basic_logging is False import logging logging . basicConfig ( format = \" %(message)s \" ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) linker = DuckDBLinker ( df , settings , set_up_basic_logging = False )","title":"Understanding and debugging Splink"},{"location":"dev_guides/debug_modes.html#understanding-and-debugging-splinks-computations","text":"Splink contains tooling to help developers understand the underlying computations, how caching and pipelining is working, and debug problems. There are two main mechanisms: debug_mode , and setting different logging levels","title":"Understanding and debugging Splink's computations"},{"location":"dev_guides/debug_modes.html#debug-mode","text":"You can turn on debug mode by setting linker.debug_mode = True . This has the following effects: Each step of Splink's calculations are executed in turn. That is, pipelining is switched off. The SQL statements being executed by Splink are displayed The results of the SQL statements are displayed in tabular format This is probably the best way to understand each step of the calculations being performed by Splink - because a lot of the implementation gets 'hidden' within pipelines for performance reasons. Note that enabling debug mode will dramatically reduce Splink's performance!","title":"Debug mode"},{"location":"dev_guides/debug_modes.html#logging","text":"Splink has a range of logging modes that output information about what Splink is doing at different levels of verbosity. Unlike debug mode, logging doesn't affect the performance of Splink.","title":"Logging"},{"location":"dev_guides/debug_modes.html#logging-levels","text":"You can set the logging level with code like logging.getLogger(\"splink\").setLevel(desired_level) although see notes below about gotyas . The logging levels in Splink are: logging.INFO ( 20 ): This outputs user facing messages about the training status of Splink models 15 : Outputs additional information about time taken and parameter estimation logging.DEBUG ( 10 ): Outputs information about the names of the SQL statements executed logging.DEBUG ( 7 ): Outputs information about the names of the components of the SQL pipelines logging.DEBUG ( 5 ): Outputs the SQL statements themselves","title":"Logging levels"},{"location":"dev_guides/debug_modes.html#how-to-control-logging","text":"Note that by default Splink sets the logging level to INFO on initialisation","title":"How to control logging"},{"location":"dev_guides/debug_modes.html#with-basic-logging","text":"import logging linker = DuckDBLinker ( df , settings , set_up_basic_logging = False ) # This must come AFTER the linker is intialised, because the logging level # will be set to INFO logging . getLogger ( \"splink\" ) . setLevel ( logging . DEBUG )","title":"With basic logging"},{"location":"dev_guides/debug_modes.html#without-basic-logging","text":"# This code can be anywhere since set_up_basic_logging is False import logging logging . basicConfig ( format = \" %(message)s \" ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) linker = DuckDBLinker ( df , settings , set_up_basic_logging = False )","title":"Without basic logging"},{"location":"dev_guides/spark_pipelining_and_caching.html","text":"Caching and pipelining in Spark \u00b6 This article assumes you've read the general guide to caching and pipelining . In Spark, some additions have to be made to this general pattern because all transformation in Spark are lazy . That is, when we call df = spark.sql(sql) , the df is not immediately computed. Furthermore, even when an action is called, the results aren't automatically persisted by Spark to disk. This differs from other backends, which execute SQL as a create table statement, meaning that the result is automatically saved. This interferes with caching, because Splink assumes that when the the function _execute_sql_against_backend() is called, this will be evaluted greedily (immediately evaluated) AND the results will be saved to the 'database'. Another quirk of Spark is that it chunks work up into tasks. This is relevant for two reasons: Tasks can suffer from skew, meaning some take longer than others, which can be bad from a performance point of view. The number of tasks and how data is partitioned controls how many files are output when results are saved. Some Splink operations results in a very large number of small files which can take a long time to read and write, relative to the same data stored in fewer files. Repartitioning can be used to rebalance workloads (reduce skew) and to avoid the 'many small files' problem. Spark-specific modifications \u00b6 The logic for Spark is captured in the implementation of _execute_sql_against_backend() in the spark_linker.py. This has three roles: It determines how to save result - using either persist , checkpoint or saving to .parquet , with .parquet being the default. It determines which results to save. Some small results such __splink__m_u_counts are immediately converted using toPandas() rather than being saved. This is because saving to disk and reloading is expensive and unnecessary. It chooses which Spark dataframes to repartition to reduce the number of files which are written/read Note that repartitioning and saving is independent. Some dataframes are saved without repartitioning. Some dataframes are repartitioned without being saved.","title":"Spark caching"},{"location":"dev_guides/spark_pipelining_and_caching.html#caching-and-pipelining-in-spark","text":"This article assumes you've read the general guide to caching and pipelining . In Spark, some additions have to be made to this general pattern because all transformation in Spark are lazy . That is, when we call df = spark.sql(sql) , the df is not immediately computed. Furthermore, even when an action is called, the results aren't automatically persisted by Spark to disk. This differs from other backends, which execute SQL as a create table statement, meaning that the result is automatically saved. This interferes with caching, because Splink assumes that when the the function _execute_sql_against_backend() is called, this will be evaluted greedily (immediately evaluated) AND the results will be saved to the 'database'. Another quirk of Spark is that it chunks work up into tasks. This is relevant for two reasons: Tasks can suffer from skew, meaning some take longer than others, which can be bad from a performance point of view. The number of tasks and how data is partitioned controls how many files are output when results are saved. Some Splink operations results in a very large number of small files which can take a long time to read and write, relative to the same data stored in fewer files. Repartitioning can be used to rebalance workloads (reduce skew) and to avoid the 'many small files' problem.","title":"Caching and pipelining in Spark"},{"location":"dev_guides/spark_pipelining_and_caching.html#spark-specific-modifications","text":"The logic for Spark is captured in the implementation of _execute_sql_against_backend() in the spark_linker.py. This has three roles: It determines how to save result - using either persist , checkpoint or saving to .parquet , with .parquet being the default. It determines which results to save. Some small results such __splink__m_u_counts are immediately converted using toPandas() rather than being saved. This is because saving to disk and reloading is expensive and unnecessary. It chooses which Spark dataframes to repartition to reduce the number of files which are written/read Note that repartitioning and saving is independent. Some dataframes are saved without repartitioning. Some dataframes are repartitioned without being saved.","title":"Spark-specific modifications"},{"location":"dev_guides/transpilation.html","text":"SQL Transpilation in Splink, and how we support multiple SQL backends \u00b6 In Splink, all the core data linking algorithms are implemented in SQL. This allows computation to be offloaded to a SQL backend of the users choice. One difficulty with this paradigm is that SQL implementations differ - the functions available in (say) the Spark dialect of SQL differ from those available in DuckDB SQL. And to make matters worse, functions with the same name may behave differently (e.g. different arguments, arguments in different orders, etc.). Splink therefore needs a mechanism of writing SQL statements that are able to run against all the target SQL backends (engines). Details are as follows: 1. Core data linking algorithms are Splink \u00b6 Core data linking algorithms are implmented in 'backend agnostic' SQL. So they're written using basic SQL functions that are common across the available in all the target backends, and don't need any translation. It has been possible to write all of the core Splink logic in SQL that is consistent between dialects. However, this is not the case with Comparisons , which tend to use backend specific SQL functions like jaro_winker , whose functino names and signatures differ between backends. 2. User-provided SQL is interpolated into these dialect-agnostic SQL statements \u00b6 The user provides custom SQL is two places in Splink: Blocking rules The sql_condition (see here ) provided as part of a Comparison The user is free to write this SQL however they want. It's up to the user to ensure the SQL they provide will execute successfully in their chosen backend. So the sql_condition must use functions that exist in the target execution engine The custom SQL is interpolated into the the SQL statements generated by Splink. Users are also able to use the comparison_level_library and comparison_library for their chosen backend. These are backend specific and are imported like from splink.spark.spark_comparison_level_library import jaro_winkler_level . This ensures that the syntax matches the chosen execution backend 3. Backends can implement transpilation and or dielct steps to further transform the SQL if needed \u00b6 Occasionally some modifications are needed to the SQL to ensure it executes against the target backend. sqlglot is used for this purpose. For instance, a custom dialect is implemented in the sparklinker. A transformer is implemented in the Athena linker.","title":"Transpilation using sqlglot"},{"location":"dev_guides/transpilation.html#sql-transpilation-in-splink-and-how-we-support-multiple-sql-backends","text":"In Splink, all the core data linking algorithms are implemented in SQL. This allows computation to be offloaded to a SQL backend of the users choice. One difficulty with this paradigm is that SQL implementations differ - the functions available in (say) the Spark dialect of SQL differ from those available in DuckDB SQL. And to make matters worse, functions with the same name may behave differently (e.g. different arguments, arguments in different orders, etc.). Splink therefore needs a mechanism of writing SQL statements that are able to run against all the target SQL backends (engines). Details are as follows:","title":"SQL Transpilation in Splink, and how we support multiple SQL backends"},{"location":"dev_guides/transpilation.html#1-core-data-linking-algorithms-are-splink","text":"Core data linking algorithms are implmented in 'backend agnostic' SQL. So they're written using basic SQL functions that are common across the available in all the target backends, and don't need any translation. It has been possible to write all of the core Splink logic in SQL that is consistent between dialects. However, this is not the case with Comparisons , which tend to use backend specific SQL functions like jaro_winker , whose functino names and signatures differ between backends.","title":"1. Core data linking algorithms are Splink"},{"location":"dev_guides/transpilation.html#2-user-provided-sql-is-interpolated-into-these-dialect-agnostic-sql-statements","text":"The user provides custom SQL is two places in Splink: Blocking rules The sql_condition (see here ) provided as part of a Comparison The user is free to write this SQL however they want. It's up to the user to ensure the SQL they provide will execute successfully in their chosen backend. So the sql_condition must use functions that exist in the target execution engine The custom SQL is interpolated into the the SQL statements generated by Splink. Users are also able to use the comparison_level_library and comparison_library for their chosen backend. These are backend specific and are imported like from splink.spark.spark_comparison_level_library import jaro_winkler_level . This ensures that the syntax matches the chosen execution backend","title":"2. User-provided SQL is interpolated into these dialect-agnostic SQL statements"},{"location":"dev_guides/transpilation.html#3-backends-can-implement-transpilation-and-or-dielct-steps-to-further-transform-the-sql-if-needed","text":"Occasionally some modifications are needed to the SQL to ensure it executes against the target backend. sqlglot is used for this purpose. For instance, a custom dialect is implemented in the sparklinker. A transformer is implemented in the Athena linker.","title":"3. Backends can implement transpilation and or dielct steps to further transform the SQL if needed"},{"location":"dev_guides/changing_splink/build_docs_locally.html","text":"Building docs locally \u00b6 To rapidly build the documentation and immediately see changes you've made you can use this script : source scripts/make_docs_locally.sh This is much faster than waiting for github actions to run if you're trying to make fiddly changes to formatting etc. The Splink repo contains a working requirements.txt for building the docs , or a more complete version: attrs==22.2.0 beautifulsoup4==4.11.2 bleach==6.0.0 certifi==2022.12.7 charset-normalizer==3.0.1 click==8.1.3 colorama==0.4.6 defusedxml==0.7.1 EditorConfig==0.12.3 fastjsonschema==2.16.2 ghp-import==2.1.0 gitdb==4.0.10 GitPython==3.1.31 griffe==0.25.5 idna==3.4 importlib-metadata==6.0.0 Jinja2==3.0.3 jsbeautifier==1.14.7 jsonschema==4.17.3 jsonschema2md==0.4.0 jupyter_client==8.0.3 jupyter_core==5.2.0 jupyterlab-pygments==0.2.2 Markdown==3.3.7 MarkupSafe==2.1.2 mergedeep==1.3.4 mistune==2.0.5 mkdocs==1.4.2 mkdocs-autorefs==0.4.1 mkdocs-click==0.8.0 mkdocs-gen-files==0.4.0 mkdocs-include-markdown-plugin==4.0.3 mkdocs-material==8.5.11 mkdocs-material-extensions==1.1.1 mkdocs-mermaid2-plugin==0.6.0 mkdocs-monorepo-plugin==1.0.4 mkdocs-schema-reader==0.11.1 mkdocs-semiliterate==0.7.0 mkdocs-simple-plugin==2.1.2 mkdocstrings==0.20.0 mkdocstrings-python==0.8.3 mkdocstrings-python-legacy==0.2.3 mknotebooks==0.7.1 nbclient==0.7.2 nbconvert==7.2.9 nbformat==5.7.3 packaging==23.0 pandocfilters==1.5.0 platformdirs==3.0.0 Pygments==2.14.0 pymdown-extensions==9.9.2 pyrsistent==0.19.3 python-dateutil==2.8.2 python-slugify==8.0.0 pytkdocs==0.16.1 PyYAML==6.0 pyyaml_env_tag==0.1 pyzmq==25.0.0 requests==2.28.2 six==1.16.0 smmap==5.0.0 soupsieve==2.4 text-unidecode==1.3 tinycss2==1.2.1 tornado==6.2 traitlets==5.9.0 urllib3==1.26.14 watchdog==2.2.1 webencodings==0.5.1 zipp==3.14.0","title":"Building Docs"},{"location":"dev_guides/changing_splink/build_docs_locally.html#building-docs-locally","text":"To rapidly build the documentation and immediately see changes you've made you can use this script : source scripts/make_docs_locally.sh This is much faster than waiting for github actions to run if you're trying to make fiddly changes to formatting etc. The Splink repo contains a working requirements.txt for building the docs , or a more complete version: attrs==22.2.0 beautifulsoup4==4.11.2 bleach==6.0.0 certifi==2022.12.7 charset-normalizer==3.0.1 click==8.1.3 colorama==0.4.6 defusedxml==0.7.1 EditorConfig==0.12.3 fastjsonschema==2.16.2 ghp-import==2.1.0 gitdb==4.0.10 GitPython==3.1.31 griffe==0.25.5 idna==3.4 importlib-metadata==6.0.0 Jinja2==3.0.3 jsbeautifier==1.14.7 jsonschema==4.17.3 jsonschema2md==0.4.0 jupyter_client==8.0.3 jupyter_core==5.2.0 jupyterlab-pygments==0.2.2 Markdown==3.3.7 MarkupSafe==2.1.2 mergedeep==1.3.4 mistune==2.0.5 mkdocs==1.4.2 mkdocs-autorefs==0.4.1 mkdocs-click==0.8.0 mkdocs-gen-files==0.4.0 mkdocs-include-markdown-plugin==4.0.3 mkdocs-material==8.5.11 mkdocs-material-extensions==1.1.1 mkdocs-mermaid2-plugin==0.6.0 mkdocs-monorepo-plugin==1.0.4 mkdocs-schema-reader==0.11.1 mkdocs-semiliterate==0.7.0 mkdocs-simple-plugin==2.1.2 mkdocstrings==0.20.0 mkdocstrings-python==0.8.3 mkdocstrings-python-legacy==0.2.3 mknotebooks==0.7.1 nbclient==0.7.2 nbconvert==7.2.9 nbformat==5.7.3 packaging==23.0 pandocfilters==1.5.0 platformdirs==3.0.0 Pygments==2.14.0 pymdown-extensions==9.9.2 pyrsistent==0.19.3 python-dateutil==2.8.2 python-slugify==8.0.0 pytkdocs==0.16.1 PyYAML==6.0 pyyaml_env_tag==0.1 pyzmq==25.0.0 requests==2.28.2 six==1.16.0 smmap==5.0.0 soupsieve==2.4 text-unidecode==1.3 tinycss2==1.2.1 tornado==6.2 traitlets==5.9.0 urllib3==1.26.14 watchdog==2.2.1 webencodings==0.5.1 zipp==3.14.0","title":"Building docs locally"},{"location":"dev_guides/changing_splink/building_env_locally.html","text":"Building a virtual environment for Splink \u00b6 Splink uses poetry to track and manage our core dependencies and additionally resolve any package conflicts that these may result in. A list of the core dependencies used within Splink can be found in pyproject.toml . To automatically create a simple virtual environment using venv , simply run this script : source scripts/create_venv.sh","title":"Building a Virtual Environment"},{"location":"dev_guides/changing_splink/building_env_locally.html#building-a-virtual-environment-for-splink","text":"Splink uses poetry to track and manage our core dependencies and additionally resolve any package conflicts that these may result in. A list of the core dependencies used within Splink can be found in pyproject.toml . To automatically create a simple virtual environment using venv , simply run this script : source scripts/create_venv.sh","title":"Building a virtual environment for Splink"},{"location":"dev_guides/changing_splink/lint.html","text":"Linting your code \u00b6 For linting, we currently make use of both ruff and black . These are used to ensure we produce readable, maintainable, and more consistent code. To quickly run both linters, simply run this bash script. The -f flag is called to run an automatic fix with ruff. If you simply wish for ruff to print the errors it finds to the console, remove this flag. source scripts/lint.sh -f # with the fix flag source scripts/lint.sh # without Alternatively, you can run ruff and black separately from a terminal instance. For black, you need to run: black . and ruff requires: ruff --fix . for automatic fixes and error printouts or ruff --show-source . for a more thorough breakdown.","title":"Linting"},{"location":"dev_guides/changing_splink/lint.html#linting-your-code","text":"For linting, we currently make use of both ruff and black . These are used to ensure we produce readable, maintainable, and more consistent code. To quickly run both linters, simply run this bash script. The -f flag is called to run an automatic fix with ruff. If you simply wish for ruff to print the errors it finds to the console, remove this flag. source scripts/lint.sh -f # with the fix flag source scripts/lint.sh # without Alternatively, you can run ruff and black separately from a terminal instance. For black, you need to run: black . and ruff requires: ruff --fix . for automatic fixes and error printouts or ruff --show-source . for a more thorough breakdown.","title":"Linting your code"},{"location":"dev_guides/changing_splink/running_tests_locally.html","text":"Running tests locally \u00b6 To run tests locally, simply run: python3 -m pytest tests/ -s --disable-pytest-warnings To run a single test, append the filename to the tests/ folder call: python3 -m pytest tests/my_test_file.py -s --disable-pytest-warnings Running tests with docker \ud83d\udc33 \u00b6 If you want to test Splink against a specific version of python, the easiest method is to utilise docker \ud83d\udc33. Docker allows you to more quickly and easily install a specific version of python and run the existing test library against it. This is particularly useful if you're using py > 3.9.10 (which is currently in use in our tests github action) and need to run a secondary set of tests. A pre-built Dockerfile for running tests against python version 3.9.10 can be located within scripts/run_tests.Dockerfile . To run, simply use the following docker command from within a terminal and the root folder of a splink clone: docker build -t run_tests:testing -f scripts/run_tests.Dockerfile . && docker run run_tests:testing This will both build and run the tests library. Feel free to replace run_tests:testing with an image name and tag you're happy with. Reusing the same image and tag will overwrite your existing image.","title":"Running Tests"},{"location":"dev_guides/changing_splink/running_tests_locally.html#running-tests-locally","text":"To run tests locally, simply run: python3 -m pytest tests/ -s --disable-pytest-warnings To run a single test, append the filename to the tests/ folder call: python3 -m pytest tests/my_test_file.py -s --disable-pytest-warnings","title":"Running tests locally"},{"location":"dev_guides/changing_splink/running_tests_locally.html#running-tests-with-docker","text":"If you want to test Splink against a specific version of python, the easiest method is to utilise docker \ud83d\udc33. Docker allows you to more quickly and easily install a specific version of python and run the existing test library against it. This is particularly useful if you're using py > 3.9.10 (which is currently in use in our tests github action) and need to run a secondary set of tests. A pre-built Dockerfile for running tests against python version 3.9.10 can be located within scripts/run_tests.Dockerfile . To run, simply use the following docker command from within a terminal and the root folder of a splink clone: docker build -t run_tests:testing -f scripts/run_tests.Dockerfile . && docker run run_tests:testing This will both build and run the tests library. Feel free to replace run_tests:testing with an image name and tag you're happy with. Reusing the same image and tag will overwrite your existing image.","title":"Running tests with docker \ud83d\udc33"},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html","tags":["API","comparisons","fuzzy-matching"],"text":"Extending existing comparisons and comparison levels \u00b6 Creating a linkage (or deduplication) model necessitates making various comparisons between (or within) your data sets. There is some choice available in what kind of comparisons you will wish to do for the linkage problem you are dealing with. Splink comes with several comparisons ready to use directly , as well as several comparison levels that you can use to construct your own comparison . You may find that within these you find yourself using a specialised version repeatedly, and would like to make a shorthand for this and contribute it to the Splink library for other users to benefit from - this page will aid you in this process. This guide supplements the guide for adding entirely new comparisons and comparison levels to show how things work when you are extending existing entries. Subclassing existing library comparison levels \u00b6 For this example, let's consider a comparison level that returns a match on two strings within a fixed Hamming distance . This is a specific example of the generic string distance function comparison level . In this case, working in splink/comparison_level_library.py , we simply subclass the appropriate level, and call its constructor, fixing whatever properties we need (using dialect-specific properties where appropriate - in this case the name of the function which calculates Hamming distance, which will be stored in the property self._hamming_name ): class HammingLevelBase ( DistanceFunctionLevelBase ): def __init__ ( self , col_name : str , distance_threshold : int , m_probability = None , ): \"\"\"Represents a comparison using a Hamming distance function, Args: col_name (str): Input column name distance_threshold (Union[int, float]): The threshold to use to assess similarity m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: A comparison level that evaluates the Hamming similarity \"\"\" super () . __init__ ( col_name , self . _hamming_name , distance_threshold , higher_is_more_similar = False , m_probability = m_probability , ) The rest of the process is identical to that described for creating brand-new comparison levels . Subclassing existing library comparisons \u00b6 As in our hamming_level example above, you can similarly subclass existing library Comparisons to create e.g. hamming_at_thresholds from the more generic distance_function_at_thresholds , similarly to how we create new comparisons . The main difficulty here is that the subclassed comparison levels will have different function arguments, so we need to check within the constructor if we are in the generic version ( distance_function_at_thresholds ) or a specific version ( hamming_level ). See the source code for DistanceFunctionAtThreholdsComparisonBase for an example.","title":"Extending existing comparisons and comparison levels"},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#extending-existing-comparisons-and-comparison-levels","text":"Creating a linkage (or deduplication) model necessitates making various comparisons between (or within) your data sets. There is some choice available in what kind of comparisons you will wish to do for the linkage problem you are dealing with. Splink comes with several comparisons ready to use directly , as well as several comparison levels that you can use to construct your own comparison . You may find that within these you find yourself using a specialised version repeatedly, and would like to make a shorthand for this and contribute it to the Splink library for other users to benefit from - this page will aid you in this process. This guide supplements the guide for adding entirely new comparisons and comparison levels to show how things work when you are extending existing entries.","title":"Extending existing comparisons and comparison levels"},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#subclassing-existing-library-comparison-levels","text":"For this example, let's consider a comparison level that returns a match on two strings within a fixed Hamming distance . This is a specific example of the generic string distance function comparison level . In this case, working in splink/comparison_level_library.py , we simply subclass the appropriate level, and call its constructor, fixing whatever properties we need (using dialect-specific properties where appropriate - in this case the name of the function which calculates Hamming distance, which will be stored in the property self._hamming_name ): class HammingLevelBase ( DistanceFunctionLevelBase ): def __init__ ( self , col_name : str , distance_threshold : int , m_probability = None , ): \"\"\"Represents a comparison using a Hamming distance function, Args: col_name (str): Input column name distance_threshold (Union[int, float]): The threshold to use to assess similarity m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: A comparison level that evaluates the Hamming similarity \"\"\" super () . __init__ ( col_name , self . _hamming_name , distance_threshold , higher_is_more_similar = False , m_probability = m_probability , ) The rest of the process is identical to that described for creating brand-new comparison levels .","title":"Subclassing existing library comparison levels"},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#subclassing-existing-library-comparisons","text":"As in our hamming_level example above, you can similarly subclass existing library Comparisons to create e.g. hamming_at_thresholds from the more generic distance_function_at_thresholds , similarly to how we create new comparisons . The main difficulty here is that the subclassed comparison levels will have different function arguments, so we need to check within the constructor if we are in the generic version ( distance_function_at_thresholds ) or a specific version ( hamming_level ). See the source code for DistanceFunctionAtThreholdsComparisonBase for an example.","title":"Subclassing existing library comparisons"},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html","tags":["API","comparisons"],"text":"Creating new comparisons and comparison levels for libraries \u00b6 The Fellegi-Sunter model that Splink implements depends on having several comparisons, which are each composed of two or more comparison levels. Splink provides several ready-made comparisons and comparison levels to use out-of-the-box, but you may find in your particular application that you have to create your own custom versions if there is not a suitable comparison/level for the SQL dialect you are working with (or for any available dialects). Having created a custom comparison you may decide that your use case is common enough that you want to contribute it to Splink for other users to benefit from. This guide will take you through the process of doing so. Looking at existing examples should also prove to be useful for further guidance, and to perhaps serve as a starting template. After creating your new levels/comparisons, be sure to add some tests to help ensure that the code runs without error and behaves as expected. This guide is for adding new comparisons and comparison levels from scratch. If you instead want to create specialised versions of existing levels, be sure to also have a look at the guide for extending existing library entries . Creating new comparison levels \u00b6 For this example, let's consider a comparison level that compares if the length of two arrays are within n of one another (without reference to the contents of these arrays) for some non-negative integer n . An example of this might be if we were linking people with partial address information in two tables --- in one table we have an array of postcodes, and the other table we have an array of road-names. We don't expect them to match, but we are probably interested if the count of the number of objects within each array are similar - each corresponding to the number of addresses per person. To create a new comparison level, you must create a new subclass of ComparisonLevel which will serve as the base comparison level for any SQL dialects that will allow this level, in the file splink/comparison_level_library.py . It will contain the full logic for creating the comparison level - any dialect dependencies will be implemented as properties on the specific dialect-dependent object. In this case we will need to refer to a property _array_length_function_name , as this can vary by dialect. We will not define the property directly on this object, as our dialect-dependent versions will inherit this property from elsewhere. We also include any customisable parameters for our level - in this case we will allow options for the maximum number of elements the array may differ by. class ArrayLengthLevelBase ( ComparisonLevel ): def __init__ ( self , col_name : str , length_difference : int , m_probability = None , ): \"\"\"Compares two arrays whose sizes are within a fixed distance | length(arr_1) - length(arr_2) | <= (length_difference) Arguments: col_name (str): Input column name length_difference (int): Maximum difference in array lengths m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: A comparison level that evaluates the size difference between two arrays \"\"\" col = InputColumn ( col_name , sql_dialect = self . _sql_dialect ) col_l , col_r = col . names_l_r () sql_exp = ( f \"abs(\" f \" { self . _array_length_function_name } ( { col_l } ) - \" f \" { self . _array_length_function_name } ( { col_r } )\" f \") <= { length_difference } \" ) level_dict = { \"sql_condition\" : sql_exp , \"label_for_charts\" : f \"Array sizes differ by at most { length_difference ) \", } if m_probability : level_dict [ \"m_probability\" ] = m_probability super () . __init__ ( level_dict , sql_dialect = self . _sql_dialect ) If you are using a new dialect-dependent property (as we are in this case), then it should be added as a property on DialectBase in splink.dialect_base.py , with either a sensible default value, or raising a NotImplementedError : class DialectBase (): ... @property def _array_length_function_name (): raise NotImplementedError ( \"`array_length_function_name` not implemented on base class\" ) Then any dialects that use a different value can override this (e.g in splink.spark.spark_base.py ): class SparkBase ( DialectBase ): ... @property def _array_length_function_name (): return \"array_size\" Then any dialects where this comparison level can be used can simply inherit from this dialect-specific base, along with the comparison level base ArrayLengthLevelBase - here in splink.spark.spark_comparison_level_library.py : class array_length_level ( SparkBase , ArrayLengthLevelBase ): pass Similarly for DuckDB define the appropriate function name in the base splink.duckdb.duckdb_base.py class DuckDBBase ( DialectBase ): ... @property def _array_length_function_name (): return \"array_length\" and then simply create the level in the corresponding library splink.duckdb.duckdb_comparison_level_library.py : class array_length_level ( DuckDBBase , ArrayLengthLevelBase ): pass The names of these should be the same for all dialects (and written in snake-case), with them being distinguished solely by path. Creating new comparisons \u00b6 The process for creating new library Comparison s is similar to the ComparisonLevel case, but slightly more involved. This is due to the fact that dialect-specific Comparison s need to 'know' about the dialect-specific ComparisonLevel s that they employ. As an example, we will consider a new Comparison that makes use of our new array_length_level above. Specifically, it will have the following levels: an optional exact_match_level one or more array_length_level s with different values of length_difference , as specified an else_level class ArrayLengthAtThresholdsComparisonBase ( Comparison ): def __init__ ( self , col_name : str , length_thresholds : Union [ int , list ] = [ 0 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_sizes : Union [ float , list ] = None , m_probability_else = None , ): \"\"\"A comparison of the data in the array column `col_name` with various size difference thresholds to assess similarity levels. An example of the output with default arguments and settings `length_thresholds = [0]` would be - An exact match - The two arrays are the same length - Anything else (i.e. the arrays are difference lengths) Args: col_name (str): The name of the array column to compare. length_thresholds (Union[int, list], optional): The difference(s) between array sizes of thresholds, to assess whether two arrays are within a given length difference. include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (float, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_sizes (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the sizes specified. Defaults to None. m_probability_else (float, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: A comparison that can be inclued in the Splink settings dictionary. \"\"\" thresholds = ensure_is_iterable ( length_thresholds ) if m_probability_or_probabilities_sizes is None : m_probability_or_probabilities_sizes = [ None ] * len ( thresholds ) m_probabilities = ensure_is_iterable ( m_probability_or_probabilities_sizes ) comparison_levels = [] comparison_levels . append ( self . _null_level ( col_name )) if include_exact_match_level : level = self . _exact_match_level ( col_name , term_frequency_adjustments = term_frequency_adjustments , m_probability = m_probability_exact_match , ) comparison_levels . append ( level ) for length_thres , m_prob in zip ( thresholds , m_probabilities ): level = self . _array_length_level ( col_name , length_difference = length_thres , m_probability = m_prob , ) comparison_levels . append ( level ) comparison_levels . append ( self . _else_level ( m_probability = m_probability_else ), ) comparison_desc = \"\" if include_exact_match_level : comparison_desc += \"Exact match vs. \" thres_desc = \", \" . join ( thresholds ) plural = \"\" if len ( thresholds ) == 1 else \"s\" comparison_desc += ( f \"Array length differences with threshold { plural } { thres_desc } vs. \" ) comparison_desc += \"anything else\" comparison_dict = { \"comparison_description\" : comparison_desc , \"comparison_levels\" : comparison_levels , } super () . __init__ ( comparison_dict ) Crucially we needed to use self._null_level , self._exact_match_level , self._else_level which already exist, but also the new self._array_length_level which relates to our new comparison level. We will need to make sure that the dialect-specific comparisons which will actually be used will have this property. Each dialect has a comparison properties base, which stores information about all of the dialect-specific comparison levels used by all comparisons. We will need to add our new level to this, which we referred to above in ArrayLengthAtThresholdsComparisonBase - for this example in splink.spark.spark_comparison_library.py : from splink.spark.spark_comparison_level_library import ( exact_match_level , ... array_length_level , ) ... class SparkComparisonProperties ( SparkBase ): @property def _exact_match_level ( self ): return exact_match_level ... @property def _array_length_level ( self ): return array_length_level Any dialect-specific version of comparisons will inherit from this (where it learns about the dialect-specific comparison levels), and the comparison itself; in our case, in the same file: ... class array_length_at_thresholds ( SparkComparisonProperties , ArrayLengthAtThresholds ): pass This is now ready to import and be used, just as any other pre-existing comparisons.","title":"Creating new comparisons and comparison levels"},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparisons-and-comparison-levels-for-libraries","text":"The Fellegi-Sunter model that Splink implements depends on having several comparisons, which are each composed of two or more comparison levels. Splink provides several ready-made comparisons and comparison levels to use out-of-the-box, but you may find in your particular application that you have to create your own custom versions if there is not a suitable comparison/level for the SQL dialect you are working with (or for any available dialects). Having created a custom comparison you may decide that your use case is common enough that you want to contribute it to Splink for other users to benefit from. This guide will take you through the process of doing so. Looking at existing examples should also prove to be useful for further guidance, and to perhaps serve as a starting template. After creating your new levels/comparisons, be sure to add some tests to help ensure that the code runs without error and behaves as expected. This guide is for adding new comparisons and comparison levels from scratch. If you instead want to create specialised versions of existing levels, be sure to also have a look at the guide for extending existing library entries .","title":"Creating new comparisons and comparison levels for libraries"},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparison-levels","text":"For this example, let's consider a comparison level that compares if the length of two arrays are within n of one another (without reference to the contents of these arrays) for some non-negative integer n . An example of this might be if we were linking people with partial address information in two tables --- in one table we have an array of postcodes, and the other table we have an array of road-names. We don't expect them to match, but we are probably interested if the count of the number of objects within each array are similar - each corresponding to the number of addresses per person. To create a new comparison level, you must create a new subclass of ComparisonLevel which will serve as the base comparison level for any SQL dialects that will allow this level, in the file splink/comparison_level_library.py . It will contain the full logic for creating the comparison level - any dialect dependencies will be implemented as properties on the specific dialect-dependent object. In this case we will need to refer to a property _array_length_function_name , as this can vary by dialect. We will not define the property directly on this object, as our dialect-dependent versions will inherit this property from elsewhere. We also include any customisable parameters for our level - in this case we will allow options for the maximum number of elements the array may differ by. class ArrayLengthLevelBase ( ComparisonLevel ): def __init__ ( self , col_name : str , length_difference : int , m_probability = None , ): \"\"\"Compares two arrays whose sizes are within a fixed distance | length(arr_1) - length(arr_2) | <= (length_difference) Arguments: col_name (str): Input column name length_difference (int): Maximum difference in array lengths m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: A comparison level that evaluates the size difference between two arrays \"\"\" col = InputColumn ( col_name , sql_dialect = self . _sql_dialect ) col_l , col_r = col . names_l_r () sql_exp = ( f \"abs(\" f \" { self . _array_length_function_name } ( { col_l } ) - \" f \" { self . _array_length_function_name } ( { col_r } )\" f \") <= { length_difference } \" ) level_dict = { \"sql_condition\" : sql_exp , \"label_for_charts\" : f \"Array sizes differ by at most { length_difference ) \", } if m_probability : level_dict [ \"m_probability\" ] = m_probability super () . __init__ ( level_dict , sql_dialect = self . _sql_dialect ) If you are using a new dialect-dependent property (as we are in this case), then it should be added as a property on DialectBase in splink.dialect_base.py , with either a sensible default value, or raising a NotImplementedError : class DialectBase (): ... @property def _array_length_function_name (): raise NotImplementedError ( \"`array_length_function_name` not implemented on base class\" ) Then any dialects that use a different value can override this (e.g in splink.spark.spark_base.py ): class SparkBase ( DialectBase ): ... @property def _array_length_function_name (): return \"array_size\" Then any dialects where this comparison level can be used can simply inherit from this dialect-specific base, along with the comparison level base ArrayLengthLevelBase - here in splink.spark.spark_comparison_level_library.py : class array_length_level ( SparkBase , ArrayLengthLevelBase ): pass Similarly for DuckDB define the appropriate function name in the base splink.duckdb.duckdb_base.py class DuckDBBase ( DialectBase ): ... @property def _array_length_function_name (): return \"array_length\" and then simply create the level in the corresponding library splink.duckdb.duckdb_comparison_level_library.py : class array_length_level ( DuckDBBase , ArrayLengthLevelBase ): pass The names of these should be the same for all dialects (and written in snake-case), with them being distinguished solely by path.","title":"Creating new comparison levels"},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparisons","text":"The process for creating new library Comparison s is similar to the ComparisonLevel case, but slightly more involved. This is due to the fact that dialect-specific Comparison s need to 'know' about the dialect-specific ComparisonLevel s that they employ. As an example, we will consider a new Comparison that makes use of our new array_length_level above. Specifically, it will have the following levels: an optional exact_match_level one or more array_length_level s with different values of length_difference , as specified an else_level class ArrayLengthAtThresholdsComparisonBase ( Comparison ): def __init__ ( self , col_name : str , length_thresholds : Union [ int , list ] = [ 0 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_sizes : Union [ float , list ] = None , m_probability_else = None , ): \"\"\"A comparison of the data in the array column `col_name` with various size difference thresholds to assess similarity levels. An example of the output with default arguments and settings `length_thresholds = [0]` would be - An exact match - The two arrays are the same length - Anything else (i.e. the arrays are difference lengths) Args: col_name (str): The name of the array column to compare. length_thresholds (Union[int, list], optional): The difference(s) between array sizes of thresholds, to assess whether two arrays are within a given length difference. include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (float, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_sizes (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the sizes specified. Defaults to None. m_probability_else (float, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: A comparison that can be inclued in the Splink settings dictionary. \"\"\" thresholds = ensure_is_iterable ( length_thresholds ) if m_probability_or_probabilities_sizes is None : m_probability_or_probabilities_sizes = [ None ] * len ( thresholds ) m_probabilities = ensure_is_iterable ( m_probability_or_probabilities_sizes ) comparison_levels = [] comparison_levels . append ( self . _null_level ( col_name )) if include_exact_match_level : level = self . _exact_match_level ( col_name , term_frequency_adjustments = term_frequency_adjustments , m_probability = m_probability_exact_match , ) comparison_levels . append ( level ) for length_thres , m_prob in zip ( thresholds , m_probabilities ): level = self . _array_length_level ( col_name , length_difference = length_thres , m_probability = m_prob , ) comparison_levels . append ( level ) comparison_levels . append ( self . _else_level ( m_probability = m_probability_else ), ) comparison_desc = \"\" if include_exact_match_level : comparison_desc += \"Exact match vs. \" thres_desc = \", \" . join ( thresholds ) plural = \"\" if len ( thresholds ) == 1 else \"s\" comparison_desc += ( f \"Array length differences with threshold { plural } { thres_desc } vs. \" ) comparison_desc += \"anything else\" comparison_dict = { \"comparison_description\" : comparison_desc , \"comparison_levels\" : comparison_levels , } super () . __init__ ( comparison_dict ) Crucially we needed to use self._null_level , self._exact_match_level , self._else_level which already exist, but also the new self._array_length_level which relates to our new comparison level. We will need to make sure that the dialect-specific comparisons which will actually be used will have this property. Each dialect has a comparison properties base, which stores information about all of the dialect-specific comparison levels used by all comparisons. We will need to add our new level to this, which we referred to above in ArrayLengthAtThresholdsComparisonBase - for this example in splink.spark.spark_comparison_library.py : from splink.spark.spark_comparison_level_library import ( exact_match_level , ... array_length_level , ) ... class SparkComparisonProperties ( SparkBase ): @property def _exact_match_level ( self ): return exact_match_level ... @property def _array_length_level ( self ): return array_length_level Any dialect-specific version of comparisons will inherit from this (where it learns about the dialect-specific comparison levels), and the comparison itself; in our case, in the same file: ... class array_length_at_thresholds ( SparkComparisonProperties , ArrayLengthAtThresholds ): pass This is now ready to import and be used, just as any other pre-existing comparisons.","title":"Creating new comparisons"},{"location":"includes/abbreviations.html","text":"","title":"Abbreviations"},{"location":"includes/tags.html","text":"Tags \u00b6 Following is a list of relevant tags: [TAGS]","title":"Tags"},{"location":"includes/tags.html#tags","text":"Following is a list of relevant tags: [TAGS]","title":"Tags"},{"location":"includes/generated_files/comparison_level_library_dialect_table.html","text":"spark duckdb athena sqlite array_intersect_level \u2713 \u2713 \u2713 columns_reversed_level \u2713 \u2713 \u2713 \u2713 datediff_level \u2713 \u2713 distance_function_level \u2713 \u2713 \u2713 \u2713 distance_in_km_level \u2713 \u2713 \u2713 else_level \u2713 \u2713 \u2713 \u2713 exact_match_level \u2713 \u2713 \u2713 \u2713 jaccard_level \u2713 \u2713 jaro_winkler_level \u2713 \u2713 levenshtein_level \u2713 \u2713 \u2713 null_level \u2713 \u2713 \u2713 \u2713 percentage_difference_level \u2713 \u2713 \u2713 \u2713","title":"Comparison level library dialect table"},{"location":"includes/generated_files/comparison_library_dialect_table.html","text":"spark duckdb athena sqlite array_intersect_at_sizes \u2713 \u2713 \u2713 datediff_at_thresholds \u2713 \u2713 distance_function_at_thresholds \u2713 \u2713 \u2713 \u2713 distance_in_km_at_thresholds \u2713 \u2713 \u2713 exact_match \u2713 \u2713 \u2713 \u2713 jaccard_at_thresholds \u2713 \u2713 jaro_winkler_at_thresholds \u2713 \u2713 levenshtein_at_thresholds \u2713 \u2713 \u2713","title":"Comparison library dialect table"},{"location":"settingseditor/editor.html","tags":["settings"],"text":"","title":"Settings Editor"},{"location":"topic_guides/backends.html","tags":["Spark","DuckDB","Athena","SQLite","Backends"],"text":"Splink's SQL backends: Spark, DuckDB, etc \u00b6 Splink is a Python library. It implements all data linking computations by generating SQL, and submitting the SQL statements to a backend of the user's chosing for execution. For smaller input datasets of up to 1-2 million records, users can link data in Python on their laptop using the DuckDB backend. This is the recommended approach because the DuckDB backend is installed automatically when the user installs Splink using pip install splink . No additional configuration is needed. Linking larger datasets requires highly computationally intensive calculations, and generates datasets which are too large to be processed on a standard laptop. For these scenarios, we recommend using one of Splink's big data backend - currently Spark or AWS Athena. When these backends are used, the SQL generated by Splink is sent to the chosen backend for execution. The Splink code you write is almost identical between backends, so it's straightforward to migrate between backends. Often, it's a good idea to start working using DuckDB on a sample of data, because it will produce results very quickly. When you're comfortable with your model, you may wish to migrate to a big data backend to estimate/predict on the full dataset. Choosing a backend \u00b6 Import the linker from the backend of your choosing, and the backend-specific comparison libraries. Once you have initialised the linker object, there is no difference in the subequent code between backends. Note however, that not all comparison functions are available in all backends. There are tables detailing the available functions for each backend on the comparison library API page and the comparison level library API page . DuckDB \u00b6 from splink.duckdb.duckdb_linker import DuckDBLinker import splink.duckdb.duckdb_comparison_library as cl import splink.duckdb.duckdb_comparison_level_library as cll linker = DuckDBLinker ( your_args ) Spark \u00b6 from splink.spark.spark_linker import SparkLinker import splink.spark.spark_comparison_library as cl import splink.spark.spark_comparison_level_library as cll linker = SparkLinker ( your_args ) AWS Athena \u00b6 from splink.athena.athena_linker import AthenaLinker import splink.athena.athena_comparison_library as cl import splink.athena.athena_comparison_level_library as cll linker = AthenaLinker ( your_args ) SQlite \u00b6 from splink.sqlite.sqlite_linker import SQLiteLinker import splink.sqlite.sqlite_comparison_library as cl import splink.sqlite.sqlite_comparison_level_library as cll linker = SQLiteLinker ( your_args )","title":"Splink's SQL backends - Spark, DuckDB etc"},{"location":"topic_guides/backends.html#splinks-sql-backends-spark-duckdb-etc","text":"Splink is a Python library. It implements all data linking computations by generating SQL, and submitting the SQL statements to a backend of the user's chosing for execution. For smaller input datasets of up to 1-2 million records, users can link data in Python on their laptop using the DuckDB backend. This is the recommended approach because the DuckDB backend is installed automatically when the user installs Splink using pip install splink . No additional configuration is needed. Linking larger datasets requires highly computationally intensive calculations, and generates datasets which are too large to be processed on a standard laptop. For these scenarios, we recommend using one of Splink's big data backend - currently Spark or AWS Athena. When these backends are used, the SQL generated by Splink is sent to the chosen backend for execution. The Splink code you write is almost identical between backends, so it's straightforward to migrate between backends. Often, it's a good idea to start working using DuckDB on a sample of data, because it will produce results very quickly. When you're comfortable with your model, you may wish to migrate to a big data backend to estimate/predict on the full dataset.","title":"Splink's SQL backends: Spark, DuckDB, etc"},{"location":"topic_guides/backends.html#choosing-a-backend","text":"Import the linker from the backend of your choosing, and the backend-specific comparison libraries. Once you have initialised the linker object, there is no difference in the subequent code between backends. Note however, that not all comparison functions are available in all backends. There are tables detailing the available functions for each backend on the comparison library API page and the comparison level library API page .","title":"Choosing a backend"},{"location":"topic_guides/backends.html#duckdb","text":"from splink.duckdb.duckdb_linker import DuckDBLinker import splink.duckdb.duckdb_comparison_library as cl import splink.duckdb.duckdb_comparison_level_library as cll linker = DuckDBLinker ( your_args )","title":"DuckDB"},{"location":"topic_guides/backends.html#spark","text":"from splink.spark.spark_linker import SparkLinker import splink.spark.spark_comparison_library as cl import splink.spark.spark_comparison_level_library as cll linker = SparkLinker ( your_args )","title":"Spark"},{"location":"topic_guides/backends.html#aws-athena","text":"from splink.athena.athena_linker import AthenaLinker import splink.athena.athena_comparison_library as cl import splink.athena.athena_comparison_level_library as cll linker = AthenaLinker ( your_args )","title":"AWS Athena"},{"location":"topic_guides/backends.html#sqlite","text":"from splink.sqlite.sqlite_linker import SQLiteLinker import splink.sqlite.sqlite_comparison_library as cl import splink.sqlite.sqlite_comparison_level_library as cll linker = SQLiteLinker ( your_args )","title":"SQlite"},{"location":"topic_guides/blocking_rules.html","tags":["Blocking","Performance","Model Training","M Probability","Expectation Maximisation"],"text":"Difference between blocking_rules_to_generate_predictions vs blocking rules for estimation \u00b6 What is the difference between the list of blocking_rules_to_generate_predictions specifed in the Splink settings dictionary, and the blocking rule that must be provided as an argument to estimate_parameters_using_expectation_maximisation ? These two kinds of blocking rules can be seen in the following code snippet: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)\" , \"l.dob = r.dob\" , ], \"comparisons\" : [ levenshtein_at_thresholds ( \"first_name\" , 2 ), exact_match ( \"surname\" ), exact_match ( \"dob\" ), exact_match ( \"city\" , term_frequency_adjustments = True ), exact_match ( \"email\" ), ], } linker = DuckDBLinker ( df , settings ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) blocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) blocking_rule_for_training = \"l.dob = r.dob and l.city = r.city\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) The answer is that they serve different purposes. What is a blocking rule? \u00b6 Blocking rules are needed because it is usually computationally intractable to compare every record with every other. A blocking rule specifies a constraint on how Splink generates pairwise record comparisons, dramatically reducing the total number of comparisons generated. For example, the blocking rule \"l.first_name = r.first_name and l.surname = r.surname\" will generate pairwise record comparisons amongst pairwise comparisons where first name and surname match. The purpose of blocking_rules_to_generate_predictions \u00b6 blocking_rules_to_generate_predictions are used by Splink when the user called linker.predict() . The purpose of these blocking rules is to try and ensure that pairwise record comparisons are generated for all true matches. For example, settings = { \"blocking_rules_to_generate_predictions\" [ \"l.first_name = r.first_name and l.surname = r.surname\" ] } will generate comparisons for all true matches where names match. But it would miss a true match where there was a typo in (say) the first name. In general, it is usually impossible to find a single rule which both: Reduces the number of comparisons generated to a computatally tractable number Ensures comparisons are generated for all true matches This is why blocking_rules_to_generate_predictions is a list. Suppose we also block on postcode : settings = { \"blocking_rules_to_generate_predictions\" [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.postcode = r.postcode\" ] } We will now generate a pairwise comparison for the record where there was a typo in the first name, so long as there isn't also a difference in the postcode. By specifying a variety of blocking_rules_to_generate_predictions , it becomes implausible that a truly matching record would not be captured by at least one of the rules. Note that Splink automatically deduplicates the record comparisons it generates. So, in the example above, the \"l.postcode = r.postcode\" blocking rule generates only records comparisons that were not already captured by the first_name and surname rule. The purpose of the blocking_rule parameter on estimate_parameters_using_expectation_maximisation \u00b6 The purpose of this blocking rule is to reduce the number of pairwise generated to a computationally-tractable number to enable the expectation maximisation algorithm to work. The expectation maximisation algorithm seems to work best when the pairwise record comparisons are a mix of anywhere between around 0.1% and 99.9% true matches. It works less effectively if there are very few examples of either matches or non-matches. It works less efficiently if there is a huge imbalance between the two (e.g. a billion non matches and only a hundred matches). It does not matter if this blocking rule excludes some true matches - it just needs to generate examples of matches and non matches. Since they serve different purposes, the blocking rules most appropriate to use with blocking_rules_to_generate_predictions will often be different to those for estimate_parameters_using_expectation_maximisation , but it is also common for the same rule to be used in both places.","title":"Blocking rules for prediction vs estimation"},{"location":"topic_guides/blocking_rules.html#difference-between-blocking_rules_to_generate_predictions-vs-blocking-rules-for-estimation","text":"What is the difference between the list of blocking_rules_to_generate_predictions specifed in the Splink settings dictionary, and the blocking rule that must be provided as an argument to estimate_parameters_using_expectation_maximisation ? These two kinds of blocking rules can be seen in the following code snippet: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)\" , \"l.dob = r.dob\" , ], \"comparisons\" : [ levenshtein_at_thresholds ( \"first_name\" , 2 ), exact_match ( \"surname\" ), exact_match ( \"dob\" ), exact_match ( \"city\" , term_frequency_adjustments = True ), exact_match ( \"email\" ), ], } linker = DuckDBLinker ( df , settings ) linker . estimate_u_using_random_sampling ( max_pairs = 1e6 ) blocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) blocking_rule_for_training = \"l.dob = r.dob and l.city = r.city\" linker . estimate_parameters_using_expectation_maximisation ( blocking_rule_for_training ) The answer is that they serve different purposes.","title":"Difference between blocking_rules_to_generate_predictions vs blocking rules for estimation"},{"location":"topic_guides/blocking_rules.html#what-is-a-blocking-rule","text":"Blocking rules are needed because it is usually computationally intractable to compare every record with every other. A blocking rule specifies a constraint on how Splink generates pairwise record comparisons, dramatically reducing the total number of comparisons generated. For example, the blocking rule \"l.first_name = r.first_name and l.surname = r.surname\" will generate pairwise record comparisons amongst pairwise comparisons where first name and surname match.","title":"What is a blocking rule?"},{"location":"topic_guides/blocking_rules.html#the-purpose-of-blocking_rules_to_generate_predictions","text":"blocking_rules_to_generate_predictions are used by Splink when the user called linker.predict() . The purpose of these blocking rules is to try and ensure that pairwise record comparisons are generated for all true matches. For example, settings = { \"blocking_rules_to_generate_predictions\" [ \"l.first_name = r.first_name and l.surname = r.surname\" ] } will generate comparisons for all true matches where names match. But it would miss a true match where there was a typo in (say) the first name. In general, it is usually impossible to find a single rule which both: Reduces the number of comparisons generated to a computatally tractable number Ensures comparisons are generated for all true matches This is why blocking_rules_to_generate_predictions is a list. Suppose we also block on postcode : settings = { \"blocking_rules_to_generate_predictions\" [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.postcode = r.postcode\" ] } We will now generate a pairwise comparison for the record where there was a typo in the first name, so long as there isn't also a difference in the postcode. By specifying a variety of blocking_rules_to_generate_predictions , it becomes implausible that a truly matching record would not be captured by at least one of the rules. Note that Splink automatically deduplicates the record comparisons it generates. So, in the example above, the \"l.postcode = r.postcode\" blocking rule generates only records comparisons that were not already captured by the first_name and surname rule.","title":"The purpose of blocking_rules_to_generate_predictions"},{"location":"topic_guides/blocking_rules.html#the-purpose-of-the-blocking_rule-parameter-on-estimate_parameters_using_expectation_maximisation","text":"The purpose of this blocking rule is to reduce the number of pairwise generated to a computationally-tractable number to enable the expectation maximisation algorithm to work. The expectation maximisation algorithm seems to work best when the pairwise record comparisons are a mix of anywhere between around 0.1% and 99.9% true matches. It works less effectively if there are very few examples of either matches or non-matches. It works less efficiently if there is a huge imbalance between the two (e.g. a billion non matches and only a hundred matches). It does not matter if this blocking rule excludes some true matches - it just needs to generate examples of matches and non matches. Since they serve different purposes, the blocking rules most appropriate to use with blocking_rules_to_generate_predictions will often be different to those for estimate_parameters_using_expectation_maximisation , but it is also common for the same rule to be used in both places.","title":"The purpose of the blocking_rule parameter on estimate_parameters_using_expectation_maximisation"},{"location":"topic_guides/comparators.html","tags":["API","comparisons","Levenstein","Jaro-Winkler","Jaccard"],"text":"Comparators \u00b6 String comparators are algorithms used in data linkage to compare and evaluate the similarity between two strings of text. These algorithms can be applied to different types of data, including names, addresses, and other identifying information, to determine if two records refer to the same individual or entity. One common use of string comparators in data linkage is to identify duplicates within a dataset. By comparing the strings of two records and calculating their similarity, the algorithm can flag records that may be duplicates and require further investigation. This can help to improve the accuracy and completeness of the data by eliminating incorrect or redundant information. Another use of string comparators in data linkage is to merge different datasets or databases. By comparing strings across multiple sources, the algorithm can identify records that refer to the same individual or entity and merge them into a single record. This can be useful for creating a more comprehensive and accurate view of the data, and for identifying trends and patterns that may not be visible in a single dataset. Jaro similarity \u00b6 The Jaro similarity measure is a string similarity measure that quantifies the similarity between two strings based on the number of common characters between them and their relative position. It is often used in applications such as entity resolution and duplicate detection, and is particularly useful for comparing strings that are similar in length and contain similar characters. The formula for Jaro similarity is as follows: \\[Jaro = \\frac{1}{3} \\left[ \\frac{m}{|s_1|} + \\frac{m}{|s_2|} + \\frac{m-t}{m} \\right]\\] where \\(s_1\\) and \\(s_2\\) are the two strings being compared, \\(m\\) is the number of common characters (which are considered matching only if they are the same and not farther than \\(\\left\\lfloor \\frac{\\min(|s_1|,|s_2|)}{2} \\right\\rfloor - 1\\) characters apart), and \\(t\\) is the number of transpositions (which is calculated as the number of matching characters that are not in the right order divided by two). Jaro similarity is not currently supported in splink. If you think this would be useful, please let us know, or contribute it yourself, on github. Jaro Winkler similarity \u00b6 The formula for Jaro Winkler is: \\[Jaro Winkler = Jaro + p \\cdot l \\cdot (1 - Jaro)\\] where : \\(Jaro\\) is the Jaro similarity score, \\(p\\) is the scaling factorfor how much the score is adjusted upwards for having common prefixes. \\(p\\) should not exceed 0.25 (i.e. 1/4, with 4 being the maximum length of the prefix being considered), otherwise the similarity could become larger than 1. The standard value for this constant in Winkler's work is \\(p\\) = 0.1 ) \\(l\\) is the length of the common prefix, and \\(1 - Jaro\\) is the penalty for non-matching characters at the beginning of the strings. See the comparison library and comparison level library docs for the Jaro Winkler functions in splink. Levenstein edit distance \u00b6 The Levenstein edit distance is a measure of the similarity between two strings of text. It is calculated by counting the number of operations (insertions, deletions, or substitutions) needed to transform one string into the other. The formula for Levenstein Distance, also known as Edit Distance, is: \\[Levenstein(s_1, s_2) = \\min \\lbrace \\begin{array}{l} \\text{insertion} \\ , \\text{deletion} , \\text{substitution} \\end{array} \\rbrace \\] where \\(s_1\\) and \\(s_2\\) are the two strings being compared. This metric measures the minimum number of edit operations (insertions, deletions, and substitutions) required to transform one string into the other, and can be used to compare strings that may not be identical but are still similar. See the comparison library and comparison level library docs for the Levenshtein functions in splink. Jaccard similarity \u00b6 The Jaccard similarity is a measure of similarity between two sets of data, and is often used in text analysis to compare the similarity of two string/documents. The Jaccard similarity formula is: \\[J(A,B)=\\frac{|A \\cap B|}{|A \\cup B|}\\] In order for the algorith to work it is necessary to first divide each string into \"shingles.\" Shingles refer to fixed-size subsets of a given set of data. For example, if we have a field containing a long string of words such as an address or a company name, the algorithm divides it into shingles of a fixed size (e.g. one word per shingle or perhaps a trigram). This creates a set of shingles from the original data, which allows us to efficiently calculate the Jaccard similarity between it and other sets of data by comparing the shingles they share See the comparison library and comparison level library docs for the Jaccard functions in splink.","title":"Comparators"},{"location":"topic_guides/comparators.html#comparators","text":"String comparators are algorithms used in data linkage to compare and evaluate the similarity between two strings of text. These algorithms can be applied to different types of data, including names, addresses, and other identifying information, to determine if two records refer to the same individual or entity. One common use of string comparators in data linkage is to identify duplicates within a dataset. By comparing the strings of two records and calculating their similarity, the algorithm can flag records that may be duplicates and require further investigation. This can help to improve the accuracy and completeness of the data by eliminating incorrect or redundant information. Another use of string comparators in data linkage is to merge different datasets or databases. By comparing strings across multiple sources, the algorithm can identify records that refer to the same individual or entity and merge them into a single record. This can be useful for creating a more comprehensive and accurate view of the data, and for identifying trends and patterns that may not be visible in a single dataset.","title":"Comparators"},{"location":"topic_guides/comparators.html#jaro-similarity","text":"The Jaro similarity measure is a string similarity measure that quantifies the similarity between two strings based on the number of common characters between them and their relative position. It is often used in applications such as entity resolution and duplicate detection, and is particularly useful for comparing strings that are similar in length and contain similar characters. The formula for Jaro similarity is as follows: \\[Jaro = \\frac{1}{3} \\left[ \\frac{m}{|s_1|} + \\frac{m}{|s_2|} + \\frac{m-t}{m} \\right]\\] where \\(s_1\\) and \\(s_2\\) are the two strings being compared, \\(m\\) is the number of common characters (which are considered matching only if they are the same and not farther than \\(\\left\\lfloor \\frac{\\min(|s_1|,|s_2|)}{2} \\right\\rfloor - 1\\) characters apart), and \\(t\\) is the number of transpositions (which is calculated as the number of matching characters that are not in the right order divided by two). Jaro similarity is not currently supported in splink. If you think this would be useful, please let us know, or contribute it yourself, on github.","title":"Jaro similarity"},{"location":"topic_guides/comparators.html#jaro-winkler-similarity","text":"The formula for Jaro Winkler is: \\[Jaro Winkler = Jaro + p \\cdot l \\cdot (1 - Jaro)\\] where : \\(Jaro\\) is the Jaro similarity score, \\(p\\) is the scaling factorfor how much the score is adjusted upwards for having common prefixes. \\(p\\) should not exceed 0.25 (i.e. 1/4, with 4 being the maximum length of the prefix being considered), otherwise the similarity could become larger than 1. The standard value for this constant in Winkler's work is \\(p\\) = 0.1 ) \\(l\\) is the length of the common prefix, and \\(1 - Jaro\\) is the penalty for non-matching characters at the beginning of the strings. See the comparison library and comparison level library docs for the Jaro Winkler functions in splink.","title":"Jaro Winkler similarity"},{"location":"topic_guides/comparators.html#levenstein-edit-distance","text":"The Levenstein edit distance is a measure of the similarity between two strings of text. It is calculated by counting the number of operations (insertions, deletions, or substitutions) needed to transform one string into the other. The formula for Levenstein Distance, also known as Edit Distance, is: \\[Levenstein(s_1, s_2) = \\min \\lbrace \\begin{array}{l} \\text{insertion} \\ , \\text{deletion} , \\text{substitution} \\end{array} \\rbrace \\] where \\(s_1\\) and \\(s_2\\) are the two strings being compared. This metric measures the minimum number of edit operations (insertions, deletions, and substitutions) required to transform one string into the other, and can be used to compare strings that may not be identical but are still similar. See the comparison library and comparison level library docs for the Levenshtein functions in splink.","title":"Levenstein edit distance"},{"location":"topic_guides/comparators.html#jaccard-similarity","text":"The Jaccard similarity is a measure of similarity between two sets of data, and is often used in text analysis to compare the similarity of two string/documents. The Jaccard similarity formula is: \\[J(A,B)=\\frac{|A \\cap B|}{|A \\cup B|}\\] In order for the algorith to work it is necessary to first divide each string into \"shingles.\" Shingles refer to fixed-size subsets of a given set of data. For example, if we have a field containing a long string of words such as an address or a company name, the algorithm divides it into shingles of a fixed size (e.g. one word per shingle or perhaps a trigram). This creates a set of shingles from the original data, which allows us to efficiently calculate the Jaccard similarity between it and other sets of data by comparing the shingles they share See the comparison library and comparison level library docs for the Jaccard functions in splink.","title":"Jaccard similarity"},{"location":"topic_guides/customising_comparisons.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Defining and customising how record comparisons are made \u00b6 A key feature of Splink is the ability to customise how record comparisons are made - that is, how similarity is defined for different data types. For example, the definition of similarity that is appropriate for a date of birth field is different than for a first name field. By tailoring the definitions of similarity, linking models are more effectively able to distinguish beteween different gradations of similarity, leading to more accurate data linking models. Note that for performance reasons, Splink requires the user to define n discrete levels (gradations) of similarity. Comparing information \u00b6 Comparisons are defined on pairwise record comparisons. Suppose for instance your data contains first_name and surname and dob : id first_name surname dob 1 john smith 1991-04-11 2 jon smith 1991-04-17 3 john smyth 1991-04-11 To compare these records, at the blocking stage, Splink will set these records against each other in a table of pairwise record comparisons: id_l id_r first_name_l first_name_r surname_l surname_r dob_l dob_r 1 2 john jon smith smith 1991-04-11 1991-04-17 1 3 john john smith smyth 1991-04-11 1991-04-11 2 3 jon john smith smyth 1991-04-17 1991-04-11 When defining comparisons, we are defining rules that operate on each row of this latter table of pairwise comparisons Comparisons , ComparisonTemplates and ComparisonLevels \u00b6 A Splink model contains a collection of Comparisons and ComparisonLevels organised in a hierarchy. An example is as follows: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: Up to one character difference \u2502 \u251c\u2500-- ComparisonLevel: Up to three character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. A fuller description of Comaprison s and ComparisonLevel s can be found here and here respectively. How are these comparisons specified? Three ways of specifying Comparisons \u00b6 In Splink, there are three ways of specifying Comparisons : Using pre-baked comparisons from a backend's ComparisonLibrary or ComparisonTemplateLibrary . (Most simple/succinct) Composing pre-defined ComparisonLevels from a backend's ComparisonLevelLibrary Writing a full spec of a Comparison by hand (most verbose/flexible) Method 1: Using the ComparisonLibrary \u00b6 The ComparisonLibrary for a each backend ( DuckDB , Spark , etc.) contains pre-baked similarity functions that cover many common use cases. These functions generate an entire Comparison , composed of several ComparisonLevels The following provides an example of using the ComparisonLibrary for DuckDB. from splink.duckdb.duckdb_comparison_library import ( exact_match , levenshtein_at_thresholds , ) first_name_comparison = exact_match ( \"first_name\" ) print ( first_name_comparison . human_readable_description ) Comparison 'Exact match vs. anything else' of \"first_name\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL - 'Exact match' with SQL rule: \"first_name_l\" = \"first_name_r\" - 'All other comparisons' with SQL rule: ELSE Note that, under the hood, these functions generate a Python dictionary, which conforms to the underlying .json specification of a model: first_name_comparison . as_dict () {'output_column_name': 'first_name', 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL', 'label_for_charts': 'Null', 'is_null_level': True}, {'sql_condition': '\"first_name_l\" = \"first_name_r\"', 'label_for_charts': 'Exact match'}, {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}], 'comparison_description': 'Exact match vs. anything else'} We can now generate a second, more complex comparison: from splink.duckdb.duckdb_comparison_library import ( exact_match , levenshtein_at_thresholds , ) dob_comparison = levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ]) print ( dob_comparison . human_readable_description ) Comparison 'Exact match vs. levenshtein at thresholds 1, 2 vs. anything else' of \"dob\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"dob_l\" IS NULL OR \"dob_r\" IS NULL - 'Exact match' with SQL rule: \"dob_l\" = \"dob_r\" - 'Levenshtein <= 1' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") <= 1 - 'Levenshtein <= 2' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") <= 2 - 'All other comparisons' with SQL rule: ELSE These Comparisons can be specified in a data linking model as follows: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ exact_match ( \"first_name\" ), levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ]), ], } Method 2: Using the ComparisonTemplateLibrary \u00b6 The ComparisonTemplateLibrary is very similar to ComparisonLibrary in that it contains pre-baked similarity functions for each backend (DuckDB, Spark, etc.) to cover common use cases. The key difference is that ComparisonTemplateLibrary contains functions to generate a 'best practice' Comparison based on the type of data in a given column. This includes: How comparison is structured (what comparison levels are included, and in what order) Default parameters (e.g. levenshtein_thresholds = [1,2] ) The following provides an example of using the ComparisonTemplateLibrary for DuckDB. from splink.duckdb.duckdb_comparison_template_library import ( date_comparison ) date_of_birth_comparison = date_comparison ( \"date_of_birth\" ) print ( date_of_birth_comparison . human_readable_description ) Comparison 'Exact match vs. Dates within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\" - 'Levenshtein <= 1' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 1 - 'Levenshtein <= 2' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 2 - 'Within 1 year' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'Within 10 years' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 10 - 'All other comparisons' with SQL rule: ELSE These Comparisons can be specified in a data linking model as follows: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ exact_match ( \"first_name\" ), date_comparison ( \"dob\" ), ] } You can customise a ComparisonTemplate by choosing your own values for the function parameters , but for anything more bespoke you will want to construct a Comparison with ComparisonLevels or provide the spec as a dictionary. Method 3: ComparisonLevels \u00b6 The ComparisonLevels API provides a lower-level API that gives the user greater control over their comparisons. For example, the user may wish to specify a comparison that has levels for a match on dmetaphone and jaro_winkler of the first_name field. The below example assumes the user has derived a column dmeta_first_name which contains the dmetaphone of the first name. from splink.spark.spark_comparison_level_library import ( exact_match_level , null_level , else_level , ) from splink.spark.spark_comparison_library import levenshtein_at_thresholds comparison_first_name = { \"output_column_name\" : \"first_name\" , \"comparison_description\" : \"First name jaro dmeta\" , \"comparison_levels\" : [ null_level ( \"first_name\" ), exact_match_level ( \"first_name\" , term_frequency_adjustments = True ), exact_match_level ( \"dmeta_first_name\" , term_frequency_adjustments = True ), else_level (), ], } from splink.comparison import Comparison print ( Comparison ( comparison_first_name ) . human_readable_description ) Comparison 'First name jaro dmeta' of `first_name` and `dmeta_first_name`. Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: `first_name_l` IS NULL OR `first_name_r` IS NULL - 'Exact match' with SQL rule: `first_name_l` = `first_name_r` - 'Exact match' with SQL rule: `dmeta_first_name_l` = `dmeta_first_name_r` - 'All other comparisons' with SQL rule: ELSE This can now be specified in the settings dictionary as follows: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ comparison_first_name , # The comparison specified above using ComparisonLevels levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), # From comparison_library ], } Method 4: Providing the spec as a dictionary \u00b6 Ultimately, comparisons are specified as a dictionary which conforms to the formal jsonschema specification of the settings dictionary and here . The library functions described above are convenience functions that provide a shorthand way to produce valid dictionaries. For maximium control over your settings, you can specify your comparisons as a dictionary. comparison_first_name = { \"output_column_name\" : \"first_name\" , \"comparison_description\" : \"First name jaro dmeta\" , \"comparison_levels\" : [ { \"sql_condition\" : \"first_name_l IS NULL OR first_name_r IS NULL\" , \"label_for_charts\" : \"Null\" , \"is_null_level\" : True , }, { \"sql_condition\" : \"first_name_l = first_name_r\" , \"label_for_charts\" : \"Exact match\" , \"tf_adjustment_column\" : \"first_name\" , \"tf_adjustment_weight\" : 1.0 , \"tf_minimum_u_value\" : 0.001 , }, { \"sql_condition\" : \"dmeta_first_name_l = dmeta_first_name_r\" , \"label_for_charts\" : \"Exact match\" , \"tf_adjustment_column\" : \"dmeta_first_name\" , \"tf_adjustment_weight\" : 1.0 , }, { \"sql_condition\" : \"jaro_winkler_sim(first_name_l, first_name_r) > 0.8\" , \"label_for_charts\" : \"Exact match\" , \"tf_adjustment_column\" : \"first_name\" , \"tf_adjustment_weight\" : 0.5 , \"tf_minimum_u_value\" : 0.001 , }, { \"sql_condition\" : \"ELSE\" , \"label_for_charts\" : \"All other comparisons\" }, ], } settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ comparison_first_name , # The comparison specified above using the dict levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), # From comparison_library ], } Creating Comparisons for specific data types \u00b6 Similarity is defined differently for types of data (e.g. names, dates of birth, postcodes, addresses, ids). Below are examples of how to structure comparisons for a variety of data types. Date Comparisons \u00b6 Date comparisons are generally structured as: Null level Exact match Fuzzy match ( using metric of choice ) Interval match (within X days/months/years) Else level The comparison_template_library contains the date_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box. from splink.duckdb.duckdb_comparison_template_library import ( date_comparison ) date_of_birth_comparison = date_comparison ( \"date_of_birth\" ) print ( date_of_birth_comparison . human_readable_description ) Comparison 'Exact match vs. Dates within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\" - 'Levenshtein <= 1' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 1 - 'Levenshtein <= 2' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 2 - 'Within 1 year' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'Within 10 years' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 10 - 'All other comparisons' with SQL rule: ELSE While also allowing flexibility to change the paramaters and/or fuzzy matching comparison level. For example: date_of_birth_comparison = date_comparison ( \"date_of_birth\" , levenshtein_thresholds = [], jaro_winkler_thresholds = [ 0.88 ], datediff_thresholds = [ 1 , 1 ], datediff_metrics = [ \"month\" , \"year\" ]) print ( date_of_birth_comparison . human_readable_description ) Comparison 'Exact match vs. Dates within jaro_winkler threshold 0.88 vs. Dates within the following thresholds Month(s): 1, Year(s): 1 vs. anything else' of \"date_of_birth\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\" - 'Jaro_winkler_similarity >= 0.88' with SQL rule: jaro_winkler_similarity(\"date_of_birth_l\", \"date_of_birth_r\") >= 0.88 - 'Within 1 month' with SQL rule: abs(date_diff('month', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'Within 1 year' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'All other comparisons' with SQL rule: ELSE To see this as a specifications dictionary you can call date_of_birth_comparison . as_dict () {'output_column_name': 'date_of_birth', 'comparison_levels': [{'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL', 'label_for_charts': 'Null', 'is_null_level': True}, {'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"', 'label_for_charts': 'Exact match'}, {'sql_condition': 'jaro_winkler_similarity(\"date_of_birth_l\", \"date_of_birth_r\") >= 0.88', 'label_for_charts': 'Jaro_winkler_similarity >= 0.88'}, {'sql_condition': '\\n abs(date_diff(\\'month\\', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1\\n ', 'label_for_charts': 'Within 1 month'}, {'sql_condition': '\\n abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1\\n ', 'label_for_charts': 'Within 1 year'}, {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}], 'comparison_description': 'Exact match vs. Dates within jaro_winkler threshold 0.88 vs. Dates within the following thresholds Month(s): 1, Year(s): 1 vs. anything else'} Which can be used as the basis for a more custom comparison, as in Method 4 , if desired. Name Comparisons \u00b6 Name comparisons for an individual name column (e.g. forename, surname) are generally structured as: Null level Exact match Fuzzy match ( using metric of choice ) Else level The comparison_template_library contains the name_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box. from splink.duckdb.duckdb_comparison_template_library import ( name_comparison ) first_name_comparison = name_comparison ( \"first_name\" ) print ( first_name_comparison . human_readable_description ) Comparison 'Exact match vs. Names within jaro_winkler thresholds 0.95, 0.88 vs. anything else' of \"first_name\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL - 'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\" - 'Jaro_winkler_similarity >= 0.95' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") >= 0.95 - 'Jaro_winkler_similarity >= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") >= 0.88 - 'All other comparisons' with SQL rule: ELSE While also allowing flexibility to change the paramaters and/or fuzzy matching comparison level. For example: surname_comparison = name_comparison ( \"surname\" , phonetic_col_name = \"surname_dm\" , term_frequency_adjustments_name = True , levenshtein_thresholds = [ 2 ], jaro_winkler_thresholds = [], jaccard_thresholds = [ 1 ] ) print ( surname_comparison . human_readable_description ) Comparison 'Exact match vs. Names with phonetic exact match vs. Dates within levenshtein threshold 2 vs. Names within jaccard threshold 1 vs. anything else' of \"surname\" and \"surname_dm\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"surname_l\" IS NULL OR \"surname_r\" IS NULL - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\" - 'Exact match surname_dm' with SQL rule: \"surname_dm_l\" = \"surname_dm_r\" - 'Levenshtein <= 2' with SQL rule: levenshtein(\"surname_l\", \"surname_r\") <= 2 - 'Jaccard >= 1' with SQL rule: jaccard(\"surname_l\", \"surname_r\") >= 1 - 'All other comparisons' with SQL rule: ELSE Where surname_dm refers to a column which has used the DoubleMetaphone algorithm on surname to give a phonetic spelling. This helps to catch names which sounds the same but have different spellings (e.g. Stephens vs Stevens). For more on Phonetic Transformations, see the topic guide . To see this as a specifications dictionary you can call surname_comparison . as_dict () {'output_column_name': 'custom_surname_surname_dm', 'comparison_levels': [{'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL', 'label_for_charts': 'Null', 'is_null_level': True}, {'sql_condition': '\"surname_l\" = \"surname_r\"', 'label_for_charts': 'Exact match surname', 'tf_adjustment_column': 'surname', 'tf_adjustment_weight': 1.0}, {'sql_condition': '\"surname_dm_l\" = \"surname_dm_r\"', 'label_for_charts': 'Exact match surname_dm'}, {'sql_condition': 'levenshtein(\"surname_l\", \"surname_r\") <= 2', 'label_for_charts': 'Levenshtein <= 2'}, {'sql_condition': 'jaccard(\"surname_l\", \"surname_r\") >= 1', 'label_for_charts': 'Jaccard >= 1'}, {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}], 'comparison_description': 'Exact match vs. Names with phonetic exact match vs. Dates within levenshtein threshold 2 vs. Names within jaccard threshold 1 vs. anything else'} Which can be used as the basis for a more custom comparison, as in Method 4 , if desired.","title":"Defining and customising comparisons"},{"location":"topic_guides/customising_comparisons.html#defining-and-customising-how-record-comparisons-are-made","text":"A key feature of Splink is the ability to customise how record comparisons are made - that is, how similarity is defined for different data types. For example, the definition of similarity that is appropriate for a date of birth field is different than for a first name field. By tailoring the definitions of similarity, linking models are more effectively able to distinguish beteween different gradations of similarity, leading to more accurate data linking models. Note that for performance reasons, Splink requires the user to define n discrete levels (gradations) of similarity.","title":"Defining and customising how record comparisons are made"},{"location":"topic_guides/customising_comparisons.html#comparing-information","text":"Comparisons are defined on pairwise record comparisons. Suppose for instance your data contains first_name and surname and dob : id first_name surname dob 1 john smith 1991-04-11 2 jon smith 1991-04-17 3 john smyth 1991-04-11 To compare these records, at the blocking stage, Splink will set these records against each other in a table of pairwise record comparisons: id_l id_r first_name_l first_name_r surname_l surname_r dob_l dob_r 1 2 john jon smith smith 1991-04-11 1991-04-17 1 3 john john smith smyth 1991-04-11 1991-04-11 2 3 jon john smith smyth 1991-04-17 1991-04-11 When defining comparisons, we are defining rules that operate on each row of this latter table of pairwise comparisons","title":"Comparing information"},{"location":"topic_guides/customising_comparisons.html#comparisons-comparisontemplates-and-comparisonlevels","text":"A Splink model contains a collection of Comparisons and ComparisonLevels organised in a hierarchy. An example is as follows: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: Up to one character difference \u2502 \u251c\u2500-- ComparisonLevel: Up to three character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. A fuller description of Comaprison s and ComparisonLevel s can be found here and here respectively. How are these comparisons specified?","title":"Comparisons, ComparisonTemplates and ComparisonLevels"},{"location":"topic_guides/customising_comparisons.html#three-ways-of-specifying-comparisons","text":"In Splink, there are three ways of specifying Comparisons : Using pre-baked comparisons from a backend's ComparisonLibrary or ComparisonTemplateLibrary . (Most simple/succinct) Composing pre-defined ComparisonLevels from a backend's ComparisonLevelLibrary Writing a full spec of a Comparison by hand (most verbose/flexible)","title":"Three ways of specifying Comparisons"},{"location":"topic_guides/customising_comparisons.html#method-1-using-the-comparisonlibrary","text":"The ComparisonLibrary for a each backend ( DuckDB , Spark , etc.) contains pre-baked similarity functions that cover many common use cases. These functions generate an entire Comparison , composed of several ComparisonLevels The following provides an example of using the ComparisonLibrary for DuckDB. from splink.duckdb.duckdb_comparison_library import ( exact_match , levenshtein_at_thresholds , ) first_name_comparison = exact_match ( \"first_name\" ) print ( first_name_comparison . human_readable_description ) Comparison 'Exact match vs. anything else' of \"first_name\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL - 'Exact match' with SQL rule: \"first_name_l\" = \"first_name_r\" - 'All other comparisons' with SQL rule: ELSE Note that, under the hood, these functions generate a Python dictionary, which conforms to the underlying .json specification of a model: first_name_comparison . as_dict () {'output_column_name': 'first_name', 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL', 'label_for_charts': 'Null', 'is_null_level': True}, {'sql_condition': '\"first_name_l\" = \"first_name_r\"', 'label_for_charts': 'Exact match'}, {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}], 'comparison_description': 'Exact match vs. anything else'} We can now generate a second, more complex comparison: from splink.duckdb.duckdb_comparison_library import ( exact_match , levenshtein_at_thresholds , ) dob_comparison = levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ]) print ( dob_comparison . human_readable_description ) Comparison 'Exact match vs. levenshtein at thresholds 1, 2 vs. anything else' of \"dob\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"dob_l\" IS NULL OR \"dob_r\" IS NULL - 'Exact match' with SQL rule: \"dob_l\" = \"dob_r\" - 'Levenshtein <= 1' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") <= 1 - 'Levenshtein <= 2' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") <= 2 - 'All other comparisons' with SQL rule: ELSE These Comparisons can be specified in a data linking model as follows: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ exact_match ( \"first_name\" ), levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ]), ], }","title":"Method 1: Using the ComparisonLibrary"},{"location":"topic_guides/customising_comparisons.html#method-2-using-the-comparisontemplatelibrary","text":"The ComparisonTemplateLibrary is very similar to ComparisonLibrary in that it contains pre-baked similarity functions for each backend (DuckDB, Spark, etc.) to cover common use cases. The key difference is that ComparisonTemplateLibrary contains functions to generate a 'best practice' Comparison based on the type of data in a given column. This includes: How comparison is structured (what comparison levels are included, and in what order) Default parameters (e.g. levenshtein_thresholds = [1,2] ) The following provides an example of using the ComparisonTemplateLibrary for DuckDB. from splink.duckdb.duckdb_comparison_template_library import ( date_comparison ) date_of_birth_comparison = date_comparison ( \"date_of_birth\" ) print ( date_of_birth_comparison . human_readable_description ) Comparison 'Exact match vs. Dates within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\" - 'Levenshtein <= 1' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 1 - 'Levenshtein <= 2' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 2 - 'Within 1 year' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'Within 10 years' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 10 - 'All other comparisons' with SQL rule: ELSE These Comparisons can be specified in a data linking model as follows: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ exact_match ( \"first_name\" ), date_comparison ( \"dob\" ), ] } You can customise a ComparisonTemplate by choosing your own values for the function parameters , but for anything more bespoke you will want to construct a Comparison with ComparisonLevels or provide the spec as a dictionary.","title":"Method 2: Using the ComparisonTemplateLibrary"},{"location":"topic_guides/customising_comparisons.html#method-3-comparisonlevels","text":"The ComparisonLevels API provides a lower-level API that gives the user greater control over their comparisons. For example, the user may wish to specify a comparison that has levels for a match on dmetaphone and jaro_winkler of the first_name field. The below example assumes the user has derived a column dmeta_first_name which contains the dmetaphone of the first name. from splink.spark.spark_comparison_level_library import ( exact_match_level , null_level , else_level , ) from splink.spark.spark_comparison_library import levenshtein_at_thresholds comparison_first_name = { \"output_column_name\" : \"first_name\" , \"comparison_description\" : \"First name jaro dmeta\" , \"comparison_levels\" : [ null_level ( \"first_name\" ), exact_match_level ( \"first_name\" , term_frequency_adjustments = True ), exact_match_level ( \"dmeta_first_name\" , term_frequency_adjustments = True ), else_level (), ], } from splink.comparison import Comparison print ( Comparison ( comparison_first_name ) . human_readable_description ) Comparison 'First name jaro dmeta' of `first_name` and `dmeta_first_name`. Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: `first_name_l` IS NULL OR `first_name_r` IS NULL - 'Exact match' with SQL rule: `first_name_l` = `first_name_r` - 'Exact match' with SQL rule: `dmeta_first_name_l` = `dmeta_first_name_r` - 'All other comparisons' with SQL rule: ELSE This can now be specified in the settings dictionary as follows: settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ comparison_first_name , # The comparison specified above using ComparisonLevels levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), # From comparison_library ], }","title":"Method 3: ComparisonLevels"},{"location":"topic_guides/customising_comparisons.html#method-4-providing-the-spec-as-a-dictionary","text":"Ultimately, comparisons are specified as a dictionary which conforms to the formal jsonschema specification of the settings dictionary and here . The library functions described above are convenience functions that provide a shorthand way to produce valid dictionaries. For maximium control over your settings, you can specify your comparisons as a dictionary. comparison_first_name = { \"output_column_name\" : \"first_name\" , \"comparison_description\" : \"First name jaro dmeta\" , \"comparison_levels\" : [ { \"sql_condition\" : \"first_name_l IS NULL OR first_name_r IS NULL\" , \"label_for_charts\" : \"Null\" , \"is_null_level\" : True , }, { \"sql_condition\" : \"first_name_l = first_name_r\" , \"label_for_charts\" : \"Exact match\" , \"tf_adjustment_column\" : \"first_name\" , \"tf_adjustment_weight\" : 1.0 , \"tf_minimum_u_value\" : 0.001 , }, { \"sql_condition\" : \"dmeta_first_name_l = dmeta_first_name_r\" , \"label_for_charts\" : \"Exact match\" , \"tf_adjustment_column\" : \"dmeta_first_name\" , \"tf_adjustment_weight\" : 1.0 , }, { \"sql_condition\" : \"jaro_winkler_sim(first_name_l, first_name_r) > 0.8\" , \"label_for_charts\" : \"Exact match\" , \"tf_adjustment_column\" : \"first_name\" , \"tf_adjustment_weight\" : 0.5 , \"tf_minimum_u_value\" : 0.001 , }, { \"sql_condition\" : \"ELSE\" , \"label_for_charts\" : \"All other comparisons\" }, ], } settings = { \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"comparisons\" : [ comparison_first_name , # The comparison specified above using the dict levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = True ), # From comparison_library ], }","title":"Method 4: Providing the spec as a dictionary"},{"location":"topic_guides/customising_comparisons.html#creating-comparisons-for-specific-data-types","text":"Similarity is defined differently for types of data (e.g. names, dates of birth, postcodes, addresses, ids). Below are examples of how to structure comparisons for a variety of data types.","title":"Creating Comparisons for specific data types"},{"location":"topic_guides/customising_comparisons.html#date-comparisons","text":"Date comparisons are generally structured as: Null level Exact match Fuzzy match ( using metric of choice ) Interval match (within X days/months/years) Else level The comparison_template_library contains the date_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box. from splink.duckdb.duckdb_comparison_template_library import ( date_comparison ) date_of_birth_comparison = date_comparison ( \"date_of_birth\" ) print ( date_of_birth_comparison . human_readable_description ) Comparison 'Exact match vs. Dates within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\" - 'Levenshtein <= 1' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 1 - 'Levenshtein <= 2' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") <= 2 - 'Within 1 year' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'Within 10 years' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 10 - 'All other comparisons' with SQL rule: ELSE While also allowing flexibility to change the paramaters and/or fuzzy matching comparison level. For example: date_of_birth_comparison = date_comparison ( \"date_of_birth\" , levenshtein_thresholds = [], jaro_winkler_thresholds = [ 0.88 ], datediff_thresholds = [ 1 , 1 ], datediff_metrics = [ \"month\" , \"year\" ]) print ( date_of_birth_comparison . human_readable_description ) Comparison 'Exact match vs. Dates within jaro_winkler threshold 0.88 vs. Dates within the following thresholds Month(s): 1, Year(s): 1 vs. anything else' of \"date_of_birth\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\" - 'Jaro_winkler_similarity >= 0.88' with SQL rule: jaro_winkler_similarity(\"date_of_birth_l\", \"date_of_birth_r\") >= 0.88 - 'Within 1 month' with SQL rule: abs(date_diff('month', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'Within 1 year' with SQL rule: abs(date_diff('year', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1 - 'All other comparisons' with SQL rule: ELSE To see this as a specifications dictionary you can call date_of_birth_comparison . as_dict () {'output_column_name': 'date_of_birth', 'comparison_levels': [{'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL', 'label_for_charts': 'Null', 'is_null_level': True}, {'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"', 'label_for_charts': 'Exact match'}, {'sql_condition': 'jaro_winkler_similarity(\"date_of_birth_l\", \"date_of_birth_r\") >= 0.88', 'label_for_charts': 'Jaro_winkler_similarity >= 0.88'}, {'sql_condition': '\\n abs(date_diff(\\'month\\', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1\\n ', 'label_for_charts': 'Within 1 month'}, {'sql_condition': '\\n abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) <= 1\\n ', 'label_for_charts': 'Within 1 year'}, {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}], 'comparison_description': 'Exact match vs. Dates within jaro_winkler threshold 0.88 vs. Dates within the following thresholds Month(s): 1, Year(s): 1 vs. anything else'} Which can be used as the basis for a more custom comparison, as in Method 4 , if desired.","title":"Date Comparisons"},{"location":"topic_guides/customising_comparisons.html#name-comparisons","text":"Name comparisons for an individual name column (e.g. forename, surname) are generally structured as: Null level Exact match Fuzzy match ( using metric of choice ) Else level The comparison_template_library contains the name_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box. from splink.duckdb.duckdb_comparison_template_library import ( name_comparison ) first_name_comparison = name_comparison ( \"first_name\" ) print ( first_name_comparison . human_readable_description ) Comparison 'Exact match vs. Names within jaro_winkler thresholds 0.95, 0.88 vs. anything else' of \"first_name\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL - 'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\" - 'Jaro_winkler_similarity >= 0.95' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") >= 0.95 - 'Jaro_winkler_similarity >= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") >= 0.88 - 'All other comparisons' with SQL rule: ELSE While also allowing flexibility to change the paramaters and/or fuzzy matching comparison level. For example: surname_comparison = name_comparison ( \"surname\" , phonetic_col_name = \"surname_dm\" , term_frequency_adjustments_name = True , levenshtein_thresholds = [ 2 ], jaro_winkler_thresholds = [], jaccard_thresholds = [ 1 ] ) print ( surname_comparison . human_readable_description ) Comparison 'Exact match vs. Names with phonetic exact match vs. Dates within levenshtein threshold 2 vs. Names within jaccard threshold 1 vs. anything else' of \"surname\" and \"surname_dm\". Similarity is assessed using the following ComparisonLevels: - 'Null' with SQL rule: \"surname_l\" IS NULL OR \"surname_r\" IS NULL - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\" - 'Exact match surname_dm' with SQL rule: \"surname_dm_l\" = \"surname_dm_r\" - 'Levenshtein <= 2' with SQL rule: levenshtein(\"surname_l\", \"surname_r\") <= 2 - 'Jaccard >= 1' with SQL rule: jaccard(\"surname_l\", \"surname_r\") >= 1 - 'All other comparisons' with SQL rule: ELSE Where surname_dm refers to a column which has used the DoubleMetaphone algorithm on surname to give a phonetic spelling. This helps to catch names which sounds the same but have different spellings (e.g. Stephens vs Stevens). For more on Phonetic Transformations, see the topic guide . To see this as a specifications dictionary you can call surname_comparison . as_dict () {'output_column_name': 'custom_surname_surname_dm', 'comparison_levels': [{'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL', 'label_for_charts': 'Null', 'is_null_level': True}, {'sql_condition': '\"surname_l\" = \"surname_r\"', 'label_for_charts': 'Exact match surname', 'tf_adjustment_column': 'surname', 'tf_adjustment_weight': 1.0}, {'sql_condition': '\"surname_dm_l\" = \"surname_dm_r\"', 'label_for_charts': 'Exact match surname_dm'}, {'sql_condition': 'levenshtein(\"surname_l\", \"surname_r\") <= 2', 'label_for_charts': 'Levenshtein <= 2'}, {'sql_condition': 'jaccard(\"surname_l\", \"surname_r\") >= 1', 'label_for_charts': 'Jaccard >= 1'}, {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}], 'comparison_description': 'Exact match vs. Names with phonetic exact match vs. Dates within levenshtein threshold 2 vs. Names within jaccard threshold 1 vs. anything else'} Which can be used as the basis for a more custom comparison, as in Method 4 , if desired.","title":"Name Comparisons"},{"location":"topic_guides/drivers_of_performance.html","tags":["Performance","Blocking"],"text":"Run times, performance, and linking large data \u00b6 This topic guide covers the fundamental drivers of the run time of Splink jobs. It also describes the tools that are built into Splink that help you to understand how long a job is likely to take. In summary, your choice of blocking rules is by far the most important driver of performance. Additional factors which affect performance are: the complexity of your comparisons, whether you apply term frequency adjustments, whether you choose to set retain_matching_columns and retain_intermediate_calculation_columns to True in your settings, whether you filter out comparisons with a match score below a given threshold (using a threshold_match_probability or threshold_match_weight when you call predict() ). Blocking rules \u00b6 In most large datasets, it is computationally intractable to compare every row with every other row. The number of comparisons grows with the square of the number of input records, using the formula \\(\\frac{n\\left(n-1\\right)}2\\) . For instance, a million input records implies around 500bn comparisons. In Splink, we use a technique called blocking to dramatically reduce the number of comparisons by comparing only records that adhere to certain rules, such as that the first name and date of birth must be equal . Blocking is described further here . Even after blocking, the number of comparisons generated is usually much higher than the number of input records - often between 10 and 1,000 times higher. As a result, the performance of Splink is influenced most heavily by the number of comparisons generated by the blocking rules, rather than the number of input records. This is the case for both main uses of blocking rules in Splink: estimating parameters using expectation maximisation, and generating predictions. (See here for more information on this distinction). How many comparisons will be generated by a blocking rule? \u00b6 The linker.count_num_comparisons_from_blocking_rule() , documented here will compute the number of comparisons that will be generated from a blocking rule. Users are recommended to use this function before attempting linkage, since some blocking rules may imply trillions of comparisons, resulting in record linkage jobs which run for hours and never complete. In general, we recommend a strategy of starting with strict blocking rules, and gradually loosening them. Sticking to less than 10 million comparisons is a good place to start, before scaling jobs up to 100s of millions (DuckDB on a laptop), or sometimes billions (Athena or Spark). Examples of strict and loose blocking rules \u00b6 To give an example of how blocking_rules_to_generate_predictions rules may be incrementally loosened, we may start with the following rule: l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob . This is a very strict rule, and will only create comparisons where full name and date of birth match. This has the advantage of creating few record comparisons, but the disadvantage that the rule will miss true matches where there are typos or nulls in any of these three fields. This blocking rule could be loosened to: substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname and l.year_of_birth = r.year_of_birth Now it allows for typos or aliases in the first name, so long as the first letter is the same, and errors in month or day of birth. Depending on the side of your input data, the rule could be further loosened to substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname or even l.surname = r.surname The user could use the linker.count_num_comparisons_from_blocking_rule() function to select which rule is appropriate for their data.","title":"Run times, performance and linking large data"},{"location":"topic_guides/drivers_of_performance.html#run-times-performance-and-linking-large-data","text":"This topic guide covers the fundamental drivers of the run time of Splink jobs. It also describes the tools that are built into Splink that help you to understand how long a job is likely to take. In summary, your choice of blocking rules is by far the most important driver of performance. Additional factors which affect performance are: the complexity of your comparisons, whether you apply term frequency adjustments, whether you choose to set retain_matching_columns and retain_intermediate_calculation_columns to True in your settings, whether you filter out comparisons with a match score below a given threshold (using a threshold_match_probability or threshold_match_weight when you call predict() ).","title":"Run times, performance, and linking large data"},{"location":"topic_guides/drivers_of_performance.html#blocking-rules","text":"In most large datasets, it is computationally intractable to compare every row with every other row. The number of comparisons grows with the square of the number of input records, using the formula \\(\\frac{n\\left(n-1\\right)}2\\) . For instance, a million input records implies around 500bn comparisons. In Splink, we use a technique called blocking to dramatically reduce the number of comparisons by comparing only records that adhere to certain rules, such as that the first name and date of birth must be equal . Blocking is described further here . Even after blocking, the number of comparisons generated is usually much higher than the number of input records - often between 10 and 1,000 times higher. As a result, the performance of Splink is influenced most heavily by the number of comparisons generated by the blocking rules, rather than the number of input records. This is the case for both main uses of blocking rules in Splink: estimating parameters using expectation maximisation, and generating predictions. (See here for more information on this distinction).","title":"Blocking rules"},{"location":"topic_guides/drivers_of_performance.html#how-many-comparisons-will-be-generated-by-a-blocking-rule","text":"The linker.count_num_comparisons_from_blocking_rule() , documented here will compute the number of comparisons that will be generated from a blocking rule. Users are recommended to use this function before attempting linkage, since some blocking rules may imply trillions of comparisons, resulting in record linkage jobs which run for hours and never complete. In general, we recommend a strategy of starting with strict blocking rules, and gradually loosening them. Sticking to less than 10 million comparisons is a good place to start, before scaling jobs up to 100s of millions (DuckDB on a laptop), or sometimes billions (Athena or Spark).","title":"How many comparisons will be generated by a blocking rule?"},{"location":"topic_guides/drivers_of_performance.html#examples-of-strict-and-loose-blocking-rules","text":"To give an example of how blocking_rules_to_generate_predictions rules may be incrementally loosened, we may start with the following rule: l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob . This is a very strict rule, and will only create comparisons where full name and date of birth match. This has the advantage of creating few record comparisons, but the disadvantage that the rule will miss true matches where there are typos or nulls in any of these three fields. This blocking rule could be loosened to: substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname and l.year_of_birth = r.year_of_birth Now it allows for typos or aliases in the first name, so long as the first letter is the same, and errors in month or day of birth. Depending on the side of your input data, the rule could be further loosened to substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname or even l.surname = r.surname The user could use the linker.count_num_comparisons_from_blocking_rule() function to select which rule is appropriate for their data.","title":"Examples of strict and loose blocking rules"},{"location":"topic_guides/link_type.html","tags":["Dedupe","Link","Link and Dedupe"],"text":"Link type: Linking, Deduping or Both \u00b6 Splink allows data to be linked, deduplicated or both. Linking refers to finding links between datasets, whereas deduplication finding links within datasets. Data linking is therefore only meaningful when more than one dataset is provided. This guide shows how to specify the settings dictionary and initialise the linker for the three link types. Deduplication \u00b6 The dedupe_only link type expects the user to provide a single input table, and is specified as follows from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"dedupe_only\" , # etc. } linker = DuckDBLinker ( df , settings ) Link only \u00b6 The link_only link type expects the user to provide a list of input tables, and is specified as follows: from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"link_only\" , # etc. } input_aliases = [ \"table_1\" , \"table_2\" , \"table_3\" ] linker = DuckDBLinker ([ df_1 , df_2 , df_3 ], settings , input_table_aliases = input_aliases ) The input_table_aliases argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink. Link and dedupe \u00b6 The link_and_dedupe link type expects the user to provide a list of input tables, and is specified as follows: from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"link_and_dedupe\" , # etc. } input_aliases = [ \"table_1\" , \"table_2\" , \"table_3\" ] linker = DuckDBLinker ([ df_1 , df_2 , df_3 ], settings , input_table_aliases = input_aliases ) The input_table_aliases argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.","title":"Link type - linking vs deduping"},{"location":"topic_guides/link_type.html#link-type-linking-deduping-or-both","text":"Splink allows data to be linked, deduplicated or both. Linking refers to finding links between datasets, whereas deduplication finding links within datasets. Data linking is therefore only meaningful when more than one dataset is provided. This guide shows how to specify the settings dictionary and initialise the linker for the three link types.","title":"Link type: Linking, Deduping or Both"},{"location":"topic_guides/link_type.html#deduplication","text":"The dedupe_only link type expects the user to provide a single input table, and is specified as follows from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"dedupe_only\" , # etc. } linker = DuckDBLinker ( df , settings )","title":"Deduplication"},{"location":"topic_guides/link_type.html#link-only","text":"The link_only link type expects the user to provide a list of input tables, and is specified as follows: from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"link_only\" , # etc. } input_aliases = [ \"table_1\" , \"table_2\" , \"table_3\" ] linker = DuckDBLinker ([ df_1 , df_2 , df_3 ], settings , input_table_aliases = input_aliases ) The input_table_aliases argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.","title":"Link only"},{"location":"topic_guides/link_type.html#link-and-dedupe","text":"The link_and_dedupe link type expects the user to provide a list of input tables, and is specified as follows: from splink.duckdb.duckdb_linker import DuckDBLinker settings = { \"link_type\" : \"link_and_dedupe\" , # etc. } input_aliases = [ \"table_1\" , \"table_2\" , \"table_3\" ] linker = DuckDBLinker ([ df_1 , df_2 , df_3 ], settings , input_table_aliases = input_aliases ) The input_table_aliases argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.","title":"Link and dedupe"},{"location":"topic_guides/optimising_spark.html","tags":["Performance","Spark","Salting","Parallelism"],"text":"Optimising Spark jobs \u00b6 This topic guide describes how to configue Spark to optimise performance - especially large linkage jobs which are slow or are not completing using default settings. It is assumed readers have already read the more general guide to linking big data , and blocking rules are proportionate to the size of the Spark cluster. As a very rough guide, on a small cluster of (say) 8 machines, we recommend starting with blocking rules that generate around 100 million comparisons. Once this is working, loosening the blocking rules to around 1 billion comparisons or more is often achievable. Summary: \u00b6 Ensure blocking rules are not generating too many comparisons. We recommend setting the break_lineage_method to \"parquet\" , which is the default num_partitions_on_repartition should be set so that each file in the output of predict() is roughly 100MB. Try setting spark.default.parallelism to around 5x the number of CPUs in your cluster For a cluster with 10 CPUs, that outputs about 8GB of data in parquet format, the following setup may be appropriate: spark . conf . set ( \"spark.default.parallelism\" , \"50\" ) spark . conf . set ( \"spark.sql.shuffle.partitions\" , \"50\" ) linker = SparkLinker ( person_standardised_nodes , settings , break_lineage_method = \"parquet\" , num_partitions_on_repartition = 80 , ) Breaking lineage \u00b6 Splink uses an iterative algorithm for model training, and more generally, lineage is long and complex. We have found that big jobs fail to complete without further optimisation. This is a well-known problem : \"This long lineage bottleneck is widely known by sophisticated Spark application programmers. A common practice for dealing with long lineage is to have the application program strategically checkpoint RDDs at code locations that truncate much of the lineage for checkpointed data and resume computation immediately from the checkpoint.\" Splink will automatically break lineage in sensible places. We have found in practice that, when running Spark jobs backed by AWS S3, the fastest method of breaking lineage is persisting outputs to .parquet file. You can do this using the break_lineage_method parameter as follows: linker = SparkLinker( person_standardised_nodes, settings, break_lineage_method=\"parquet\" ) Other options are checkpoint and persist . For different Spark setups, particularly if you have fast local storage, you may find these options perform better. Spark Parallelism \u00b6 We suggest setting default parallelism to roughly 5x the number of CPUs in your cluster. This is a very rough rule of thumb, and if you're encountering performance problems you may wish to experiment with different values. One way to set default parallelism is as follows: from pyspark.context import SparkContext , SparkConf from pyspark.sql import SparkSession conf = SparkConf () conf . set ( \"spark.default.parallelism\" , \"50\" ) conf . set ( \"spark.sql.shuffle.partitions\" , \"50\" ) sc = SparkContext . getOrCreate ( conf = conf ) spark = SparkSession ( sc ) In general, increasing parallelism will make Spark 'chunk' your job into a larger amount of smaller tasks. This may solve memory issues. But note there is a tradeoff here: if you increase parallelism too high, Spark may take too much time scheduling large numbers of tasks, and may even run out of memory performing this work. See here . Also note that when blocking, jobs cannot be split into a large number of tasks than the cardinality of the blocking rule. For example, if you block on month of birth, this will be split into 12 tasks, irrespective of the parallelism setting. See here . You can use salting (below) to partially address this limitation. Repartition after blocking \u00b6 For some jobs, setting repartition_after_blocking=True when you initialise the SparkLinker may improve performance. Salting \u00b6 For very large jobs, you may find that salting your blocking keys results in faster run times. General Spark config \u00b6 Splink generates large numbers of record comparisons from relatively small input datasets. This is an unusual type of workload, and so default Spark parameters are not always appropriate. Some of the issues encountered are similar to performance issues encountered with cartesian joins - so some of the tips in relevant articles may help.","title":"Optimising Spark performance"},{"location":"topic_guides/optimising_spark.html#optimising-spark-jobs","text":"This topic guide describes how to configue Spark to optimise performance - especially large linkage jobs which are slow or are not completing using default settings. It is assumed readers have already read the more general guide to linking big data , and blocking rules are proportionate to the size of the Spark cluster. As a very rough guide, on a small cluster of (say) 8 machines, we recommend starting with blocking rules that generate around 100 million comparisons. Once this is working, loosening the blocking rules to around 1 billion comparisons or more is often achievable.","title":"Optimising Spark jobs"},{"location":"topic_guides/optimising_spark.html#summary","text":"Ensure blocking rules are not generating too many comparisons. We recommend setting the break_lineage_method to \"parquet\" , which is the default num_partitions_on_repartition should be set so that each file in the output of predict() is roughly 100MB. Try setting spark.default.parallelism to around 5x the number of CPUs in your cluster For a cluster with 10 CPUs, that outputs about 8GB of data in parquet format, the following setup may be appropriate: spark . conf . set ( \"spark.default.parallelism\" , \"50\" ) spark . conf . set ( \"spark.sql.shuffle.partitions\" , \"50\" ) linker = SparkLinker ( person_standardised_nodes , settings , break_lineage_method = \"parquet\" , num_partitions_on_repartition = 80 , )","title":"Summary:"},{"location":"topic_guides/optimising_spark.html#breaking-lineage","text":"Splink uses an iterative algorithm for model training, and more generally, lineage is long and complex. We have found that big jobs fail to complete without further optimisation. This is a well-known problem : \"This long lineage bottleneck is widely known by sophisticated Spark application programmers. A common practice for dealing with long lineage is to have the application program strategically checkpoint RDDs at code locations that truncate much of the lineage for checkpointed data and resume computation immediately from the checkpoint.\" Splink will automatically break lineage in sensible places. We have found in practice that, when running Spark jobs backed by AWS S3, the fastest method of breaking lineage is persisting outputs to .parquet file. You can do this using the break_lineage_method parameter as follows: linker = SparkLinker( person_standardised_nodes, settings, break_lineage_method=\"parquet\" ) Other options are checkpoint and persist . For different Spark setups, particularly if you have fast local storage, you may find these options perform better.","title":"Breaking lineage"},{"location":"topic_guides/optimising_spark.html#spark-parallelism","text":"We suggest setting default parallelism to roughly 5x the number of CPUs in your cluster. This is a very rough rule of thumb, and if you're encountering performance problems you may wish to experiment with different values. One way to set default parallelism is as follows: from pyspark.context import SparkContext , SparkConf from pyspark.sql import SparkSession conf = SparkConf () conf . set ( \"spark.default.parallelism\" , \"50\" ) conf . set ( \"spark.sql.shuffle.partitions\" , \"50\" ) sc = SparkContext . getOrCreate ( conf = conf ) spark = SparkSession ( sc ) In general, increasing parallelism will make Spark 'chunk' your job into a larger amount of smaller tasks. This may solve memory issues. But note there is a tradeoff here: if you increase parallelism too high, Spark may take too much time scheduling large numbers of tasks, and may even run out of memory performing this work. See here . Also note that when blocking, jobs cannot be split into a large number of tasks than the cardinality of the blocking rule. For example, if you block on month of birth, this will be split into 12 tasks, irrespective of the parallelism setting. See here . You can use salting (below) to partially address this limitation.","title":"Spark Parallelism"},{"location":"topic_guides/optimising_spark.html#repartition-after-blocking","text":"For some jobs, setting repartition_after_blocking=True when you initialise the SparkLinker may improve performance.","title":"Repartition after blocking"},{"location":"topic_guides/optimising_spark.html#salting","text":"For very large jobs, you may find that salting your blocking keys results in faster run times.","title":"Salting"},{"location":"topic_guides/optimising_spark.html#general-spark-config","text":"Splink generates large numbers of record comparisons from relatively small input datasets. This is an unusual type of workload, and so default Spark parameters are not always appropriate. Some of the issues encountered are similar to performance issues encountered with cartesian joins - so some of the tips in relevant articles may help.","title":"General Spark config"},{"location":"topic_guides/phonetic.html","tags":["API","comparisons","blocking","Soundex","Double Metaphone"],"text":"Phonetic transformation algorithms \u00b6 Phonetic transformation algorithms can be used to identify words that sound similar, even if they are spelled differently. These algorithms are often used in a preprocessing step for data linking and can assist in the blocking and comparison process. By incorporating them into blocking rules, it allows for possible candidate pairs of entities with phonetically similar transforms to be considered for linking. This can result in a \"fuzzier\" blocking process, which may be beneficial for certain projects. Similarly, phonetically similar transforms offer another way to do fuzzy-matching within a comparison. Below are some examples of well known phonetic transformation algorithmns. Soundex \u00b6 Soundex is a widely-used algorithm for phonetic matching. It was developed by Margaret K. Odell and Robert C. Russell in the early 1900s. The basic idea behind Soundex is to encode words based on their sounds, rather than their spelling. This allows words that sound similar to be encoded in the same way, even if they are spelled differently. To encode a word with Soundex, the algorithm follows these steps: Retain the first letter of the word. Replace each of the following letters with the corresponding number: B, F, P, V: 1 C, G, J, K, Q, S, X, Z: 2 D, T: 3 L: 4 M, N: 5 R: 6 Replace all other letters with the number 0. As for an example of similar names having the same Soundex code, consider the names \"Smith\" and \"Smythe\". These names are spelled differently, but they have the same pronunciation, so they would be encoded as \"S530\" using Soundex. Double Metaphone \u00b6 Double Metaphone is a more advanced algorithm for phonetic matching, developed by Lawrence Philips in the 1990s. It is based on the Soundex algorithm, but is designed to be more accurate and to handle a wider range of words. To encode a word with Double Metaphone, the algorithm follows these steps: Retain the first letter of the word. Remove any vowels, except for the first letter. Replace certain letters with other letters or combinations of letters, as follows: B: B C: X if followed by \"ia\" or \"h\" (e.g. \"Cia\" becomes \"X\", \"Ch\" becomes \"X\"), otherwise \"S\" D: J if followed by \"ge\", \"gy\", \"gi\", otherwise \"T\" G: J if followed by \"g\", \"d\", \"i\", \"y\", \"e\", otherwise \"K\" K: K L: L M: M N: N P: F Q: K R: R S: X if followed by \"h\" (e.g. \"Sh\" becomes \"X\"), otherwise \"S\" T: X if followed by \"ia\" or \"ch\" (e.g. \"Tia\" becomes \"X\", \"Tch\" becomes \"X\"), otherwise \"T\" V: F X: KS Z: S For example, surnames such as Cone and Kohn are encoded in the same way as \"KN\" , because they sound similar even though they are spelled differently.","title":"Phonetic transformations"},{"location":"topic_guides/phonetic.html#phonetic-transformation-algorithms","text":"Phonetic transformation algorithms can be used to identify words that sound similar, even if they are spelled differently. These algorithms are often used in a preprocessing step for data linking and can assist in the blocking and comparison process. By incorporating them into blocking rules, it allows for possible candidate pairs of entities with phonetically similar transforms to be considered for linking. This can result in a \"fuzzier\" blocking process, which may be beneficial for certain projects. Similarly, phonetically similar transforms offer another way to do fuzzy-matching within a comparison. Below are some examples of well known phonetic transformation algorithmns.","title":"Phonetic transformation algorithms"},{"location":"topic_guides/phonetic.html#soundex","text":"Soundex is a widely-used algorithm for phonetic matching. It was developed by Margaret K. Odell and Robert C. Russell in the early 1900s. The basic idea behind Soundex is to encode words based on their sounds, rather than their spelling. This allows words that sound similar to be encoded in the same way, even if they are spelled differently. To encode a word with Soundex, the algorithm follows these steps: Retain the first letter of the word. Replace each of the following letters with the corresponding number: B, F, P, V: 1 C, G, J, K, Q, S, X, Z: 2 D, T: 3 L: 4 M, N: 5 R: 6 Replace all other letters with the number 0. As for an example of similar names having the same Soundex code, consider the names \"Smith\" and \"Smythe\". These names are spelled differently, but they have the same pronunciation, so they would be encoded as \"S530\" using Soundex.","title":"Soundex"},{"location":"topic_guides/phonetic.html#double-metaphone","text":"Double Metaphone is a more advanced algorithm for phonetic matching, developed by Lawrence Philips in the 1990s. It is based on the Soundex algorithm, but is designed to be more accurate and to handle a wider range of words. To encode a word with Double Metaphone, the algorithm follows these steps: Retain the first letter of the word. Remove any vowels, except for the first letter. Replace certain letters with other letters or combinations of letters, as follows: B: B C: X if followed by \"ia\" or \"h\" (e.g. \"Cia\" becomes \"X\", \"Ch\" becomes \"X\"), otherwise \"S\" D: J if followed by \"ge\", \"gy\", \"gi\", otherwise \"T\" G: J if followed by \"g\", \"d\", \"i\", \"y\", \"e\", otherwise \"K\" K: K L: L M: M N: N P: F Q: K R: R S: X if followed by \"h\" (e.g. \"Sh\" becomes \"X\"), otherwise \"S\" T: X if followed by \"ia\" or \"ch\" (e.g. \"Tia\" becomes \"X\", \"Tch\" becomes \"X\"), otherwise \"T\" V: F X: KS Z: S For example, surnames such as Cone and Kohn are encoded in the same way as \"KN\" , because they sound similar even though they are spelled differently.","title":"Double Metaphone"},{"location":"topic_guides/querying_splink_results.html","text":"Retrieving and Querying Splink results with the SplinkDataFrame \u00b6 Splink returns tables of results using a class called a SplinkDataFrame . e.g. when you run df_predict = linker.predict() df_predict is a SplinkDataFrame A SplinkDataFrame is a abstraction Splink's results, which under the hood are a table in the underlying database. It's possible to convert a SplinkDataFrame into a Pandas dataframe using splink_df.as_pandas_dataframe() . However, this is not recommended because Splink results can be very large, so converting them into pandas can be slow and result in out of memory errors. You can find out the name of the table in the underlying database using df_predict.physical_name . This enables you to run SQL queries directly against the results. You can execute queries using linker.query_sql . This is the recommended approach as it's typically faster and more memory efficient than using pandas dataframes. The following is an example of this approach, in which we use SQL to find the best match to each input record in a link_type=\"link_only\" job (i.e remove duplicate matches): # Linker is a duckdb linker with link_type set to \"link_only\" results = linker . predict ( threshold_match_probability = 0.75 ) sql = f \"\"\" with ranked as ( select *, row_number() OVER ( PARTITION BY unique_id_l order by match_weight desc ) as row_number from { results . physical_name } ) select * from ranked where row_number = 1 \"\"\" linker . query_sql ( sql ) Note that linker.query_sql will return a pandas dataframe by default, but you can instead return a SplinkDataFrame as follows: linker . query_sql ( sql , output_type = 'splink_df' )","title":"Retrieving and querying Splink results"},{"location":"topic_guides/querying_splink_results.html#retrieving-and-querying-splink-results-with-the-splinkdataframe","text":"Splink returns tables of results using a class called a SplinkDataFrame . e.g. when you run df_predict = linker.predict() df_predict is a SplinkDataFrame A SplinkDataFrame is a abstraction Splink's results, which under the hood are a table in the underlying database. It's possible to convert a SplinkDataFrame into a Pandas dataframe using splink_df.as_pandas_dataframe() . However, this is not recommended because Splink results can be very large, so converting them into pandas can be slow and result in out of memory errors. You can find out the name of the table in the underlying database using df_predict.physical_name . This enables you to run SQL queries directly against the results. You can execute queries using linker.query_sql . This is the recommended approach as it's typically faster and more memory efficient than using pandas dataframes. The following is an example of this approach, in which we use SQL to find the best match to each input record in a link_type=\"link_only\" job (i.e remove duplicate matches): # Linker is a duckdb linker with link_type set to \"link_only\" results = linker . predict ( threshold_match_probability = 0.75 ) sql = f \"\"\" with ranked as ( select *, row_number() OVER ( PARTITION BY unique_id_l order by match_weight desc ) as row_number from { results . physical_name } ) select * from ranked where row_number = 1 \"\"\" linker . query_sql ( sql ) Note that linker.query_sql will return a pandas dataframe by default, but you can instead return a SplinkDataFrame as follows: linker . query_sql ( sql , output_type = 'splink_df' )","title":"Retrieving and Querying Splink results with the SplinkDataFrame"},{"location":"topic_guides/salting.html","tags":["Performance","Salting","Spark"],"text":"Salting blocking rules \u00b6 For very large linkages using Apache Spark, Splink supports salting blocking rules. Under certain conditions, this can help Spark better parallelise workflows, leading to shorter run times, and avoiding out of memory errors. It is most likely to help where you have blocking rules that create very large numbers of comparisons (100m records+) and where there is skew in how record comparisons are made (e.g. blocking on full name creates more comparisons amongst 'John Smith's than many other names). Further information about the motivation for salting can be found here . Note that salting is only available for the Spark backend How to use salting \u00b6 To enable salting using the SparkLinker , you provide some of your blocking rules as a dictionary rather than a string. This enables you to choose the number of salts for each blocking rule. Blocking rules provided as plain strings default to no salting ( salting_partitions = 1 ) The following code snippet illustrates: import logging from pyspark.context import SparkContext, SparkConf from pyspark.sql import SparkSession from splink.spark.spark_linker import SparkLinker from splink.spark.spark_comparison_library import levenshtein_at_thresholds, exact_match conf = SparkConf() conf.set(\"spark.driver.memory\", \"12g\") conf.set(\"spark.sql.shuffle.partitions\", \"8\") conf.set(\"spark.default.parallelism\", \"8\") sc = SparkContext.getOrCreate(conf=conf) spark = SparkSession(sc) settings = { \"probability_two_random_records_match\": 0.01, \"link_type\": \"dedupe_only\", \"blocking_rules_to_generate_predictions\": [ \"l.dob = r.dob\", {\"blocking_rule\": \"l.first_name = r.first_name\", \"salting_partitions\": 4}, ], \"comparisons\": [ levenshtein_at_thresholds(\"first_name\", 2), exact_match(\"surname\"), exact_match(\"dob\"), exact_match(\"city\", term_frequency_adjustments=True), exact_match(\"email\"), ], \"retain_matching_columns\": True, \"retain_intermediate_calculation_columns\": True, \"additional_columns_to_retain\": [\"group\"], \"max_iterations\": 1, \"em_convergence\": 0.01, } df = spark.read.csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\", header=True) linker = SparkLinker(df, settings) logging.getLogger(\"splink\").setLevel(5) linker.load_settings(settings) linker.deterministic_link() And we can see that salting has been applied by looking at the SQL generated in the log: SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '0' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.dob = r.dob WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 1 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 2 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 3 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 4 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id","title":"Salting blocking rules"},{"location":"topic_guides/salting.html#salting-blocking-rules","text":"For very large linkages using Apache Spark, Splink supports salting blocking rules. Under certain conditions, this can help Spark better parallelise workflows, leading to shorter run times, and avoiding out of memory errors. It is most likely to help where you have blocking rules that create very large numbers of comparisons (100m records+) and where there is skew in how record comparisons are made (e.g. blocking on full name creates more comparisons amongst 'John Smith's than many other names). Further information about the motivation for salting can be found here . Note that salting is only available for the Spark backend","title":"Salting blocking rules"},{"location":"topic_guides/salting.html#how-to-use-salting","text":"To enable salting using the SparkLinker , you provide some of your blocking rules as a dictionary rather than a string. This enables you to choose the number of salts for each blocking rule. Blocking rules provided as plain strings default to no salting ( salting_partitions = 1 ) The following code snippet illustrates: import logging from pyspark.context import SparkContext, SparkConf from pyspark.sql import SparkSession from splink.spark.spark_linker import SparkLinker from splink.spark.spark_comparison_library import levenshtein_at_thresholds, exact_match conf = SparkConf() conf.set(\"spark.driver.memory\", \"12g\") conf.set(\"spark.sql.shuffle.partitions\", \"8\") conf.set(\"spark.default.parallelism\", \"8\") sc = SparkContext.getOrCreate(conf=conf) spark = SparkSession(sc) settings = { \"probability_two_random_records_match\": 0.01, \"link_type\": \"dedupe_only\", \"blocking_rules_to_generate_predictions\": [ \"l.dob = r.dob\", {\"blocking_rule\": \"l.first_name = r.first_name\", \"salting_partitions\": 4}, ], \"comparisons\": [ levenshtein_at_thresholds(\"first_name\", 2), exact_match(\"surname\"), exact_match(\"dob\"), exact_match(\"city\", term_frequency_adjustments=True), exact_match(\"email\"), ], \"retain_matching_columns\": True, \"retain_intermediate_calculation_columns\": True, \"additional_columns_to_retain\": [\"group\"], \"max_iterations\": 1, \"em_convergence\": 0.01, } df = spark.read.csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\", header=True) linker = SparkLinker(df, settings) logging.getLogger(\"splink\").setLevel(5) linker.load_settings(settings) linker.deterministic_link() And we can see that salting has been applied by looking at the SQL generated in the log: SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '0' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.dob = r.dob WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 1 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 2 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 3 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id UNION ALL SELECT l.unique_id AS unique_id_l, r.unique_id AS unique_id_r, l.first_name AS first_name_l, r.first_name AS first_name_r, l.surname AS surname_l, r.surname AS surname_r, l.dob AS dob_l, r.dob AS dob_r, l.city AS city_l, r.city AS city_r, l.tf_city AS tf_city_l, r.tf_city AS tf_city_r, l.email AS email_l, r.email AS email_r, l.`group` AS `group_l`, r.`group` AS `group_r`, '1' AS match_key FROM __splink__df_concat_with_tf AS l INNER JOIN __splink__df_concat_with_tf AS r ON l.first_name = r.first_name AND CEIL(l.__splink_salt * 4) = 4 AND NOT ( COALESCE(( l.dob = r.dob ), FALSE) ) WHERE l.unique_id < r.unique_id","title":"How to use salting"}]}